<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Russ Cox" />
  <meta name="author" content="Frans Kaashoek" />
  <meta name="author" content="Robert Morris" />
  <title>xv6: a simple, Unix-like teaching operating system</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height; auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title"><strong>xv6: a simple, Unix-like teaching operating
system</strong></h1>
<p class="author">Russ Cox</p>
<p class="author">Frans Kaashoek</p>
<p class="author">Robert Morris</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#foreword-and-acknowledgments"
id="toc-foreword-and-acknowledgments">Foreword and
acknowledgments</a></li>
<li><a href="#CH:UNIX" id="toc-CH:UNIX"><span
class="toc-section-number">1</span> Operating system interfaces</a>
<ul>
<li><a href="#processes-and-memory" id="toc-processes-and-memory"><span
class="toc-section-number">1.1</span> Processes and memory</a></li>
<li><a href="#io-and-file-descriptors"
id="toc-io-and-file-descriptors"><span
class="toc-section-number">1.2</span> I/O and File descriptors</a></li>
<li><a href="#pipes" id="toc-pipes"><span
class="toc-section-number">1.3</span> Pipes</a></li>
<li><a href="#file-system" id="toc-file-system"><span
class="toc-section-number">1.4</span> File system</a></li>
<li><a href="#real-world" id="toc-real-world"><span
class="toc-section-number">1.5</span> Real world</a></li>
<li><a href="#exercises" id="toc-exercises"><span
class="toc-section-number">1.6</span> Exercises</a></li>
</ul></li>
<li><a href="#CH:FIRST" id="toc-CH:FIRST"><span
class="toc-section-number">2</span> Operating system organization</a>
<ul>
<li><a href="#abstracting-physical-resources"
id="toc-abstracting-physical-resources"><span
class="toc-section-number">2.1</span> Abstracting physical
resources</a></li>
<li><a href="#user-mode-supervisor-mode-and-system-calls"
id="toc-user-mode-supervisor-mode-and-system-calls"><span
class="toc-section-number">2.2</span> User mode, supervisor mode, and
system calls</a></li>
<li><a href="#kernel-organization" id="toc-kernel-organization"><span
class="toc-section-number">2.3</span> Kernel organization</a></li>
<li><a href="#code-xv6-organization"
id="toc-code-xv6-organization"><span
class="toc-section-number">2.4</span> Code: xv6 organization</a></li>
<li><a href="#process-overview" id="toc-process-overview"><span
class="toc-section-number">2.5</span> Process overview</a></li>
<li><a href="#code-starting-xv6-the-first-process-and-system-call"
id="toc-code-starting-xv6-the-first-process-and-system-call"><span
class="toc-section-number">2.6</span> Code: starting xv6, the first
process and system call</a></li>
<li><a href="#security-model" id="toc-security-model"><span
class="toc-section-number">2.7</span> Security Model</a></li>
<li><a href="#real-world-1" id="toc-real-world-1"><span
class="toc-section-number">2.8</span> Real world</a></li>
<li><a href="#exercises-1" id="toc-exercises-1"><span
class="toc-section-number">2.9</span> Exercises</a></li>
</ul></li>
<li><a href="#CH:MEM" id="toc-CH:MEM"><span
class="toc-section-number">3</span> Page tables</a>
<ul>
<li><a href="#paging-hardware" id="toc-paging-hardware"><span
class="toc-section-number">3.1</span> Paging hardware</a></li>
<li><a href="#kernel-address-space" id="toc-kernel-address-space"><span
class="toc-section-number">3.2</span> Kernel address space</a></li>
<li><a href="#code-creating-an-address-space"
id="toc-code-creating-an-address-space"><span
class="toc-section-number">3.3</span> Code: creating an address
space</a></li>
<li><a href="#physical-memory-allocation"
id="toc-physical-memory-allocation"><span
class="toc-section-number">3.4</span> Physical memory
allocation</a></li>
<li><a href="#code-physical-memory-allocator"
id="toc-code-physical-memory-allocator"><span
class="toc-section-number">3.5</span> Code: Physical memory
allocator</a></li>
<li><a href="#process-address-space"
id="toc-process-address-space"><span
class="toc-section-number">3.6</span> Process address space</a></li>
<li><a href="#code-sbrk" id="toc-code-sbrk"><span
class="toc-section-number">3.7</span> Code: sbrk</a></li>
<li><a href="#code-exec" id="toc-code-exec"><span
class="toc-section-number">3.8</span> Code: exec</a></li>
<li><a href="#real-world-2" id="toc-real-world-2"><span
class="toc-section-number">3.9</span> Real world</a></li>
<li><a href="#exercises-2" id="toc-exercises-2"><span
class="toc-section-number">3.10</span> Exercises</a></li>
</ul></li>
<li><a href="#CH:TRAP" id="toc-CH:TRAP"><span
class="toc-section-number">4</span> Traps and system calls</a>
<ul>
<li><a href="#risc-v-trap-machinery"
id="toc-risc-v-trap-machinery"><span
class="toc-section-number">4.1</span> RISC-V trap machinery</a></li>
<li><a href="#traps-from-user-space"
id="toc-traps-from-user-space"><span
class="toc-section-number">4.2</span> Traps from user space</a></li>
<li><a href="#code-calling-system-calls"
id="toc-code-calling-system-calls"><span
class="toc-section-number">4.3</span> Code: Calling system
calls</a></li>
<li><a href="#code-system-call-arguments"
id="toc-code-system-call-arguments"><span
class="toc-section-number">4.4</span> Code: System call
arguments</a></li>
<li><a href="#s:ktraps" id="toc-s:ktraps"><span
class="toc-section-number">4.5</span> Traps from kernel space</a></li>
<li><a href="#sec:pagefaults" id="toc-sec:pagefaults"><span
class="toc-section-number">4.6</span> Page-fault exceptions</a></li>
<li><a href="#real-world-3" id="toc-real-world-3"><span
class="toc-section-number">4.7</span> Real world</a></li>
<li><a href="#exercises-3" id="toc-exercises-3"><span
class="toc-section-number">4.8</span> Exercises</a></li>
</ul></li>
<li><a href="#CH:INTERRUPT" id="toc-CH:INTERRUPT"><span
class="toc-section-number">5</span> Interrupts and device drivers</a>
<ul>
<li><a href="#code-console-input" id="toc-code-console-input"><span
class="toc-section-number">5.1</span> Code: Console input</a></li>
<li><a href="#code-console-output" id="toc-code-console-output"><span
class="toc-section-number">5.2</span> Code: Console output</a></li>
<li><a href="#concurrency-in-drivers"
id="toc-concurrency-in-drivers"><span
class="toc-section-number">5.3</span> Concurrency in drivers</a></li>
<li><a href="#timer-interrupts" id="toc-timer-interrupts"><span
class="toc-section-number">5.4</span> Timer interrupts</a></li>
<li><a href="#real-world-4" id="toc-real-world-4"><span
class="toc-section-number">5.5</span> Real world</a></li>
<li><a href="#exercises-4" id="toc-exercises-4"><span
class="toc-section-number">5.6</span> Exercises</a></li>
</ul></li>
<li><a href="#CH:LOCK" id="toc-CH:LOCK"><span
class="toc-section-number">6</span> Locking</a>
<ul>
<li><a href="#races" id="toc-races"><span
class="toc-section-number">6.1</span> Races</a></li>
<li><a href="#code-locks" id="toc-code-locks"><span
class="toc-section-number">6.2</span> Code: Locks</a></li>
<li><a href="#code-using-locks" id="toc-code-using-locks"><span
class="toc-section-number">6.3</span> Code: Using locks</a></li>
<li><a href="#deadlock-and-lock-ordering"
id="toc-deadlock-and-lock-ordering"><span
class="toc-section-number">6.4</span> Deadlock and lock
ordering</a></li>
<li><a href="#re-entrant-locks" id="toc-re-entrant-locks"><span
class="toc-section-number">6.5</span> Re-entrant locks</a></li>
<li><a href="#s:lockinter" id="toc-s:lockinter"><span
class="toc-section-number">6.6</span> Locks and interrupt
handlers</a></li>
<li><a href="#instruction-and-memory-ordering"
id="toc-instruction-and-memory-ordering"><span
class="toc-section-number">6.7</span> Instruction and memory
ordering</a></li>
<li><a href="#sleep-locks" id="toc-sleep-locks"><span
class="toc-section-number">6.8</span> Sleep locks</a></li>
<li><a href="#real-world-5" id="toc-real-world-5"><span
class="toc-section-number">6.9</span> Real world</a></li>
<li><a href="#exercises-5" id="toc-exercises-5"><span
class="toc-section-number">6.10</span> Exercises</a></li>
</ul></li>
<li><a href="#CH:SCHED" id="toc-CH:SCHED"><span
class="toc-section-number">7</span> Scheduling</a>
<ul>
<li><a href="#multiplexing" id="toc-multiplexing"><span
class="toc-section-number">7.1</span> Multiplexing</a></li>
<li><a href="#code-context-switching"
id="toc-code-context-switching"><span
class="toc-section-number">7.2</span> Code: Context switching</a></li>
<li><a href="#code-scheduling" id="toc-code-scheduling"><span
class="toc-section-number">7.3</span> Code: Scheduling</a></li>
<li><a href="#code-mycpu-and-myproc"
id="toc-code-mycpu-and-myproc"><span
class="toc-section-number">7.4</span> Code: mycpu and myproc</a></li>
<li><a href="#sec:sleep" id="toc-sec:sleep"><span
class="toc-section-number">7.5</span> Sleep and wakeup</a></li>
<li><a href="#code-sleep-and-wakeup"
id="toc-code-sleep-and-wakeup"><span
class="toc-section-number">7.6</span> Code: Sleep and wakeup</a></li>
<li><a href="#code-pipes" id="toc-code-pipes"><span
class="toc-section-number">7.7</span> Code: Pipes</a></li>
<li><a href="#code-wait-exit-and-kill"
id="toc-code-wait-exit-and-kill"><span
class="toc-section-number">7.8</span> Code: Wait, exit, and
kill</a></li>
<li><a href="#process-locking" id="toc-process-locking"><span
class="toc-section-number">7.9</span> Process Locking</a></li>
<li><a href="#real-world-6" id="toc-real-world-6"><span
class="toc-section-number">7.10</span> Real world</a></li>
<li><a href="#exercises-6" id="toc-exercises-6"><span
class="toc-section-number">7.11</span> Exercises</a></li>
</ul></li>
<li><a href="#CH:FS" id="toc-CH:FS"><span
class="toc-section-number">8</span> File system</a>
<ul>
<li><a href="#overview" id="toc-overview"><span
class="toc-section-number">8.1</span> Overview</a></li>
<li><a href="#s:bcache" id="toc-s:bcache"><span
class="toc-section-number">8.2</span> Buffer cache layer</a></li>
<li><a href="#code-buffer-cache" id="toc-code-buffer-cache"><span
class="toc-section-number">8.3</span> Code: Buffer cache</a></li>
<li><a href="#logging-layer" id="toc-logging-layer"><span
class="toc-section-number">8.4</span> Logging layer</a></li>
<li><a href="#log-design" id="toc-log-design"><span
class="toc-section-number">8.5</span> Log design</a></li>
<li><a href="#code-logging" id="toc-code-logging"><span
class="toc-section-number">8.6</span> Code: logging</a></li>
<li><a href="#code-block-allocator" id="toc-code-block-allocator"><span
class="toc-section-number">8.7</span> Code: Block allocator</a></li>
<li><a href="#inode-layer" id="toc-inode-layer"><span
class="toc-section-number">8.8</span> Inode layer</a></li>
<li><a href="#code-inodes" id="toc-code-inodes"><span
class="toc-section-number">8.9</span> Code: Inodes</a></li>
<li><a href="#code-inode-content" id="toc-code-inode-content"><span
class="toc-section-number">8.10</span> Code: Inode content</a></li>
<li><a href="#code-directory-layer" id="toc-code-directory-layer"><span
class="toc-section-number">8.11</span> Code: directory layer</a></li>
<li><a href="#code-path-names" id="toc-code-path-names"><span
class="toc-section-number">8.12</span> Code: Path names</a></li>
<li><a href="#file-descriptor-layer"
id="toc-file-descriptor-layer"><span
class="toc-section-number">8.13</span> File descriptor layer</a></li>
<li><a href="#code-system-calls" id="toc-code-system-calls"><span
class="toc-section-number">8.14</span> Code: System calls</a></li>
<li><a href="#real-world-7" id="toc-real-world-7"><span
class="toc-section-number">8.15</span> Real world</a></li>
<li><a href="#exercises-7" id="toc-exercises-7"><span
class="toc-section-number">8.16</span> Exercises</a></li>
</ul></li>
<li><a href="#CH:LOCK2" id="toc-CH:LOCK2"><span
class="toc-section-number">9</span> Concurrency revisited</a>
<ul>
<li><a href="#locking-patterns" id="toc-locking-patterns"><span
class="toc-section-number">9.1</span> Locking patterns</a></li>
<li><a href="#lock-like-patterns" id="toc-lock-like-patterns"><span
class="toc-section-number">9.2</span> Lock-like patterns</a></li>
<li><a href="#no-locks-at-all" id="toc-no-locks-at-all"><span
class="toc-section-number">9.3</span> No locks at all</a></li>
<li><a href="#parallelism" id="toc-parallelism"><span
class="toc-section-number">9.4</span> Parallelism</a></li>
<li><a href="#exercises-8" id="toc-exercises-8"><span
class="toc-section-number">9.5</span> Exercises</a></li>
</ul></li>
<li><a href="#CH:SUM" id="toc-CH:SUM"><span
class="toc-section-number">10</span> Summary</a></li>
<li><a href="#index" id="toc-index"><span
class="toc-section-number">11</span> Index</a></li>
</ul>
</nav>
<section id="foreword-and-acknowledgments" class="level1 unnumbered">
<h1 class="unnumbered">Foreword and acknowledgments</h1>
<p>This is a draft text intended for a class on operating systems. It
explains the main concepts of operating systems by studying an example
kernel, named xv6. Xv6 is modeled on Dennis Ritchie’s and Ken Thompson’s
Unix Version 6 (v6) <span class="citation" data-cites="unix">(<a
href="#ref-unix" role="doc-biblioref">Ritchie and Thompson
1974</a>)</span>. Xv6 loosely follows the structure and style of v6, but
is implemented in ANSI C <span class="citation"
data-cites="kernighan">(<a href="#ref-kernighan"
role="doc-biblioref">Kernighan 1988</a>)</span> for a multi-core
RISC-V <span class="citation" data-cites="riscv">(<a href="#ref-riscv"
role="doc-biblioref">Patterson and Waterman 2017</a>)</span>.</p>
<p>This text should be read along with the source code for xv6, an
approach inspired by John Lions’ Commentary on UNIX 6th Edition <span
class="citation" data-cites="lions">(<a href="#ref-lions"
role="doc-biblioref">Lions 2000</a>)</span>. See <a
href="https://pdos.csail.mit.edu/6.1810"
class="uri">https://pdos.csail.mit.edu/6.1810</a> for pointers to
on-line resources for v6 and xv6, including several lab assignments
using xv6.</p>
<p>We have used this text in 6.828 and 6.1810, the operating system
classes at MIT. We thank the faculty, teaching assistants, and students
of those classes who have all directly or indirectly contributed to xv6.
In particular, we would like to thank Adam Belay, Austin Clements, and
Nickolai Zeldovich. Finally, we would like to thank people who emailed
us bugs in the text or suggestions for improvements: Abutalib Aghayev,
Sebastian Boehm, brandb97, Anton Burtsev, Raphael Carvalho, Tej Chajed,
Rasit Eskicioglu, Color Fuzzy, Wojciech Gac, Giuseppe, Tao Guo, Haibo
Hao, Naoki Hayama, Chris Henderson, Robert Hilderman, Eden Hochbaum,
Wolfgang Keller, Henry Laih, Jin Li, Austin Liew, Pavan Maddamsetti,
Jacek Masiulaniec, Michael McConville, m3hm00d, miguelgvieira, Mark
Morrissey, Muhammed Mourad, Harry Pan, Harry Porter, Siyuan Qian, Askar
Safin, Salman Shah, Huang Sha, Vikram Shenoy, Adeodato Simó, Ruslan
Savchenko, Pawel Szczurko, Warren Toomey, tyfkda, tzerbib, Vanush
Vaswani, Xi Wang, and Zou Chang Wei, Sam Whitlock, LucyShawYang, and
Meng Zhou</p>
<p>If you spot errors or have suggestions for improvement, please send
email to Frans Kaashoek and Robert Morris
(kaashoek,rtm@csail.mit.edu).</p>
</section>
<section id="CH:UNIX" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span>
Operating system interfaces</h1>
<p>The job of an operating system is to share a computer among multiple
programs and to provide a more useful set of services than the hardware
alone supports. An operating system manages and abstracts the low-level
hardware, so that, for example, a word processor need not concern itself
with which type of disk hardware is being used. An operating system
shares the hardware among multiple programs so that they run (or appear
to run) at the same time. Finally, operating systems provide controlled
ways for programs to interact, so that they can share data or work
together.</p>
<p>An operating system provides services to user programs through an
interface. Designing a good interface turns out to be difficult. On the
one hand, we would like the interface to be simple and narrow because
that makes it easier to get the implementation right. On the other hand,
we may be tempted to offer many sophisticated features to applications.
The trick in resolving this tension is to design interfaces that rely on
a few mechanisms that can be combined to provide much generality.</p>
<p>This book uses a single operating system as a concrete example to
illustrate operating system concepts. That operating system, xv6,
provides the basic interfaces introduced by Ken Thompson and Dennis
Ritchie’s Unix operating system <span class="citation"
data-cites="unix">(<a href="#ref-unix" role="doc-biblioref">Ritchie and
Thompson 1974</a>)</span>, as well as mimicking Unix’s internal design.
Unix provides a narrow interface whose mechanisms combine well, offering
a surprising degree of generality. This interface has been so successful
that modern operating systems—BSD, Linux, macOS, Solaris, and even, to a
lesser extent, Microsoft Windows—have Unix-like interfaces.
Understanding xv6 is a good start toward understanding any of these
systems and many others.</p>
<p>As Figure <a href="#fig:os" data-reference-type="ref"
data-reference="fig:os">1.1</a> shows, xv6 takes the traditional form of
a <em><span id="kernel_1">kernel</span></em>, a special program that
provides services to running programs. Each running program, called a
<em><span id="process_1">process</span></em>, has memory containing
instructions, data, and a stack. The instructions implement the
program’s computation. The data are the variables on which the
computation acts. The stack organizes the program’s procedure calls. A
given computer typically has many processes but only a single
kernel.</p>
<p>When a process needs to invoke a kernel service, it invokes a
<em><span id="system_call_1">system call</span></em>, one of the calls
in the operating system’s interface. The system call enters the kernel;
the kernel performs the service and returns. Thus a process alternates
between executing in <em><span id="user_space_1">user space</span></em>
and <em><span id="kernel_space_1">kernel space</span></em>.</p>
<p>As described in detail in subsequent chapters, the kernel uses the
hardware protection mechanisms provided by a CPU<a href="#fn1"
class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> to
ensure that each process executing in user space can access only its own
memory. The kernel executes with the hardware privileges required to
implement these protections; user programs execute without those
privileges. When a user program invokes a system call, the hardware
raises the privilege level and starts executing a pre-arranged function
in the kernel.</p>
<figure id="fig:os">
<img src="fig/os.svg" />
<figcaption>A kernel and two user processes.</figcaption>
</figure>
<p>The collection of system calls that a kernel provides is the
interface that user programs see. The xv6 kernel provides a subset of
the services and system calls that Unix kernels traditionally offer.
Figure <a href="#fig:api" data-reference-type="ref"
data-reference="fig:api">1.2</a> lists all of xv6’s system calls.</p>
<p>The rest of this chapter outlines xv6’s services—processes, memory,
file descriptors, pipes, and a file system—and illustrates them with
code snippets and discussions of how the <em><span
id="shell_1">shell</span></em>, Unix’s command-line user interface, uses
them. The shell’s use of system calls illustrates how carefully they
have been designed.</p>
<p>The shell is an ordinary program that reads commands from the user
and executes them. The fact that the shell is a user program, and not
part of the kernel, illustrates the power of the system call interface:
there is nothing special about the shell. It also means that the shell
is easy to replace; as a result, modern Unix systems have a variety of
shells to choose from, each with its own user interface and scripting
features. The xv6 shell is a simple implementation of the essence of the
Unix Bourne shell. Its implementation can be found at <a
href="xv6-riscv-src/user/sh.c#L1"><span>(user/sh.c:1)</span></a>.</p>
<section id="processes-and-memory" class="level2" data-number="1.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span>
Processes and memory</h2>
<p>An xv6 process consists of user-space memory (instructions, data, and
stack) and per-process state private to the kernel. Xv6 <em><span
id="time-share_1">time-share</span></em>s processes: it transparently
switches the available CPUs among the set of processes waiting to
execute. When a process is not executing, xv6 saves the process’s CPU
registers, restoring them when it next runs the process. The kernel
associates a process identifier, or <code id="PID_1">PID</code>, with
each process.</p>
<figure id="fig:api">
<table>
<thead>
<tr class="header">
<th style="text-align: left;"><span><strong>System
call</strong></span></th>
<th
style="text-align: left;"><span><strong>Description</strong></span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">int fork()</td>
<td style="text-align: left;">Create a process, return child’s PID.</td>
</tr>
<tr class="even">
<td style="text-align: left;">int exit(int status)</td>
<td style="text-align: left;">Terminate the current process; status
reported to wait(). No return.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">int wait(int *status)</td>
<td style="text-align: left;">Wait for a child to exit; exit status in
*status; returns child PID.</td>
</tr>
<tr class="even">
<td style="text-align: left;">int kill(int pid)</td>
<td style="text-align: left;">Terminate process PID. Returns 0, or -1
for error.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">int getpid()</td>
<td style="text-align: left;">Return the current process’s PID.</td>
</tr>
<tr class="even">
<td style="text-align: left;">int sleep(int n)</td>
<td style="text-align: left;">Pause for n clock ticks.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">int exec(char *file, char *argv[])</td>
<td style="text-align: left;">Load a file and execute it with arguments;
only returns if error.</td>
</tr>
<tr class="even">
<td style="text-align: left;">char *sbrk(int n)</td>
<td style="text-align: left;">Grow process’s memory by n bytes. Returns
start of new memory.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">int open(char *file, int flags)</td>
<td style="text-align: left;">Open a file; flags indicate read/write;
returns an fd (file descriptor).</td>
</tr>
<tr class="even">
<td style="text-align: left;">int write(int fd, char *buf, int n)</td>
<td style="text-align: left;">Write n bytes from buf to file descriptor
fd; returns n.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">int read(int fd, char *buf, int n)</td>
<td style="text-align: left;">Read n bytes into buf; returns number
read; or 0 if end of file.</td>
</tr>
<tr class="even">
<td style="text-align: left;">int close(int fd)</td>
<td style="text-align: left;">Release open file fd.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">int dup(int fd)</td>
<td style="text-align: left;">Return a new file descriptor referring to
the same file as fd.</td>
</tr>
<tr class="even">
<td style="text-align: left;">int pipe(int p[])</td>
<td style="text-align: left;">Create a pipe, put read/write file
descriptors in p[0] and p[1].</td>
</tr>
<tr class="odd">
<td style="text-align: left;">int chdir(char *dir)</td>
<td style="text-align: left;">Change the current directory.</td>
</tr>
<tr class="even">
<td style="text-align: left;">int mkdir(char *dir)</td>
<td style="text-align: left;">Create a new directory.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">int mknod(char *file, int, int)</td>
<td style="text-align: left;">Create a device file.</td>
</tr>
<tr class="even">
<td style="text-align: left;">int fstat(int fd, struct stat *st)</td>
<td style="text-align: left;">Place info about an open file into
*st.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">int stat(char *file, struct stat *st)</td>
<td style="text-align: left;">Place info about a named file into
*st.</td>
</tr>
<tr class="even">
<td style="text-align: left;">int link(char *file1, char *file2)</td>
<td style="text-align: left;">Create another name (file2) for the file
file1.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">int unlink(char *file)</td>
<td style="text-align: left;">Remove a file.</td>
</tr>
</tbody>
</table>
<figcaption>Xv6 system calls. If not otherwise stated, these calls
return 0 for no error, and -1 if there’s an error.</figcaption>
</figure>
<p>A process may create a new process using the <code
id="fork_1">fork</code> system call. <code>fork</code> gives the new
process an exact copy of the calling process’s memory, both instructions
and data. <code>fork</code> returns in both the original and new
processes. In the original process, <code>fork</code> returns the new
process’s PID. In the new process, <code>fork</code> returns zero. The
original and new processes are often called the <em><span
id="parent_1">parent</span></em> and <em><span
id="child_1">child</span></em>.</p>
<p>For example, consider the following program fragment written in the C
programming language <span class="citation" data-cites="kernighan">(<a
href="#ref-kernighan" role="doc-biblioref">Kernighan
1988</a>)</span>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode C"><code class="sourceCode c"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> pid <span class="op">=</span> fork<span class="op">();</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span><span class="op">(</span>pid <span class="op">&gt;</span> <span class="dv">0</span><span class="op">){</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  printf<span class="op">(</span><span class="st">&quot;parent: child=</span><span class="sc">%d\n</span><span class="st">&quot;</span><span class="op">,</span> pid<span class="op">);</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  pid <span class="op">=</span> wait<span class="op">((</span><span class="dt">int</span> <span class="op">*)</span> <span class="dv">0</span><span class="op">);</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  printf<span class="op">(</span><span class="st">&quot;child </span><span class="sc">%d</span><span class="st"> is done</span><span class="sc">\n</span><span class="st">&quot;</span><span class="op">,</span> pid<span class="op">);</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="op">}</span> <span class="cf">else</span> <span class="cf">if</span><span class="op">(</span>pid <span class="op">==</span> <span class="dv">0</span><span class="op">){</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  printf<span class="op">(</span><span class="st">&quot;child: exiting</span><span class="sc">\n</span><span class="st">&quot;</span><span class="op">);</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  exit<span class="op">(</span><span class="dv">0</span><span class="op">);</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="op">}</span> <span class="cf">else</span> <span class="op">{</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  printf<span class="op">(</span><span class="st">&quot;fork error</span><span class="sc">\n</span><span class="st">&quot;</span><span class="op">);</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>The <code id="exit_1">exit</code> system call causes the calling
process to stop executing and to release resources such as memory and
open files. Exit takes an integer status argument, conventionally 0 to
indicate success and 1 to indicate failure. The <code
id="wait_1">wait</code> system call returns the PID of an exited (or
killed) child of the current process and copies the exit status of the
child to the address passed to wait; if none of the caller’s children
has exited, <code id="wait_2">wait</code> waits for one to do so. If the
caller has no children, <code>wait</code> immediately returns -1. If the
parent doesn’t care about the exit status of a child, it can pass a 0
address to <code>wait</code>.</p>
<p>In the example, the output lines</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode C"><code class="sourceCode c"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>parent<span class="op">:</span> child<span class="op">=</span><span class="dv">1234</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>child<span class="op">:</span> exiting</span></code></pre></div>
<p>might come out in either order (or even intermixed), depending on
whether the parent or child gets to its <code
id="printf_1">printf</code> call first. After the child exits, the
parent’s <code id="wait_3">wait</code> returns, causing the parent to
print</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode C"><code class="sourceCode c"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>parent<span class="op">:</span> child <span class="dv">1234</span> is done</span></code></pre></div>
<p>Although the child has the same memory contents as the parent
initially, the parent and child are executing with separate memory and
separate registers: changing a variable in one does not affect the
other. For example, when the return value of <code>wait</code> is stored
into <code>pid</code> in the parent process, it doesn’t change the
variable <code>pid</code> in the child. The value of <code>pid</code> in
the child will still be zero.</p>
<p>The <code id="exec_1">exec</code> system call replaces the calling
process’s memory with a new memory image loaded from a file stored in
the file system. The file must have a particular format, which specifies
which part of the file holds instructions, which part is data, at which
instruction to start, etc. Xv6 uses the ELF format, which Chapter <a
href="#CH:MEM" data-reference-type="ref" data-reference="CH:MEM">3</a>
discusses in more detail. Usually the file is the result of compiling a
program’s source code. When <code id="exec_2">exec</code> succeeds, it
does not return to the calling program; instead, the instructions loaded
from the file start executing at the entry point declared in the ELF
header. <code>exec</code> takes two arguments: the name of the file
containing the executable and an array of string arguments. For
example:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode C"><code class="sourceCode c"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="dt">char</span> <span class="op">*</span>argv<span class="op">[</span><span class="dv">3</span><span class="op">];</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>argv<span class="op">[</span><span class="dv">0</span><span class="op">]</span> <span class="op">=</span> <span class="st">&quot;echo&quot;</span><span class="op">;</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>argv<span class="op">[</span><span class="dv">1</span><span class="op">]</span> <span class="op">=</span> <span class="st">&quot;hello&quot;</span><span class="op">;</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>argv<span class="op">[</span><span class="dv">2</span><span class="op">]</span> <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>exec<span class="op">(</span><span class="st">&quot;/bin/echo&quot;</span><span class="op">,</span> argv<span class="op">);</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>printf<span class="op">(</span><span class="st">&quot;exec error</span><span class="sc">\n</span><span class="st">&quot;</span><span class="op">);</span></span></code></pre></div>
<p>This fragment replaces the calling program with an instance of the
program <code>/bin/echo</code> running with the argument list
<code>echo</code> <code>hello</code>. Most programs ignore the first
element of the argument array, which is conventionally the name of the
program.</p>
<p>The xv6 shell uses the above calls to run programs on behalf of
users. The main structure of the shell is simple; see <code>main</code>
<a href="xv6-riscv-src/user/sh.c#L146"><span>(user/sh.c:146)</span></a>.
The main loop reads a line of input from the user with <code
id="getcmd_1">getcmd</code>. Then it calls <code>fork</code>, which
creates a copy of the shell process. The parent calls <code>wait</code>,
while the child runs the command. For example, if the user had typed
“<code>echo hello</code>” to the shell, <code>runcmd</code> would have
been called with “<code>echo hello</code>” as the argument.
<code>runcmd</code> <a
href="xv6-riscv-src/user/sh.c#L55"><span>(user/sh.c:55)</span></a> runs
the actual command. For “<code>echo hello</code>”, it would call
<code>exec</code> <a
href="xv6-riscv-src/user/sh.c#L79"><span>(user/sh.c:79)</span></a>. If
<code>exec</code> succeeds then the child will execute instructions from
<code>echo</code> instead of <code>runcmd</code>. At some point
<code>echo</code> will call <code>exit</code>, which will cause the
parent to return from <code>wait</code> in <code>main</code> <a
href="xv6-riscv-src/user/sh.c#L146"><span>(user/sh.c:146)</span></a>.</p>
<p>You might wonder why <code id="fork_2">fork</code> and <code
id="exec_3">exec</code> are not combined in a single call; we will see
later that the shell exploits the separation in its implementation of
I/O redirection. To avoid the wastefulness of creating a duplicate
process and then immediately replacing it (with <code>exec</code>),
operating kernels optimize the implementation of <code>fork</code> for
this use case by using virtual memory techniques such as copy-on-write
(see Section <a href="#sec:pagefaults" data-reference-type="ref"
data-reference="sec:pagefaults">4.6</a>).</p>
<p>Xv6 allocates most user-space memory implicitly: <code
id="fork_3">fork</code> allocates the memory required for the child’s
copy of the parent’s memory, and <code id="exec_4">exec</code> allocates
enough memory to hold the executable file. A process that needs more
memory at run-time (perhaps for <code id="malloc_1">malloc</code>) can
call <code>sbrk(n)</code> to grow its data memory by <code>n</code>
bytes; <code id="sbrk_1">sbrk</code> returns the location of the new
memory.</p>
</section>
<section id="io-and-file-descriptors" class="level2" data-number="1.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span> I/O
and File descriptors</h2>
<p>A <em><span id="file_descriptor_1">file descriptor</span></em> is a
small integer representing a kernel-managed object that a process may
read from or write to. A process may obtain a file descriptor by opening
a file, directory, or device, or by creating a pipe, or by duplicating
an existing descriptor. For simplicity we’ll often refer to the object a
file descriptor refers to as a “file”; the file descriptor interface
abstracts away the differences between files, pipes, and devices, making
them all look like streams of bytes. We’ll refer to input and output as
<em><span id="I/O_1">I/O</span></em>.</p>
<p>Internally, the xv6 kernel uses the file descriptor as an index into
a per-process table, so that every process has a private space of file
descriptors starting at zero. By convention, a process reads from file
descriptor 0 (standard input), writes output to file descriptor 1
(standard output), and writes error messages to file descriptor 2
(standard error). As we will see, the shell exploits the convention to
implement I/O redirection and pipelines. The shell ensures that it
always has three file descriptors open <a
href="xv6-riscv-src/user/sh.c#L152"><span>(user/sh.c:152)</span></a>,
which are by default file descriptors for the console.</p>
<p>The <code>read</code> and <code>write</code> system calls read bytes
from and write bytes to open files named by file descriptors. The call
<code>read(fd</code>, <code>buf</code>, <code>n)</code> reads at most
<code>n</code> bytes from the file descriptor <code>fd</code>, copies
them into <code>buf</code>, and returns the number of bytes read. Each
file descriptor that refers to a file has an offset associated with it.
<code>read</code> reads data from the current file offset and then
advances that offset by the number of bytes read: a subsequent
<code>read</code> will return the bytes following the ones returned by
the first <code>read</code>. When there are no more bytes to read,
<code>read</code> returns zero to indicate the end of the file.</p>
<p>The call <code>write(fd</code>, <code>buf</code>, <code>n)</code>
writes <code>n</code> bytes from <code>buf</code> to the file descriptor
<code>fd</code> and returns the number of bytes written. Fewer than
<code>n</code> bytes are written only when an error occurs. Like
<code>read</code>, <code>write</code> writes data at the current file
offset and then advances that offset by the number of bytes written:
each <code>write</code> picks up where the previous one left off.</p>
<p>The following program fragment (which forms the essence of the
program <code>cat</code>) copies data from its standard input to its
standard output. If an error occurs, it writes a message to the standard
error.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode C"><code class="sourceCode c"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="dt">char</span> buf<span class="op">[</span><span class="dv">512</span><span class="op">];</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> n<span class="op">;</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span><span class="op">(;;){</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>  n <span class="op">=</span> read<span class="op">(</span><span class="dv">0</span><span class="op">,</span> buf<span class="op">,</span> <span class="kw">sizeof</span> buf<span class="op">);</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span><span class="op">(</span>n <span class="op">==</span> <span class="dv">0</span><span class="op">)</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">break</span><span class="op">;</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span><span class="op">(</span>n <span class="op">&lt;</span> <span class="dv">0</span><span class="op">){</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    fprintf<span class="op">(</span><span class="dv">2</span><span class="op">,</span> <span class="st">&quot;read error</span><span class="sc">\n</span><span class="st">&quot;</span><span class="op">);</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    exit<span class="op">(</span><span class="dv">1</span><span class="op">);</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span><span class="op">(</span>write<span class="op">(</span><span class="dv">1</span><span class="op">,</span> buf<span class="op">,</span> n<span class="op">)</span> <span class="op">!=</span> n<span class="op">){</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    fprintf<span class="op">(</span><span class="dv">2</span><span class="op">,</span> <span class="st">&quot;write error</span><span class="sc">\n</span><span class="st">&quot;</span><span class="op">);</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    exit<span class="op">(</span><span class="dv">1</span><span class="op">);</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>The important thing to note in the code fragment is that
<code>cat</code> doesn’t know whether it is reading from a file,
console, or a pipe. Similarly <code>cat</code> doesn’t know whether it
is printing to a console, a file, or whatever. The use of file
descriptors and the convention that file descriptor 0 is input and file
descriptor 1 is output allows a simple implementation of
<code>cat</code>.</p>
<p>The <code>close</code> system call releases a file descriptor, making
it free for reuse by a future <code>open</code>, <code>pipe</code>, or
<code>dup</code> system call (see below). A newly allocated file
descriptor is always the lowest-numbered unused descriptor of the
current process.</p>
<p>File descriptors and <code id="fork_4">fork</code> interact to make
I/O redirection easy to implement. <code>fork</code> copies the parent’s
file descriptor table along with its memory, so that the child starts
with exactly the same open files as the parent. The system call <code
id="exec_5">exec</code> replaces the calling process’s memory but
preserves its file table. This behavior allows the shell to implement
<em><span id="I/O_redirection_1">I/O redirection</span></em> by forking,
re-opening chosen file descriptors in the child, and then calling
<code>exec</code> to run the new program. Here is a simplified version
of the code a shell runs for the command <code>cat</code>
<code>&lt;</code> <code>input.txt</code>:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode C"><code class="sourceCode c"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="dt">char</span> <span class="op">*</span>argv<span class="op">[</span><span class="dv">2</span><span class="op">];</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>argv<span class="op">[</span><span class="dv">0</span><span class="op">]</span> <span class="op">=</span> <span class="st">&quot;cat&quot;</span><span class="op">;</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>argv<span class="op">[</span><span class="dv">1</span><span class="op">]</span> <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span><span class="op">(</span>fork<span class="op">()</span> <span class="op">==</span> <span class="dv">0</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>  close<span class="op">(</span><span class="dv">0</span><span class="op">);</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>  open<span class="op">(</span><span class="st">&quot;input.txt&quot;</span><span class="op">,</span> O_RDONLY<span class="op">);</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>  exec<span class="op">(</span><span class="st">&quot;cat&quot;</span><span class="op">,</span> argv<span class="op">);</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>After the child closes file descriptor 0, <code>open</code> is
guaranteed to use that file descriptor for the newly opened
<code>input.txt</code>: 0 will be the smallest available file
descriptor. <code>cat</code> then executes with file descriptor 0
(standard input) referring to <code>input.txt</code>. The parent
process’s file descriptors are not changed by this sequence, since it
modifies only the child’s descriptors.</p>
<p>The code for I/O redirection in the xv6 shell works in exactly this
way <a
href="xv6-riscv-src/user/sh.c#L83"><span>(user/sh.c:83)</span></a>.
Recall that at this point in the code the shell has already forked the
child shell and that <code>runcmd</code> will call <code>exec</code> to
load the new program.</p>
<p>The second argument to <code>open</code> consists of a set of flags,
expressed as bits, that control what <code>open</code> does. The
possible values are defined in the file control (fcntl) header <a
href="xv6-riscv-src/kernel/fcntl.h#L1-L5">(kernel/fcntl.h:1-5)</a>:
<code>O_RDONLY</code>, <code>O_WRONLY</code>, <code>O_RDWR</code>,
<code>O_CREATE</code>, and <code>O_TRUNC</code>, which instruct
<code>open</code> to open the file for reading, or for writing, or for
both reading and writing, to create the file if it doesn’t exist, and to
truncate the file to zero length.</p>
<p>Now it should be clear why it is helpful that <code>fork</code> and
<code>exec</code> are separate calls: between the two, the shell has a
chance to redirect the child’s I/O without disturbing the I/O setup of
the main shell. One could instead imagine a hypothetical combined
<code>forkexec</code> system call, but the options for doing I/O
redirection with such a call seem awkward. The shell could modify its
own I/O setup before calling <code>forkexec</code> (and then un-do those
modifications); or <code>forkexec</code> could take instructions for I/O
redirection as arguments; or (least attractively) every program like
<code>cat</code> could be taught to do its own I/O redirection.</p>
<p>Although <code>fork</code> copies the file descriptor table, each
underlying file offset is shared between parent and child. Consider this
example:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode C"><code class="sourceCode c"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span><span class="op">(</span>fork<span class="op">()</span> <span class="op">==</span> <span class="dv">0</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  write<span class="op">(</span><span class="dv">1</span><span class="op">,</span> <span class="st">&quot;hello &quot;</span><span class="op">,</span> <span class="dv">6</span><span class="op">);</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  exit<span class="op">(</span><span class="dv">0</span><span class="op">);</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="op">}</span> <span class="cf">else</span> <span class="op">{</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>  wait<span class="op">(</span><span class="dv">0</span><span class="op">);</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  write<span class="op">(</span><span class="dv">1</span><span class="op">,</span> <span class="st">&quot;world</span><span class="sc">\n</span><span class="st">&quot;</span><span class="op">,</span> <span class="dv">6</span><span class="op">);</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>At the end of this fragment, the file attached to file descriptor 1
will contain the data <code>hello</code> <code>world</code>. The
<code>write</code> in the parent (which, thanks to <code>wait</code>,
runs only after the child is done) picks up where the child’s
<code>write</code> left off. This behavior helps produce sequential
output from sequences of shell commands, like <code>(echo</code>
<code>hello</code>; <code>echo</code> <code>world)</code>
<code>&gt;output.txt</code>.</p>
<p>The <code>dup</code> system call duplicates an existing file
descriptor, returning a new one that refers to the same underlying I/O
object. Both file descriptors share an offset, just as the file
descriptors duplicated by <code>fork</code> do. This is another way to
write <code>hello</code> <code>world</code> into a file:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode C"><code class="sourceCode c"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>fd <span class="op">=</span> dup<span class="op">(</span><span class="dv">1</span><span class="op">);</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>write<span class="op">(</span><span class="dv">1</span><span class="op">,</span> <span class="st">&quot;hello &quot;</span><span class="op">,</span> <span class="dv">6</span><span class="op">);</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>write<span class="op">(</span>fd<span class="op">,</span> <span class="st">&quot;world</span><span class="sc">\n</span><span class="st">&quot;</span><span class="op">,</span> <span class="dv">6</span><span class="op">);</span></span></code></pre></div>
<p>Two file descriptors share an offset if they were derived from the
same original file descriptor by a sequence of <code>fork</code> and
<code>dup</code> calls. Otherwise file descriptors do not share offsets,
even if they resulted from <code>open</code> calls for the same file.
<code>dup</code> allows shells to implement commands like this:
<code>ls</code> <code>existing-file</code>
<code>non-existing-file</code> <code>&gt;</code> <code>tmp1</code>
<code>2&gt;&amp;1</code>. The <code>2&gt;&amp;1</code> tells the shell
to give the command a file descriptor 2 that is a duplicate of
descriptor 1. Both the name of the existing file and the error message
for the non-existing file will show up in the file <code>tmp1</code>.
The xv6 shell doesn’t support I/O redirection for the error file
descriptor, but now you know how to implement it.</p>
<p>File descriptors are a powerful abstraction, because they hide the
details of what they are connected to: a process writing to file
descriptor 1 may be writing to a file, to a device like the console, or
to a pipe.</p>
</section>
<section id="pipes" class="level2" data-number="1.3">
<h2 data-number="1.3"><span class="header-section-number">1.3</span>
Pipes</h2>
<p>A <em><span id="pipe_1">pipe</span></em> is a small kernel buffer
exposed to processes as a pair of file descriptors, one for reading and
one for writing. Writing data to one end of the pipe makes that data
available for reading from the other end of the pipe. Pipes provide a
way for processes to communicate.</p>
<p>The following example code runs the program <code>wc</code> with
standard input connected to the read end of a pipe.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode C"><code class="sourceCode c"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> p<span class="op">[</span><span class="dv">2</span><span class="op">];</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="dt">char</span> <span class="op">*</span>argv<span class="op">[</span><span class="dv">2</span><span class="op">];</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>argv<span class="op">[</span><span class="dv">0</span><span class="op">]</span> <span class="op">=</span> <span class="st">&quot;wc&quot;</span><span class="op">;</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>argv<span class="op">[</span><span class="dv">1</span><span class="op">]</span> <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>pipe<span class="op">(</span>p<span class="op">);</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span><span class="op">(</span>fork<span class="op">()</span> <span class="op">==</span> <span class="dv">0</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>  close<span class="op">(</span><span class="dv">0</span><span class="op">);</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>  dup<span class="op">(</span>p<span class="op">[</span><span class="dv">0</span><span class="op">]);</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>  close<span class="op">(</span>p<span class="op">[</span><span class="dv">0</span><span class="op">]);</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>  close<span class="op">(</span>p<span class="op">[</span><span class="dv">1</span><span class="op">]);</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>  exec<span class="op">(</span><span class="st">&quot;/bin/wc&quot;</span><span class="op">,</span> argv<span class="op">);</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="op">}</span> <span class="cf">else</span> <span class="op">{</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>  close<span class="op">(</span>p<span class="op">[</span><span class="dv">0</span><span class="op">]);</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>  write<span class="op">(</span>p<span class="op">[</span><span class="dv">1</span><span class="op">],</span> <span class="st">&quot;hello world</span><span class="sc">\n</span><span class="st">&quot;</span><span class="op">,</span> <span class="dv">12</span><span class="op">);</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>  close<span class="op">(</span>p<span class="op">[</span><span class="dv">1</span><span class="op">]);</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>The program calls <code>pipe</code>, which creates a new pipe and
records the read and write file descriptors in the array <code>p</code>.
After <code>fork</code>, both parent and child have file descriptors
referring to the pipe. The child calls <code>close</code> and
<code>dup</code> to make file descriptor zero refer to the read end of
the pipe, closes the file descriptors in <code>p</code>, and calls
<code>exec</code> to run <code>wc</code>. When <code>wc</code> reads
from its standard input, it reads from the pipe. The parent closes the
read side of the pipe, writes to the pipe, and then closes the write
side.</p>
<p>If no data is available, a <code>read</code> on a pipe waits for
either data to be written or for all file descriptors referring to the
write end to be closed; in the latter case, <code>read</code> will
return 0, just as if the end of a data file had been reached. The fact
that <code>read</code> blocks until it is impossible for new data to
arrive is one reason that it’s important for the child to close the
write end of the pipe before executing <code>wc</code> above: if one of
<code>wc</code> ’s file descriptors referred to the write end of the
pipe, <code>wc</code> would never see end-of-file.</p>
<p>The xv6 shell implements pipelines such as
<code>grep fork sh.c | wc -l</code> in a manner similar to the above
code <a
href="xv6-riscv-src/user/sh.c#L101"><span>(user/sh.c:101)</span></a>.
The child process creates a pipe to connect the left end of the pipeline
with the right end. Then it calls <code>fork</code> and
<code>runcmd</code> for the left end of the pipeline and
<code>fork</code> and <code>runcmd</code> for the right end, and waits
for both to finish. The right end of the pipeline may be a command that
itself includes a pipe (e.g., <code>a</code> <code>|</code>
<code>b</code> <code>|</code> <code>c)</code>, which itself forks two
new child processes (one for <code>b</code> and one for <code>c</code>).
Thus, the shell may create a tree of processes. The leaves of this tree
are commands and the interior nodes are processes that wait until the
left and right children complete.</p>
<p>Pipes may seem no more powerful than temporary files: the
pipeline</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode C"><code class="sourceCode c"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>echo hello world <span class="op">|</span> wc</span></code></pre></div>
<p>could be implemented without pipes as</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode C"><code class="sourceCode c"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>echo hello world <span class="op">&gt;/</span>tmp<span class="op">/</span>xyz<span class="op">;</span> wc <span class="op">&lt;/</span>tmp<span class="op">/</span>xyz</span></code></pre></div>
<p>Pipes have at least three advantages over temporary files in this
situation. First, pipes automatically clean themselves up; with the file
redirection, a shell would have to be careful to remove
<code>/tmp/xyz</code> when done. Second, pipes can pass arbitrarily long
streams of data, while file redirection requires enough free space on
disk to store all the data. Third, pipes allow for parallel execution of
pipeline stages, while the file approach requires the first program to
finish before the second starts.</p>
</section>
<section id="file-system" class="level2" data-number="1.4">
<h2 data-number="1.4"><span class="header-section-number">1.4</span>
File system</h2>
<p>The xv6 file system provides data files, which contain uninterpreted
byte arrays, and directories, which contain named references to data
files and other directories. The directories form a tree, starting at a
special directory called the <em><span id="root_1">root</span></em>. A
<em><span id="path_1">path</span></em> like <code>/a/b/c</code> refers
to the file or directory named <code>c</code> inside the directory named
<code>b</code> inside the directory named <code>a</code> in the root
directory <code>/</code>. Paths that don’t begin with <code>/</code> are
evaluated relative to the calling process’s <em><span
id="current_directory_1">current directory</span></em>, which can be
changed with the <code>chdir</code> system call. Both these code
fragments open the same file (assuming all the directories involved
exist):</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode C"><code class="sourceCode c"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>chdir<span class="op">(</span><span class="st">&quot;/a&quot;</span><span class="op">);</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>chdir<span class="op">(</span><span class="st">&quot;b&quot;</span><span class="op">);</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>open<span class="op">(</span><span class="st">&quot;c&quot;</span><span class="op">,</span> O_RDONLY<span class="op">);</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>open<span class="op">(</span><span class="st">&quot;/a/b/c&quot;</span><span class="op">,</span> O_RDONLY<span class="op">);</span></span></code></pre></div>
<p>The first fragment changes the process’s current directory to
<code>/a/b</code>; the second neither refers to nor changes the
process’s current directory.</p>
<p>There are system calls to create new files and directories:
<code>mkdir</code> creates a new directory, <code>open</code> with the
<code>O_CREATE</code> flag creates a new data file, and
<code>mknod</code> creates a new device file. This example illustrates
all three:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode C"><code class="sourceCode c"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>mkdir<span class="op">(</span><span class="st">&quot;/dir&quot;</span><span class="op">);</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>fd <span class="op">=</span> open<span class="op">(</span><span class="st">&quot;/dir/file&quot;</span><span class="op">,</span> O_CREATE<span class="op">|</span>O_WRONLY<span class="op">);</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>close<span class="op">(</span>fd<span class="op">);</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>mknod<span class="op">(</span><span class="st">&quot;/console&quot;</span><span class="op">,</span> <span class="dv">1</span><span class="op">,</span> <span class="dv">1</span><span class="op">);</span></span></code></pre></div>
<p><code>mknod</code> creates a special file that refers to a device.
Associated with a device file are the major and minor device numbers
(the two arguments to <code>mknod</code>), which uniquely identify a
kernel device. When a process later opens a device file, the kernel
diverts <code>read</code> and <code>write</code> system calls to the
kernel device implementation instead of passing them to the file
system.</p>
<p>A file’s name is distinct from the file itself; the same underlying
file, called an <em><span id="inode_1">inode</span></em>, can have
multiple names, called <em><span id="links_1">links</span></em>. Each
link consists of an entry in a directory; the entry contains a file name
and a reference to an inode. An inode holds <em><span
id="metadata_1">metadata</span></em> about a file, including its type
(file or directory or device), its length, the location of the file’s
content on disk, and the number of links to a file.</p>
<p>The <code>fstat</code> system call retrieves information from the
inode that a file descriptor refers to. It fills in a
<code>struct</code> <code>stat</code>, defined in <code>stat.h</code> <a
href="xv6-riscv-src/kernel/stat.h"><span>(kernel/stat.h)</span></a>
as:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode C"><code class="sourceCode c"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#define T_DIR     </span><span class="dv">1</span><span class="pp">   </span><span class="co">// Directory</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="pp">#define T_FILE    </span><span class="dv">2</span><span class="pp">   </span><span class="co">// File</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="pp">#define T_DEVICE  </span><span class="dv">3</span><span class="pp">   </span><span class="co">// Device</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="kw">struct</span> stat <span class="op">{</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span> dev<span class="op">;</span>     <span class="co">// File system&#39;s disk device</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>  uint ino<span class="op">;</span>    <span class="co">// Inode number</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>  <span class="dt">short</span> type<span class="op">;</span>  <span class="co">// Type of file</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">short</span> nlink<span class="op">;</span> <span class="co">// Number of links to file</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>  uint64 size<span class="op">;</span> <span class="co">// Size of file in bytes</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="op">};</span></span></code></pre></div>
<p>The <code>link</code> system call creates another file system name
referring to the same inode as an existing file. This fragment creates a
new file named both <code>a</code> and <code>b</code>.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode C"><code class="sourceCode c"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>open<span class="op">(</span><span class="st">&quot;a&quot;</span><span class="op">,</span> O_CREATE<span class="op">|</span>O_WRONLY<span class="op">);</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>link<span class="op">(</span><span class="st">&quot;a&quot;</span><span class="op">,</span> <span class="st">&quot;b&quot;</span><span class="op">);</span></span></code></pre></div>
<p>Reading from or writing to <code>a</code> is the same as reading from
or writing to <code>b</code>. Each inode is identified by a unique
<em>inode</em> <em>number</em>. After the code sequence above, it is
possible to determine that <code>a</code> and <code>b</code> refer to
the same underlying contents by inspecting the result of
<code>fstat</code>: both will return the same inode number
(<code>ino</code>), and the <code>nlink</code> count will be set to
2.</p>
<p>The <code>unlink</code> system call removes a name from the file
system. The file’s inode and the disk space holding its content are only
freed when the file’s link count is zero and no file descriptors refer
to it. Thus adding</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode C"><code class="sourceCode c"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>unlink<span class="op">(</span><span class="st">&quot;a&quot;</span><span class="op">);</span></span></code></pre></div>
<p>to the last code sequence leaves the inode and file content
accessible as <code>b</code>. Furthermore,</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode C"><code class="sourceCode c"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>fd <span class="op">=</span> open<span class="op">(</span><span class="st">&quot;/tmp/xyz&quot;</span><span class="op">,</span> O_CREATE<span class="op">|</span>O_RDWR<span class="op">);</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>unlink<span class="op">(</span><span class="st">&quot;/tmp/xyz&quot;</span><span class="op">);</span></span></code></pre></div>
<p>is an idiomatic way to create a temporary inode with no name that
will be cleaned up when the process closes <code>fd</code> or exits.</p>
<p>Unix provides file utilities callable from the shell as user-level
programs, for example <code>mkdir</code>, <code>ln</code>, and
<code>rm</code>. This design allows anyone to extend the command-line
interface by adding new user-level programs. In hindsight this plan
seems obvious, but other systems designed at the time of Unix often
built such commands into the shell (and built the shell into the
kernel).</p>
<p>One exception is <code>cd</code>, which is built into the shell <a
href="xv6-riscv-src/user/sh.c#L161"><span>(user/sh.c:161)</span></a>.
<code>cd</code> must change the current working directory of the shell
itself. If <code>cd</code> were run as a regular command, then the shell
would fork a child process, the child process would run <code>cd</code>,
and <code>cd</code> would change the <em>child</em> ’s working
directory. The parent’s (i.e., the shell’s) working directory would not
change.</p>
</section>
<section id="real-world" class="level2" data-number="1.5">
<h2 data-number="1.5"><span class="header-section-number">1.5</span>
Real world</h2>
<p>Unix’s combination of “standard” file descriptors, pipes, and
convenient shell syntax for operations on them was a major advance in
writing general-purpose reusable programs. The idea sparked a culture of
“software tools” that was responsible for much of Unix’s power and
popularity, and the shell was the first so-called “scripting language.”
The Unix system call interface persists today in systems like BSD,
Linux, and macOS.</p>
<p>The Unix system call interface has been standardized through the
Portable Operating System Interface (POSIX) standard. Xv6 is
<em>not</em> POSIX compliant: it is missing many system calls (including
basic ones such as <code>lseek</code>), and many of the system calls it
does provide differ from the standard. Our main goals for xv6 are
simplicity and clarity while providing a simple UNIX-like system-call
interface. Several people have extended xv6 with a few more system calls
and a simple C library in order to run basic Unix programs. Modern
kernels, however, provide many more system calls, and many more kinds of
kernel services, than xv6. For example, they support networking,
windowing systems, user-level threads, drivers for many devices, and so
on. Modern kernels evolve continuously and rapidly, and offer many
features beyond POSIX.</p>
<p>Unix unified access to multiple types of resources (files,
directories, and devices) with a single set of file-name and
file-descriptor interfaces. This idea can be extended to more kinds of
resources; a good example is Plan 9 <span class="citation"
data-cites="Presotto91plan9">(<a href="#ref-Presotto91plan9"
role="doc-biblioref">Presotto et al. 1991</a>)</span>, which applied the
“resources are files” concept to networks, graphics, and more. However,
most Unix-derived operating systems have not followed this route.</p>
<p>The file system and file descriptors have been powerful abstractions.
Even so, there are other models for operating system interfaces.
Multics, a predecessor of Unix, abstracted file storage in a way that
made it look like memory, producing a very different flavor of
interface. The complexity of the Multics design had a direct influence
on the designers of Unix, who aimed to build something simpler.</p>
<p>Xv6 does not provide a notion of users or of protecting one user from
another; in Unix terms, all xv6 processes run as root.</p>
<p>This book examines how xv6 implements its Unix-like interface, but
the ideas and concepts apply to more than just Unix. Any operating
system must multiplex processes onto the underlying hardware, isolate
processes from each other, and provide mechanisms for controlled
inter-process communication. After studying xv6, you should be able to
look at other, more complex operating systems and see the concepts
underlying xv6 in those systems as well.</p>
</section>
<section id="exercises" class="level2" data-number="1.6">
<h2 data-number="1.6"><span class="header-section-number">1.6</span>
Exercises</h2>
<ol>
<li><p>Write a program that uses UNIX system calls to “ping-pong” a byte
between two processes over a pair of pipes, one for each direction.
Measure the program’s performance, in exchanges per second.</p></li>
</ol>
</section>
</section>
<section id="CH:FIRST" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span>
Operating system organization</h1>
<p>A key requirement for an operating system is to support several
activities at once. For example, using the system call interface
described in Chapter <a href="#CH:UNIX" data-reference-type="ref"
data-reference="CH:UNIX">1</a> a process can start new processes with
<code>fork</code>. The operating system must <em><span
id="time-share_2">time-share</span></em> the resources of the computer
among these processes. For example, even if there are more processes
than there are hardware CPUs, the operating system must ensure that all
of the processes get a chance to execute. The operating system must also
arrange for <em><span id="isolation_1">isolation</span></em> between the
processes. That is, if one process has a bug and malfunctions, it
shouldn’t affect processes that don’t depend on the buggy process.
Complete isolation, however, is too strong, since it should be possible
for processes to intentionally interact; pipelines are an example. Thus
an operating system must fulfill three requirements: multiplexing,
isolation, and interaction.</p>
<p>This chapter provides an overview of how operating systems are
organized to achieve these three requirements. It turns out there are
many ways to do so, but this text focuses on mainstream designs centered
around a <em><span id="monolithic_kernel_1">monolithic
kernel</span></em>, which is used by many Unix operating systems. This
chapter also provides an overview of an xv6 process, which is the unit
of isolation in xv6, and the creation of the first process when xv6
starts.</p>
<p>Xv6 runs on a <em><span id="multi-core_1">multi-core</span></em><a
href="#fn2" class="footnote-ref" id="fnref2"
role="doc-noteref"><sup>2</sup></a> RISC-V microprocessor, and much of
its low-level functionality (for example, its process implementation) is
specific to RISC-V. RISC-V is a 64-bit CPU, and xv6 is written in “LP64”
C, which means long (L) and pointers (P) in the C programming language
are 64 bits, but int is 32-bit. This book assumes the reader has done a
bit of machine-level programming on some architecture, and will
introduce RISC-V-specific ideas as they come up. A useful reference for
RISC-V is “The RISC-V Reader: An Open Architecture Atlas” <span
class="citation" data-cites="riscv">(<a href="#ref-riscv"
role="doc-biblioref">Patterson and Waterman 2017</a>)</span>. The
user-level ISA <span class="citation" data-cites="riscv:user">(<a
href="#ref-riscv:user" role="doc-biblioref">Waterman and Asanovic
2019</a>)</span> and the privileged architecture <span class="citation"
data-cites="riscv:priv">(<a href="#ref-riscv:priv"
role="doc-biblioref">Waterman, Asanovic, and Hauser 2021</a>)</span> are
the official specifications.</p>
<p>The CPU in a complete computer is surrounded by support hardware,
much of it in the form of I/O interfaces. Xv6 is written for the support
hardware simulated by qemu’s “-machine virt” option. This includes RAM,
a ROM containing boot code, a serial connection to the user’s
keyboard/screen, and a disk for storage.</p>
<section id="abstracting-physical-resources" class="level2"
data-number="2.1">
<h2 data-number="2.1"><span class="header-section-number">2.1</span>
Abstracting physical resources</h2>
<p>The first question one might ask when encountering an operating
system is why have it at all? That is, one could implement the system
calls in Figure <a href="#fig:api" data-reference-type="ref"
data-reference="fig:api">1.2</a> as a library, with which applications
link. In this plan, each application could even have its own library
tailored to its needs. Applications could directly interact with
hardware resources and use those resources in the best way for the
application (e.g., to achieve high or predictable performance). Some
operating systems for embedded devices or real-time systems are
organized in this way.</p>
<p>The downside of this library approach is that, if there is more than
one application running, the applications must be well-behaved. For
example, each application must periodically give up the CPU so that
other applications can run. Such a <em>cooperative</em> time-sharing
scheme may be OK if all applications trust each other and have no bugs.
It’s more typical for applications to not trust each other, and to have
bugs, so one often wants stronger isolation than a cooperative scheme
provides.</p>
<p>To achieve strong isolation it’s helpful to forbid applications from
directly accessing sensitive hardware resources, and instead to abstract
the resources into services. For example, Unix applications interact
with storage only through the file system’s <code>open</code>,
<code>read</code>, <code>write</code>, and <code>close</code> system
calls, instead of reading and writing the disk directly. This provides
the application with the convenience of pathnames, and it allows the
operating system (as the implementer of the interface) to manage the
disk. Even if isolation is not a concern, programs that interact
intentionally (or just wish to keep out of each other’s way) are likely
to find a file system a more convenient abstraction than direct use of
the disk.</p>
<p>Similarly, Unix transparently switches hardware CPUs among processes,
saving and restoring register state as necessary, so that applications
don’t have to be aware of time sharing. This transparency allows the
operating system to share CPUs even if some applications are in infinite
loops.</p>
<p>As another example, Unix processes use <code>exec</code> to build up
their memory image, instead of directly interacting with physical
memory. This allows the operating system to decide where to place a
process in memory; if memory is tight, the operating system might even
store some of a process’s data on disk. <code>exec</code> also provides
users with the convenience of a file system to store executable program
images.</p>
<p>Many forms of interaction among Unix processes occur via file
descriptors. Not only do file descriptors abstract away many details
(e.g., where data in a pipe or file is stored), they are also defined in
a way that simplifies interaction. For example, if one application in a
pipeline fails, the kernel generates an end-of-file signal for the next
process in the pipeline.</p>
<p>The system-call interface in Figure <a href="#fig:api"
data-reference-type="ref" data-reference="fig:api">1.2</a> is carefully
designed to provide both programmer convenience and the possibility of
strong isolation. The Unix interface is not the only way to abstract
resources, but it has proven to be a very good one.</p>
</section>
<section id="user-mode-supervisor-mode-and-system-calls" class="level2"
data-number="2.2">
<h2 data-number="2.2"><span class="header-section-number">2.2</span>
User mode, supervisor mode, and system calls</h2>
<p>Strong isolation requires a hard boundary between applications and
the operating system. If the application makes a mistake, we don’t want
the operating system to fail or other applications to fail. Instead, the
operating system should be able to clean up the failed application and
continue running other applications. To achieve strong isolation, the
operating system must arrange that applications cannot modify (or even
read) the operating system’s data structures and instructions and that
applications cannot access other processes’ memory.</p>
<p>CPUs provide hardware support for strong isolation. For example,
RISC-V has three modes in which the CPU can execute instructions:
<em><span id="machine_mode_1">machine mode</span></em>, <em><span
id="supervisor_mode_1">supervisor mode</span></em>, and <em><span
id="user_mode_1">user mode</span></em>. Instructions executing in
machine mode have full privilege; a CPU starts in machine mode. Machine
mode is mostly intended for configuring a computer. Xv6 executes a few
lines in machine mode and then changes to supervisor mode.</p>
<p>In supervisor mode the CPU is allowed to execute <em><span
id="privileged_instructions_1">privileged instructions</span></em>: for
example, enabling and disabling interrupts, reading and writing the
register that holds the address of a page table, etc. If an application
in user mode attempts to execute a privileged instruction, then the CPU
doesn’t execute the instruction, but switches to supervisor mode so that
supervisor-mode code can terminate the application, because it did
something it shouldn’t be doing. Figure <a href="#fig:os"
data-reference-type="ref" data-reference="fig:os">1.1</a> in Chapter <a
href="#CH:UNIX" data-reference-type="ref" data-reference="CH:UNIX">1</a>
illustrates this organization. An application can execute only user-mode
instructions (e.g., adding numbers, etc.) and is said to be running in
<em><span id="user_space_2">user space</span></em>, while the software
in supervisor mode can also execute privileged instructions and is said
to be running in <em><span id="kernel_space_2">kernel space</span></em>.
The software running in kernel space (or in supervisor mode) is called
the <em><span id="kernel_2">kernel</span></em>.</p>
<p>An application that wants to invoke a kernel function (e.g., the
<code>read</code> system call in xv6) must transition to the kernel; an
application <em>cannot</em> invoke a kernel function directly. CPUs
provide a special instruction that switches the CPU from user mode to
supervisor mode and enters the kernel at an entry point specified by the
kernel. (RISC-V provides the <code id="ecall_1">ecall</code> instruction
for this purpose.) Once the CPU has switched to supervisor mode, the
kernel can then validate the arguments of the system call (e.g., check
if the address passed to the system call is part of the application’s
memory), decide whether the application is allowed to perform the
requested operation (e.g., check if the application is allowed to write
the specified file), and then deny it or execute it. It is important
that the kernel control the entry point for transitions to supervisor
mode; if the application could decide the kernel entry point, a
malicious application could, for example, enter the kernel at a point
where the validation of arguments is skipped.</p>
</section>
<section id="kernel-organization" class="level2" data-number="2.3">
<h2 data-number="2.3"><span class="header-section-number">2.3</span>
Kernel organization</h2>
<p>A key design question is what part of the operating system should run
in supervisor mode. One possibility is that the entire operating system
resides in the kernel, so that the implementations of all system calls
run in supervisor mode. This organization is called a <em><span
id="monolithic_kernel_2">monolithic kernel</span></em>.</p>
<p>In this organization the entire operating system runs with full
hardware privilege. This organization is convenient because the OS
designer doesn’t have to decide which part of the operating system
doesn’t need full hardware privilege. Furthermore, it is easier for
different parts of the operating system to cooperate. For example, an
operating system might have a buffer cache that can be shared both by
the file system and the virtual memory system.</p>
<p>A downside of the monolithic organization is that the interfaces
between different parts of the operating system are often complex (as we
will see in the rest of this text), and therefore it is easy for an
operating system developer to make a mistake. In a monolithic kernel, a
mistake is fatal, because an error in supervisor mode will often cause
the kernel to fail. If the kernel fails, the computer stops working, and
thus all applications fail too. The computer must reboot to start
again.</p>
<p>To reduce the risk of mistakes in the kernel, OS designers can
minimize the amount of operating system code that runs in supervisor
mode, and execute the bulk of the operating system in user mode. This
kernel organization is called a <em><span
id="microkernel_1">microkernel</span></em>.</p>
<figure id="fig:mkernel">
<img src="fig/mkernel.svg" />
<figcaption>A microkernel with a file-system server</figcaption>
</figure>
<p>Figure <a href="#fig:mkernel" data-reference-type="ref"
data-reference="fig:mkernel">2.1</a> illustrates this microkernel
design. In the figure, the file system runs as a user-level process. OS
services running as processes are called servers. To allow applications
to interact with the file server, the kernel provides an inter-process
communication mechanism to send messages from one user-mode process to
another. For example, if an application like the shell wants to read or
write a file, it sends a message to the file server and waits for a
response.</p>
<p>In a microkernel, the kernel interface consists of a few low-level
functions for starting applications, sending messages, accessing device
hardware, etc. This organization allows the kernel to be relatively
simple, as most of the operating system resides in user-level
servers.</p>
<p>In the real world, both monolithic kernels and microkernels are
popular. Many Unix kernels are monolithic. For example, Linux has a
monolithic kernel, although some OS functions run as user-level servers
(e.g., the windowing system). Linux delivers high performance to
OS-intensive applications, partially because the subsystems of the
kernel can be tightly integrated.</p>
<p>Operating systems such as Minix, L4, and QNX are organized as a
microkernel with servers, and have seen wide deployment in embedded
settings. A variant of L4, seL4, is small enough that it has been
verified for memory safety and other security properties <span
class="citation" data-cites="sel4">(<a href="#ref-sel4"
role="doc-biblioref">Klein et al. 2009</a>)</span>.</p>
<p>There is much debate among developers of operating systems about
which organization is better, and there is no conclusive evidence one
way or the other. Furthermore, it depends much on what “better” means:
faster performance, smaller code size, reliability of the kernel,
reliability of the complete operating system (including user-level
services), etc.</p>
<p>There are also practical considerations that may be more important
than the question of which organization. Some operating systems have a
microkernel but run some of the user-level services in kernel space for
performance reasons. Some operating systems have monolithic kernels
because that is how they started and there is little incentive to move
to a pure microkernel organization, because new features may be more
important than rewriting the existing operating system to fit a
microkernel design.</p>
<p>From this book’s perspective, microkernel and monolithic operating
systems share many key ideas. They implement system calls, they use page
tables, they handle interrupts, they support processes, they use locks
for concurrency control, they implement a file system, etc. This book
focuses on these core ideas.</p>
<p>Xv6 is implemented as a monolithic kernel, like most Unix operating
systems. Thus, the xv6 kernel interface corresponds to the operating
system interface, and the kernel implements the complete operating
system. Since xv6 doesn’t provide many services, its kernel is smaller
than some microkernels, but conceptually xv6 is monolithic.</p>
</section>
<section id="code-xv6-organization" class="level2" data-number="2.4">
<h2 data-number="2.4"><span class="header-section-number">2.4</span>
Code: xv6 organization</h2>
<figure id="fig:source">
<table>
<thead>
<tr class="header">
<th style="text-align: left;"><span><strong>File</strong></span></th>
<th
style="text-align: left;"><span><strong>Description</strong></span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">bio.c</td>
<td style="text-align: left;">Disk block cache for the file system.</td>
</tr>
<tr class="even">
<td style="text-align: left;">console.c</td>
<td style="text-align: left;">Connect to the user keyboard and
screen.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">entry.S</td>
<td style="text-align: left;">Very first boot instructions.</td>
</tr>
<tr class="even">
<td style="text-align: left;">exec.c</td>
<td style="text-align: left;">exec() system call.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">file.c</td>
<td style="text-align: left;">File descriptor support.</td>
</tr>
<tr class="even">
<td style="text-align: left;">fs.c</td>
<td style="text-align: left;">File system.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">kalloc.c</td>
<td style="text-align: left;">Physical page allocator.</td>
</tr>
<tr class="even">
<td style="text-align: left;">kernelvec.S</td>
<td style="text-align: left;">Handle traps from kernel, and timer
interrupts.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">log.c</td>
<td style="text-align: left;">File system logging and crash
recovery.</td>
</tr>
<tr class="even">
<td style="text-align: left;">main.c</td>
<td style="text-align: left;">Control initialization of other modules
during boot.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">pipe.c</td>
<td style="text-align: left;">Pipes.</td>
</tr>
<tr class="even">
<td style="text-align: left;">plic.c</td>
<td style="text-align: left;">RISC-V interrupt controller.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">printf.c</td>
<td style="text-align: left;">Formatted output to the console.</td>
</tr>
<tr class="even">
<td style="text-align: left;">proc.c</td>
<td style="text-align: left;">Processes and scheduling.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">sleeplock.c</td>
<td style="text-align: left;">Locks that yield the CPU.</td>
</tr>
<tr class="even">
<td style="text-align: left;">spinlock.c</td>
<td style="text-align: left;">Locks that don’t yield the CPU.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">start.c</td>
<td style="text-align: left;">Early machine-mode boot code.</td>
</tr>
<tr class="even">
<td style="text-align: left;">string.c</td>
<td style="text-align: left;">C string and byte-array library.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">swtch.S</td>
<td style="text-align: left;">Thread switching.</td>
</tr>
<tr class="even">
<td style="text-align: left;">syscall.c</td>
<td style="text-align: left;">Dispatch system calls to handling
function.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">sysfile.c</td>
<td style="text-align: left;">File-related system calls.</td>
</tr>
<tr class="even">
<td style="text-align: left;">sysproc.c</td>
<td style="text-align: left;">Process-related system calls.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">trampoline.S</td>
<td style="text-align: left;">Assembly code to switch between user and
kernel.</td>
</tr>
<tr class="even">
<td style="text-align: left;">trap.c</td>
<td style="text-align: left;">C code to handle and return from traps and
interrupts.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">uart.c</td>
<td style="text-align: left;">Serial-port console device driver.</td>
</tr>
<tr class="even">
<td style="text-align: left;">virtio_disk.c</td>
<td style="text-align: left;">Disk device driver.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">vm.c</td>
<td style="text-align: left;">Manage page tables and address
spaces.</td>
</tr>
</tbody>
</table>
<figcaption>Xv6 kernel source files.</figcaption>
</figure>
<p>The xv6 kernel source is in the <span><code>kernel/</code></span>
sub-directory. The source is divided into files, following a rough
notion of modularity; Figure <a href="#fig:source"
data-reference-type="ref" data-reference="fig:source">2.2</a> lists the
files. The inter-module interfaces are defined in <code>defs.h</code> <a
href="xv6-riscv-src/kernel/defs.h"><span>(kernel/defs.h)</span></a>.</p>
</section>
<section id="process-overview" class="level2" data-number="2.5">
<h2 data-number="2.5"><span class="header-section-number">2.5</span>
Process overview</h2>
<p>The unit of isolation in xv6 (as in other Unix operating systems) is
a <em><span id="process_2">process</span></em>. The process abstraction
prevents one process from wrecking or spying on another process’s
memory, CPU, file descriptors, etc. It also prevents a process from
wrecking the kernel itself, so that a process can’t subvert the kernel’s
isolation mechanisms. The kernel must implement the process abstraction
with care because a buggy or malicious application may trick the kernel
or hardware into doing something bad (e.g., circumventing isolation).
The mechanisms used by the kernel to implement processes include the
user/supervisor mode flag, address spaces, and time-slicing of
threads.</p>
<p>To help enforce isolation, the process abstraction provides the
illusion to a program that it has its own private machine. A process
provides a program with what appears to be a private memory system, or
<em><span id="address_space_1">address space</span></em>, which other
processes cannot read or write. A process also provides the program with
what appears to be its own CPU to execute the program’s
instructions.</p>
<p>Xv6 uses page tables (which are implemented by hardware) to give each
process its own address space. The RISC-V page table translates (or
“maps”) a <em><span id="virtual_address_1">virtual address</span></em>
(the address that an RISC-V instruction manipulates) to a <em><span
id="physical_address_1">physical address</span></em> (an address that
the CPU chip sends to main memory).</p>
<figure id="fig:as">
<img src="fig/as.svg" />
<figcaption>Layout of a process’s virtual address space</figcaption>
</figure>
<p>Xv6 maintains a separate page table for each process that defines
that process’s address space. As illustrated in Figure <a href="#fig:as"
data-reference-type="ref" data-reference="fig:as">2.3</a>, an address
space includes the process’s <em><span id="user_memory_1">user
memory</span></em> starting at virtual address zero. Instructions come
first, followed by global variables, then the stack, and finally a
“heap” area (for malloc) that the process can expand as needed. There
are a number of factors that limit the maximum size of a process’s
address space: pointers on the RISC-V are 64 bits wide; the hardware
only uses the low 39 bits when looking up virtual addresses in page
tables; and xv6 only uses 38 of those 39 bits. Thus, the maximum address
is <span class="math inline">2<sup>38</sup> − 1</span> = 0x3fffffffff,
which is <code>MAXVA</code> <a
href="xv6-riscv-src/kernel/riscv.h#L363"><span>(kernel/riscv.h:363)</span></a>.
At the top of the address space xv6 reserves a page for a <em><span
id="trampoline_1">trampoline</span></em> and a page mapping the
process’s <em><span id="trapframe_1">trapframe</span></em>. Xv6 uses
these two pages to transition into the kernel and back; the trampoline
page contains the code to transition in and out of the kernel and
mapping the trapframe is necessary to save/restore the state of the user
process, as we will explain in Chapter <a href="#CH:TRAP"
data-reference-type="ref" data-reference="CH:TRAP">4</a>.</p>
<p>The xv6 kernel maintains many pieces of state for each process, which
it gathers into a <code id="struct_proc_1">struct proc</code> <a
href="xv6-riscv-src/kernel/proc.h#L85"><span>(kernel/proc.h:85)</span></a>.
A process’s most important pieces of kernel state are its page table,
its kernel stack, and its run state. We’ll use the notation <code
id="p-&gt;xxx_1">p-&gt;xxx</code> to refer to elements of the
<code>proc</code> structure; for example, <code
id="p-&gt;pagetable_1">p-&gt;pagetable</code> is a pointer to the
process’s page table.</p>
<p>Each process has a thread of execution (or <em><span
id="thread_1">thread</span></em> for short) that executes the process’s
instructions. A thread can be suspended and later resumed. To switch
transparently between processes, the kernel suspends the currently
running thread and resumes another process’s thread. Much of the state
of a thread (local variables, function call return addresses) is stored
on the thread’s stacks. Each process has two stacks: a user stack and a
kernel stack (<code id="p-&gt;kstack_1">p-&gt;kstack</code>). When the
process is executing user instructions, only its user stack is in use,
and its kernel stack is empty. When the process enters the kernel (for a
system call or interrupt), the kernel code executes on the process’s
kernel stack; while a process is in the kernel, its user stack still
contains saved data, but isn’t actively used. A process’s thread
alternates between actively using its user stack and its kernel stack.
The kernel stack is separate (and protected from user code) so that the
kernel can execute even if a process has wrecked its user stack.</p>
<p>A process can make a system call by executing the RISC-V <code
id="ecall_2">ecall</code> instruction. This instruction raises the
hardware privilege level and changes the program counter to a
kernel-defined entry point. The code at the entry point switches to a
kernel stack and executes the kernel instructions that implement the
system call. When the system call completes, the kernel switches back to
the user stack and returns to user space by calling the <code
id="sret_1">sret</code> instruction, which lowers the hardware privilege
level and resumes executing user instructions just after the system call
instruction. A process’s thread can “block” in the kernel to wait for
I/O, and resume where it left off when the I/O has finished.</p>
<p><code id="p-&gt;state_1">p-&gt;state</code> indicates whether the
process is allocated, ready to run, running, waiting for I/O, or
exiting.</p>
<p><code id="p-&gt;pagetable_2">p-&gt;pagetable</code> holds the
process’s page table, in the format that the RISC-V hardware expects.
Xv6 causes the paging hardware to use a process’s
<code>p-&gt;pagetable</code> when executing that process in user space.
A process’s page table also serves as the record of the addresses of the
physical pages allocated to store the process’s memory.</p>
<p>In summary, a process bundles two design ideas: an address space to
give a process the illusion of its own memory, and, a thread, to give
the process the illusion of its own CPU. In xv6, a process consists of
one address space and one thread. In real operating systems a process
may have more than one thread to take advantage of multiple CPUs.</p>
</section>
<section id="code-starting-xv6-the-first-process-and-system-call"
class="level2" data-number="2.6">
<h2 data-number="2.6"><span class="header-section-number">2.6</span>
Code: starting xv6, the first process and system call</h2>
<p>To make xv6 more concrete, we’ll outline how the kernel starts and
runs the first process. The subsequent chapters will describe the
mechanisms that show up in this overview in more detail.</p>
<p>When the RISC-V computer powers on, it initializes itself and runs a
boot loader which is stored in read-only memory. The boot loader loads
the xv6 kernel into memory. Then, in machine mode, the CPU executes xv6
starting at <code id="_entry_1">_entry</code> <a
href="xv6-riscv-src/kernel/entry.S#L7"><span>(kernel/entry.S:7)</span></a>.
The RISC-V starts with paging hardware disabled: virtual addresses map
directly to physical addresses.</p>
<p>The loader loads the xv6 kernel into memory at physical address
<code>0x80000000</code>. The reason it places the kernel at
<code>0x80000000</code> rather than <code>0x0</code> is because the
address range <code>0x0:0x80000000</code> contains I/O devices.</p>
<p>The instructions at <code>_entry</code> set up a stack so that xv6
can run C code. Xv6 declares space for an initial stack,
<code>stack0</code>, in the file <code>start.c</code> <a
href="xv6-riscv-src/kernel/start.c#L11"><span>(kernel/start.c:11)</span></a>.
The code at <code>_entry</code> loads the stack pointer register
<code>sp</code> with the address <code>stack0+4096</code>, the top of
the stack, because the stack on RISC-V grows down. Now that the kernel
has a stack, <code>_entry</code> calls into C code at <code>start</code>
<a
href="xv6-riscv-src/kernel/start.c#L21"><span>(kernel/start.c:21)</span></a>.</p>
<p>The function <code>start</code> performs some configuration that is
only allowed in machine mode, and then switches to supervisor mode. To
enter supervisor mode, RISC-V provides the instruction
<code>mret</code>. This instruction is most often used to return from a
previous call from supervisor mode to machine mode. <code>start</code>
isn’t returning from such a call, and instead sets things up as if there
had been one: it sets the previous privilege mode to supervisor in the
register <code>mstatus</code>, it sets the return address to
<code>main</code> by writing <code>main</code>’s address into the
register <code>mepc</code>, disables virtual address translation in
supervisor mode by writing <code>0</code> into the page-table register
<code>satp</code>, and delegates all interrupts and exceptions to
supervisor mode.</p>
<p>Before jumping into supervisor mode, <code>start</code> performs one
more task: it programs the clock chip to generate timer interrupts. With
this housekeeping out of the way, <code>start</code> “returns” to
supervisor mode by calling <code>mret</code>. This causes the program
counter to change to <code>main</code> <a
href="xv6-riscv-src/kernel/main.c#L11"><span>(kernel/main.c:11)</span></a>.</p>
<p>After <code>main</code> <a
href="xv6-riscv-src/kernel/main.c#L11"><span>(kernel/main.c:11)</span></a>
initializes several devices and subsystems, it creates the first process
by calling <code>userinit</code> <a
href="xv6-riscv-src/kernel/proc.c#L233"><span>(kernel/proc.c:233)</span></a>.
The first process executes a small program written in RISC-V assembly,
which makes the first system call in xv6. <code
id="initcode.S_1">initcode.S</code> <a
href="xv6-riscv-src/user/initcode.S#L3"><span>(user/initcode.S:3)</span></a>
loads the number for the <code>exec</code> system call,
<code>SYS_EXEC</code> <a
href="xv6-riscv-src/kernel/syscall.h#L8"><span>(kernel/syscall.h:8)</span></a>,
into register <span><code>a7</code></span>, and then calls
<code>ecall</code> to re-enter the kernel.</p>
<p>The kernel uses the number in register <span><code>a7</code></span>
in <code>syscall</code> <a
href="xv6-riscv-src/kernel/syscall.c#L132"><span>(kernel/syscall.c:132)</span></a>
to call the desired system call. The system call table <a
href="xv6-riscv-src/kernel/syscall.c#L107"><span>(kernel/syscall.c:107)</span></a>
maps <code>SYS_EXEC</code> to <code>sys_exec</code>, which the kernel
invokes. As we saw in Chapter <a href="#CH:UNIX"
data-reference-type="ref" data-reference="CH:UNIX">1</a>, <code
id="exec_6">exec</code> replaces the memory and registers of the current
process with a new program (in this case, <code
id="/init_1">/init</code>).</p>
<p>Once the kernel has completed <code>exec</code>, it returns to user
space in the <code>/init</code> process. <code>init</code> <a
href="xv6-riscv-src/user/init.c#L15"><span>(user/init.c:15)</span></a>
creates a new console device file if needed and then opens it as file
descriptors 0, 1, and 2. Then it starts a shell on the console. The
system is up.</p>
</section>
<section id="security-model" class="level2" data-number="2.7">
<h2 data-number="2.7"><span class="header-section-number">2.7</span>
Security Model</h2>
<p>You may wonder how the operating system deals with buggy or malicious
code. Because coping with malice is strictly harder than dealing with
accidental bugs, it’s reasonable to view this topic as relating to
security. Here’s a high-level view of typical security assumptions and
goals in operating system design.</p>
<p>The operating system must assume that a process’s user-level code
will do its best to wreck the kernel or other processes. User code may
try to dereference pointers outside its allowed address space; it may
attempt to execute any RISC-V instructions, even those not intended for
user code; it may try to read and write any RISC-V control register; it
may try to directly access device hardware; and it may pass clever
values to system calls in an attempt to trick the kernel into crashing
or doing something stupid. The kernel’s goal to restrict each user
processes so that all it can do is read/write/execute its own user
memory, use the 32 general-purpose RISC-V registers, and affect the
kernel and other processes in the ways that system calls are intended to
allow. The kernel must prevent any other actions. This is typically an
absolute requirement in kernel design.</p>
<p>The expectations for the kernel’s own code are quite different.
Kernel code is assumed to be written by well-meaning and careful
programmers. Kernel code is expected to be bug-free, and certainly to
contain nothing malicious. This assumption affects how we analyze kernel
code. For example, there are many internal kernel functions (e.g., the
spin locks) that would cause serious problems if kernel code used them
incorrectly. When examining any specific piece of kernel code, we’ll
want to convince ourselves that it behaves correctly. We assume,
however, that kernel code in general is correctly written, and follows
all the rules about use of the kernel’s own functions and data
structures. At the hardware level, the RISC-V CPU, RAM, disk, etc. are
assumed to operate as advertised in the documentation, with no hardware
bugs.</p>
<p>Of course in real life things are not so straightforward. It’s
difficult to prevent clever user code from making a system unusable (or
causing it to panic) by consuming kernel-protected resources – disk
space, CPU time, process table slots, etc. It’s usually impossible to
write bug-free code or design bug-free hardware; if the writers of
malicious user code are aware of kernel or hardware bugs, they will
exploit them. Even in mature, widely-used kernels, such as Linux, people
discover new vulnerabilities continuously <span class="citation"
data-cites="mitre:cves">(<a href="#ref-mitre:cves"
role="doc-biblioref"><span>“Linux Common Vulnerabilities and Exposures
(<span>CVEs</span>),”</span> n.d.</a>)</span>. It’s worthwhile to design
safeguards into the kernel against the possibility that it has bugs:
assertions, type checking, stack guard pages, etc. Finally, the
distinction between user and kernel code is sometimes blurred: some
privileged user-level processes may provide essential services and
effectively be part of the operating system, and in some operating
systems privileged user code can insert new code into the kernel (as
with Linux’s loadable kernel modules).</p>
</section>
<section id="real-world-1" class="level2" data-number="2.8">
<h2 data-number="2.8"><span class="header-section-number">2.8</span>
Real world</h2>
<p>Most operating systems have adopted the process concept, and most
processes look similar to xv6’s. Modern operating systems, however,
support several threads within a process, to allow a single process to
exploit multiple CPUs. Supporting multiple threads in a process involves
quite a bit of machinery that xv6 doesn’t have, including potential
interface changes (e.g., Linux’s <code>clone</code>, a variant of
<code>fork</code>), to control which aspects of a process threads
share.</p>
</section>
<section id="exercises-1" class="level2" data-number="2.9">
<h2 data-number="2.9"><span class="header-section-number">2.9</span>
Exercises</h2>
<ol>
<li><p>Add a system call to xv6 that returns the amount of free memory
available.</p></li>
</ol>
</section>
</section>
<section id="CH:MEM" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Page
tables</h1>
<p>Page tables are the most popular mechanism through which the
operating system provides each process with its own private address
space and memory. Page tables determine what memory addresses mean, and
what parts of physical memory can be accessed. They allow xv6 to isolate
different process’s address spaces and to multiplex them onto a single
physical memory. Page tables are a popular design because they provide a
level of indirection that allow operating systems to perform many
tricks. Xv6 performs a few tricks: mapping the same memory (a trampoline
page) in several address spaces, and guarding kernel and user stacks
with an unmapped page. The rest of this chapter explains the page tables
that the RISC-V hardware provides and how xv6 uses them.</p>
<section id="paging-hardware" class="level2" data-number="3.1">
<h2 data-number="3.1"><span class="header-section-number">3.1</span>
Paging hardware</h2>
<p>As a reminder, RISC-V instructions (both user and kernel) manipulate
virtual addresses. The machine’s RAM, or physical memory, is indexed
with physical addresses. The RISC-V page table hardware connects these
two kinds of addresses, by mapping each virtual address to a physical
address.</p>
<figure id="fig:riscv_address">
<img src="fig/riscv_address.svg" />
<figcaption>RISC-V virtual and physical addresses, with a simplified
logical page table.</figcaption>
</figure>
<p>Xv6 runs on Sv39 RISC-V, which means that only the bottom 39 bits of
a 64-bit virtual address are used; the top 25 bits are not used. In this
Sv39 configuration, a RISC-V page table is logically an array of <span
class="math inline">2<sup>27</sup></span> (134,217,728) <em><span
id="page_table_entries_(PTEs)_1">page table entries (PTEs)</span></em>.
Each PTE contains a 44-bit physical page number (PPN) and some flags.
The paging hardware translates a virtual address by using the top 27
bits of the 39 bits to index into the page table to find a PTE, and
making a 56-bit physical address whose top 44 bits come from the PPN in
the PTE and whose bottom 12 bits are copied from the original virtual
address. Figure <a href="#fig:riscv_address" data-reference-type="ref"
data-reference="fig:riscv_address">3.1</a> shows this process with a
logical view of the page table as a simple array of PTEs (see Figure <a
href="#fig:riscv_pagetable" data-reference-type="ref"
data-reference="fig:riscv_pagetable">3.2</a> for a fuller story). A page
table gives the operating system control over virtual-to-physical
address translations at the granularity of aligned chunks of 4096 (<span
class="math inline">2<sup>12</sup></span>) bytes. Such a chunk is called
a <em><span id="page_1">page</span></em>.</p>
<figure id="fig:riscv_pagetable">
<img src="fig/riscv_pagetable.svg" />
<figcaption>RISC-V address translation details.</figcaption>
</figure>
<p>In Sv39 RISC-V, the top 25 bits of a virtual address are not used for
translation. The physical address also has room for growth: there is
room in the PTE format for the physical page number to grow by another
10 bits. The designers of RISC-V chose these numbers based on technology
predictions. <span class="math inline">2<sup>39</sup></span> bytes is
512 GB, which should be enough address space for applications running on
RISC-V computers. <span class="math inline">2<sup>56</sup></span> is
enough physical memory space for the near future to fit many I/O devices
and DRAM chips. If more is needed, the RISC-V designers have defined
Sv48 with 48-bit virtual addresses <span class="citation"
data-cites="riscv:priv">(<a href="#ref-riscv:priv"
role="doc-biblioref">Waterman, Asanovic, and Hauser
2021</a>)</span>.</p>
<p>As Figure <a href="#fig:riscv_pagetable" data-reference-type="ref"
data-reference="fig:riscv_pagetable">3.2</a> shows, a RISC-V CPU
translates a virtual address into a physical in three steps. A page
table is stored in physical memory as a three-level tree. The root of
the tree is a 4096-byte page-table page that contains 512 PTEs, which
contain the physical addresses for page-table pages in the next level of
the tree. Each of those pages contains 512 PTEs for the final level in
the tree. The paging hardware uses the top 9 bits of the 27 bits to
select a PTE in the root page-table page, the middle 9 bits to select a
PTE in a page-table page in the next level of the tree, and the bottom 9
bits to select the final PTE. (In Sv48 RISC-V a page table has four
levels, and bits 39 through 47 of a virtual address index into the
top-level.)</p>
<p>If any of the three PTEs required to translate an address is not
present, the paging hardware raises a <em><span
id="page-fault_exception_1">page-fault exception</span></em>, leaving it
up to the kernel to handle the exception (see Chapter <a href="#CH:TRAP"
data-reference-type="ref" data-reference="CH:TRAP">4</a>).</p>
<p>The three-level structure of Figure <a href="#fig:riscv_pagetable"
data-reference-type="ref" data-reference="fig:riscv_pagetable">3.2</a>
allows a memory-efficient way of recording PTEs, compared to the
single-level design of Figure <a href="#fig:riscv_address"
data-reference-type="ref" data-reference="fig:riscv_address">3.1</a>. In
the common case in which large ranges of virtual addresses have no
mappings, the three-level structure can omit entire page directories.
For example, if an application uses only a few pages starting at address
zero, then the entries 1 through 511 of the top-level page directory are
invalid, and the kernel doesn’t have to allocate pages those for 511
intermediate page directories. Furthermore, the kernel also doesn’t have
to allocate pages for the bottom-level page directories for those 511
intermediate page directories. So, in this example, the three-level
design saves 511 pages for intermediate page directories and <span
class="math inline">511 × 512</span> pages for bottom-level page
directories.</p>
<p>Although a CPU walks the three-level structure in hardware as part of
executing a load or store instruction, a potential downside of three
levels is that the CPU must load three PTEs from memory to perform the
translation of the virtual address in the load/store instruction to a
physical address. To avoid the cost of loading PTEs from physical
memory, a RISC-V CPU caches page table entries in a <em><span
id="Translation_Look-aside_Buffer_(TLB)_1">Translation Look-aside Buffer
(TLB)</span></em>.</p>
<p>Each PTE contains flag bits that tell the paging hardware how the
associated virtual address is allowed to be used. <code
id="PTE_V_1">PTE_V</code> indicates whether the PTE is present: if it is
not set, a reference to the page causes an exception (i.e., is not
allowed). <code id="PTE_R_1">PTE_R</code> controls whether instructions
are allowed to read to the page. <code id="PTE_W_1">PTE_W</code>
controls whether instructions are allowed to write to the page. <code
id="PTE_X_1">PTE_X</code> controls whether the CPU may interpret the
content of the page as instructions and execute them. <code
id="PTE_U_1">PTE_U</code> controls whether instructions in user mode are
allowed to access the page; if <code id="PTE_U_2">PTE_U</code> is not
set, the PTE can be used only in supervisor mode. Figure <a
href="#fig:riscv_pagetable" data-reference-type="ref"
data-reference="fig:riscv_pagetable">3.2</a> shows how it all works. The
flags and all other page hardware-related structures are defined in <a
href="xv6-riscv-src/kernel/riscv.h"><span>(kernel/riscv.h)</span></a></p>
<p>To tell a CPU to use a page table, the kernel must write the physical
address of the root page-table page into the <code>satp</code> register.
A CPU will translate all addresses generated by subsequent instructions
using the page table pointed to by its own <code>satp</code>. Each CPU
has its own <code>satp</code> so that different CPUs can run different
processes, each with a private address space described by its own page
table.</p>
<p>Typically a kernel maps all of physical memory into its page table so
that it can read and write any location in physical memory using
load/store instructions. Since the page directories are in physical
memory, the kernel can program the content of a PTE in a page directory
by writing to the virtual address of the PTE using a standard store
instruction.</p>
<p>A few notes about terms. Physical memory refers to storage cells in
DRAM. A byte of physical memory has an address, called a physical
address. Instructions use only virtual addresses, which the paging
hardware translates to physical addresses, and then sends to the DRAM
hardware to read or write storage. Unlike physical memory and virtual
addresses, virtual memory isn’t a physical object, but refers to the
collection of abstractions and mechanisms the kernel provides to manage
physical memory and virtual addresses.</p>
<figure id="fig:xv6_layout">
<img src="fig/xv6_layout.svg" />
<figcaption>On the left, xv6’s kernel address space.
<span><span>RWX</span></span> refer to PTE read, write, and execute
permissions. On the right, the RISC-V physical address space that xv6
expects to see.</figcaption>
</figure>
</section>
<section id="kernel-address-space" class="level2" data-number="3.2">
<h2 data-number="3.2"><span class="header-section-number">3.2</span>
Kernel address space</h2>
<p>Xv6 maintains one page table per process, describing each process’s
user address space, plus a single page table that describes the kernel’s
address space. The kernel configures the layout of its address space to
give itself access to physical memory and various hardware resources at
predictable virtual addresses. Figure <a href="#fig:xv6_layout"
data-reference-type="ref" data-reference="fig:xv6_layout">3.3</a> shows
how this layout maps kernel virtual addresses to physical addresses. The
file <a
href="xv6-riscv-src/kernel/memlayout.h"><span>(kernel/memlayout.h)</span></a>
declares the constants for xv6’s kernel memory layout.</p>
<p>QEMU simulates a computer that includes RAM (physical memory)
starting at physical address <code>0x80000000</code> and continuing
through at least <code>0x88000000</code>, which xv6 calls
<code>PHYSTOP</code>. The QEMU simulation also includes I/O devices such
as a disk interface. QEMU exposes the device interfaces to software as
<em><span id="memory-mapped_1">memory-mapped</span></em> control
registers that sit below <code>0x80000000</code> in the physical address
space. The kernel can interact with the devices by reading/writing these
special physical addresses; such reads and writes communicate with the
device hardware rather than with RAM. Chapter <a href="#CH:TRAP"
data-reference-type="ref" data-reference="CH:TRAP">4</a> explains how
xv6 interacts with devices.</p>
<p>The kernel gets at RAM and memory-mapped device registers using
“direct mapping;” that is, mapping the resources at virtual addresses
that are equal to the physical address. For example, the kernel itself
is located at <code>KERNBASE=0x80000000</code> in both the virtual
address space and in physical memory. Direct mapping simplifies kernel
code that reads or writes physical memory. For example, when
<code>fork</code> allocates user memory for the child process, the
allocator returns the physical address of that memory; <code>fork</code>
uses that address directly as a virtual address when it is copying the
parent’s user memory to the child.</p>
<p>There are a couple of kernel virtual addresses that aren’t
direct-mapped:</p>
<ul>
<li><p>The trampoline page. It is mapped at the top of the virtual
address space; user page tables have this same mapping. Chapter <a
href="#CH:TRAP" data-reference-type="ref" data-reference="CH:TRAP">4</a>
discusses the role of the trampoline page, but we see here an
interesting use case of page tables; a physical page (holding the
trampoline code) is mapped twice in the virtual address space of the
kernel: once at top of the virtual address space and once with a direct
mapping.</p></li>
<li><p>The kernel stack pages. Each process has its own kernel stack,
which is mapped high so that below it xv6 can leave an unmapped
<em><span id="guard_page_1">guard page</span></em>. The guard page’s PTE
is invalid (i.e., <code>PTE_V</code> is not set), so that if the kernel
overflows a kernel stack, it will likely cause an exception and the
kernel will panic. Without a guard page an overflowing stack would
overwrite other kernel memory, resulting in incorrect operation. A panic
crash is preferable.</p></li>
</ul>
<p>While the kernel uses its stacks via the high-memory mappings, they
are also accessible to the kernel through a direct-mapped address. An
alternate design might have just the direct mapping, and use the stacks
at the direct-mapped address. In that arrangement, however, providing
guard pages would involve unmapping virtual addresses that would
otherwise refer to physical memory, which would then be hard to use.</p>
<p>The kernel maps the pages for the trampoline and the kernel text with
the permissions <code>PTE_R</code> and <code>PTE_X</code>. The kernel
reads and executes instructions from these pages. The kernel maps the
other pages with the permissions <code>PTE_R</code> and
<code>PTE_W</code>, so that it can read and write the memory in those
pages. The mappings for the guard pages are invalid.</p>
</section>
<section id="code-creating-an-address-space" class="level2"
data-number="3.3">
<h2 data-number="3.3"><span class="header-section-number">3.3</span>
Code: creating an address space</h2>
<p>Most of the xv6 code for manipulating address spaces and page tables
resides in <span><code>vm.c</code></span> <a
href="xv6-riscv-src/kernel/vm.c#L1"><span>(kernel/vm.c:1)</span></a>.
The central data structure is <span><code>pagetable_t</code></span>,
which is really a pointer to a RISC-V root page-table page; a
<span><code>pagetable_t</code></span> may be either the kernel page
table, or one of the per-process page tables. The central functions are
<span><code>walk</code></span>, which finds the PTE for a virtual
address, and <span><code>mappages</code></span>, which installs PTEs for
new mappings. Functions starting with <span><code>kvm</code></span>
manipulate the kernel page table; functions starting with
<span><code>uvm</code></span> manipulate a user page table; other
functions are used for both. <span><code>copyout</code></span> and
<span><code>copyin</code></span> copy data to and from user virtual
addresses provided as system call arguments; they are in
<span><code>vm.c</code></span> because they need to explicitly translate
those addresses in order to find the corresponding physical memory.</p>
<p>Early in the boot sequence, <code id="main_1">main</code> calls <code
id="kvminit_1">kvminit</code> <a
href="xv6-riscv-src/kernel/vm.c#L54"><span>(kernel/vm.c:54)</span></a>
to create the kernel’s page table using <code
id="kvmmake_1">kvmmake</code> <a
href="xv6-riscv-src/kernel/vm.c#L20"><span>(kernel/vm.c:20)</span></a>.
This call occurs before xv6 has enabled paging on the RISC-V, so
addresses refer directly to physical memory. <code>kvmmake</code> first
allocates a page of physical memory to hold the root page-table page.
Then it calls <code id="kvmmap_1">kvmmap</code> to install the
translations that the kernel needs. The translations include the
kernel’s instructions and data, physical memory up to <code
id="PHYSTOP_1">PHYSTOP</code>, and memory ranges which are actually
devices. <code id="proc_mapstacks_1">proc_mapstacks</code> <a
href="xv6-riscv-src/kernel/proc.c#L33"><span>(kernel/proc.c:33)</span></a>
allocates a kernel stack for each process. It calls <code>kvmmap</code>
to map each stack at the virtual address generated by
<code>KSTACK</code>, which leaves room for the invalid stack-guard
pages.</p>
<p><code id="kvmmap_2">kvmmap</code> <a
href="xv6-riscv-src/kernel/vm.c#L132"><span>(kernel/vm.c:132)</span></a>
calls <code id="mappages_1">mappages</code> <a
href="xv6-riscv-src/kernel/vm.c#L143"><span>(kernel/vm.c:143)</span></a>,
which installs mappings into a page table for a range of virtual
addresses to a corresponding range of physical addresses. It does this
separately for each virtual address in the range, at page intervals. For
each virtual address to be mapped, <code>mappages</code> calls <code
id="walk_1">walk</code> to find the address of the PTE for that address.
It then initializes the PTE to hold the relevant physical page number,
the desired permissions (<code>PTE_W</code>, <code>PTE_X</code>, and/or
<code>PTE_R</code>), and <code>PTE_V</code> to mark the PTE as valid <a
href="xv6-riscv-src/kernel/vm.c#L158"><span>(kernel/vm.c:158)</span></a>.</p>
<p><code id="walk_2">walk</code> <a
href="xv6-riscv-src/kernel/vm.c#L86"><span>(kernel/vm.c:86)</span></a>
mimics the RISC-V paging hardware as it looks up the PTE for a virtual
address (see Figure <a href="#fig:riscv_pagetable"
data-reference-type="ref" data-reference="fig:riscv_pagetable">3.2</a>).
<code>walk</code> descends the 3-level page table 9 bits at the time. It
uses each level’s 9 bits of virtual address to find the PTE of either
the next-level page table or the final page <a
href="xv6-riscv-src/kernel/vm.c#L92"><span>(kernel/vm.c:92)</span></a>.
If the PTE isn’t valid, then the required page hasn’t yet been
allocated; if the <code>alloc</code> argument is set, <code>walk</code>
allocates a new page-table page and puts its physical address in the
PTE. It returns the address of the PTE in the lowest layer in the tree
<a
href="xv6-riscv-src/kernel/vm.c#L102"><span>(kernel/vm.c:102)</span></a>.</p>
<p>The above code depends on physical memory being direct-mapped into
the kernel virtual address space. For example, as <code>walk</code>
descends levels of the page table, it pulls the (physical) address of
the next-level-down page table from a PTE <a
href="xv6-riscv-src/kernel/vm.c#L94"><span>(kernel/vm.c:94)</span></a>,
and then uses that address as a virtual address to fetch the PTE at the
next level down <a
href="xv6-riscv-src/kernel/vm.c#L92"><span>(kernel/vm.c:92)</span></a>.</p>
<p><code id="main_2">main</code> calls <code
id="kvminithart_1">kvminithart</code> <a
href="xv6-riscv-src/kernel/vm.c#L62"><span>(kernel/vm.c:62)</span></a>
to install the kernel page table. It writes the physical address of the
root page-table page into the register <code>satp</code>. After this the
CPU will translate addresses using the kernel page table. Since the
kernel uses an identity mapping, the now virtual address of the next
instruction will map to the right physical memory address.</p>
<p>Each RISC-V CPU caches page table entries in a <em><span
id="Translation_Look-aside_Buffer_(TLB)_2">Translation Look-aside Buffer
(TLB)</span></em>, and when xv6 changes a page table, it must tell the
CPU to invalidate corresponding cached TLB entries. If it didn’t, then
at some point later the TLB might use an old cached mapping, pointing to
a physical page that in the meantime has been allocated to another
process, and as a result, a process might be able to scribble on some
other process’s memory. The RISC-V has an instruction <code
id="sfence.vma_1">sfence.vma</code> that flushes the current CPU’s TLB.
Xv6 executes <span><code>sfence.vma</code></span> in
<span><code>kvminithart</code></span> after reloading the
<code>satp</code> register, and in the trampoline code that switches to
a user page table before returning to user space <a
href="xv6-riscv-src/kernel/trampoline.S#L89"><span>(kernel/trampoline.S:89)</span></a>.</p>
<p>It is also necessary to issue <code>sfence.vma</code> before changing
<code>satp</code>, in order to wait for completion of all outstanding
loads and stores. This wait ensures that preceding updates to the page
table have completed, and ensures that preceding loads and stores use
the old page table, not the new one.</p>
<p>To avoid flushing the complete TLB, RISC-V CPUs may support address
space identifiers (ASIDs) <span class="citation"
data-cites="riscv:priv">(<a href="#ref-riscv:priv"
role="doc-biblioref">Waterman, Asanovic, and Hauser 2021</a>)</span>.
The kernel can then flush just the TLB entries for a particular address
space. Xv6 does not use this feature.</p>
</section>
<section id="physical-memory-allocation" class="level2"
data-number="3.4">
<h2 data-number="3.4"><span class="header-section-number">3.4</span>
Physical memory allocation</h2>
<p>The kernel must allocate and free physical memory at run-time for
page tables, user memory, kernel stacks, and pipe buffers.</p>
<p>Xv6 uses the physical memory between the end of the kernel and <code
id="PHYSTOP_2">PHYSTOP</code> for run-time allocation. It allocates and
frees whole 4096-byte pages at a time. It keeps track of which pages are
free by threading a linked list through the pages themselves. Allocation
consists of removing a page from the linked list; freeing consists of
adding the freed page to the list.</p>
</section>
<section id="code-physical-memory-allocator" class="level2"
data-number="3.5">
<h2 data-number="3.5"><span class="header-section-number">3.5</span>
Code: Physical memory allocator</h2>
<p>The allocator resides in <span><code>kalloc.c</code></span> <a
href="xv6-riscv-src/kernel/kalloc.c#L1"><span>(kernel/kalloc.c:1)</span></a>.
The allocator’s data structure is a <em>free list</em> of physical
memory pages that are available for allocation. Each free page’s list
element is a <code id="struct_run_1">struct run</code> <a
href="xv6-riscv-src/kernel/kalloc.c#L17"><span>(kernel/kalloc.c:17)</span></a>.
Where does the allocator get the memory to hold that data structure? It
store each free page’s <code>run</code> structure in the free page
itself, since there’s nothing else stored there. The free list is
protected by a spin lock <a
href="xv6-riscv-src/kernel/kalloc.c#L21-L24">(kernel/kalloc.c:21-24)</a>.
The list and the lock are wrapped in a struct to make clear that the
lock protects the fields in the struct. For now, ignore the lock and the
calls to <code>acquire</code> and <code>release</code>; Chapter <a
href="#CH:LOCK" data-reference-type="ref" data-reference="CH:LOCK">6</a>
will examine locking in detail.</p>
<p>The function <code id="main_3">main</code> calls <code
id="kinit_1">kinit</code> to initialize the allocator <a
href="xv6-riscv-src/kernel/kalloc.c#L27"><span>(kernel/kalloc.c:27)</span></a>.
<code>kinit</code> initializes the free list to hold every page between
the end of the kernel and <span><code>PHYSTOP</code></span>. Xv6 ought
to determine how much physical memory is available by parsing
configuration information provided by the hardware. Instead xv6 assumes
that the machine has 128 megabytes of RAM. <code>kinit</code> calls
<code id="freerange_1">freerange</code> to add memory to the free list
via per-page calls to <code id="kfree_1">kfree</code>. A PTE can only
refer to a physical address that is aligned on a 4096-byte boundary (is
a multiple of 4096), so <code>freerange</code> uses <code
id="PGROUNDUP_1">PGROUNDUP</code> to ensure that it frees only aligned
physical addresses. The allocator starts with no memory; these calls to
<code>kfree</code> give it some to manage.</p>
<p>The allocator sometimes treats addresses as integers in order to
perform arithmetic on them (e.g., traversing all pages in
<code>freerange</code>), and sometimes uses addresses as pointers to
read and write memory (e.g., manipulating the <code>run</code> structure
stored in each page); this dual use of addresses is the main reason that
the allocator code is full of C type casts. The other reason is that
freeing and allocation inherently change the type of the memory.</p>
<p>The function <code>kfree</code> <a
href="xv6-riscv-src/kernel/kalloc.c#L47"><span>(kernel/kalloc.c:47)</span></a>
begins by setting every byte in the memory being freed to the value 1.
This will cause code that uses memory after freeing it (uses “dangling
references”) to read garbage instead of the old valid contents;
hopefully that will cause such code to break faster. Then
<code>kfree</code> prepends the page to the free list: it casts
<code>pa</code> to a pointer to <code>struct</code> <code>run</code>,
records the old start of the free list in <code>r-&gt;next</code>, and
sets the free list equal to <code>r</code>. <code
id="kalloc_1">kalloc</code> removes and returns the first element in the
free list.</p>
</section>
<section id="process-address-space" class="level2" data-number="3.6">
<h2 data-number="3.6"><span class="header-section-number">3.6</span>
Process address space</h2>
<p>Each process has a separate page table, and when xv6 switches between
processes, it also changes page tables. Figure <a
href="#fig:processlayout" data-reference-type="ref"
data-reference="fig:processlayout">3.4</a> shows a process’s address
space in more detail than Figure <a href="#fig:as"
data-reference-type="ref" data-reference="fig:as">2.3</a>. A process’s
user memory starts at virtual address zero and can grow up to
<code>MAXVA</code> <a
href="xv6-riscv-src/kernel/riscv.h#L360"><span>(kernel/riscv.h:360)</span></a>,
allowing a process to address in principle 256 Gigabytes of memory.</p>
<p>A process’s address space consists of pages that contain the text of
the program (which xv6 maps with the permissions <code>PTE_R</code>,
<code>PTE_X</code>, and <code>PTE_U</code>), pages that contain the
pre-initialized data of the program, a page for the stack, and pages for
the heap. Xv6 maps the data, stack, and heap with the permissions
<code>PTE_R</code>, <code>PTE_W</code>, and <code>PTE_U</code>.</p>
<p>Using permissions within a user address space is a common technique
to harden a user process. If the text were mapped with
<code>PTE_W</code>, then a process could accidentally modify its own
program; for example, a programming error may cause the program to write
to a null pointer, modifying instructions at address 0, and then
continue running, perhaps creating more havoc. To detect such errors
immediately, xv6 maps the text without <code>PTE_W</code>; if a program
accidentally attempts to store to address 0, the hardware will refuse to
execute the store and raises a page fault (see Section <a
href="#sec:pagefaults" data-reference-type="ref"
data-reference="sec:pagefaults">4.6</a>). The kernel then kills the
process and prints out an informative message so that the developer can
track down the problem.</p>
<p>Similarly, by mapping data without <code>PTE_X</code>, a user program
cannot accidentally jump to an address in the program’s data and start
executing at that address.</p>
<p>In the real world, hardening a process by setting permissions
carefully also aids in defending against security attacks. An adversary
may feed carefully-constructed input to a program (e.g., a Web server)
that triggers a bug in the program in the hope of turning that bug into
an exploit <span class="citation" data-cites="aleph:smashing">(<a
href="#ref-aleph:smashing" role="doc-biblioref">One, n.d.</a>)</span>.
Setting permissions carefully and other techniques, such as randomizing
of the layout of the user address space, make such attacks harder.</p>
<p>The stack is a single page, and is shown with the initial contents as
created by exec. Strings containing the command-line arguments, as well
as an array of pointers to them, are at the very top of the stack. Just
under that are values that allow a program to start at <code>main</code>
as if the function <code>main(argc</code>, <code>argv)</code> had just
been called.</p>
<p>To detect a user stack overflowing the allocated stack memory, xv6
places an inaccessible guard page right below the stack by clearing the
<code>PTE_U</code> flag. If the user stack overflows and the process
tries to use an address below the stack, the hardware will generate a
page-fault exception because the guard page is inaccessible to a program
running in user mode. A real-world operating system might instead
automatically allocate more memory for the user stack when it
overflows.</p>
<p>When a process asks xv6 for more user memory, xv6 grows the process’s
heap. Xv6 first uses <span><code>kalloc</code></span> to allocate
physical pages. It then adds PTEs to the process’s page table that point
to the new physical pages. Xv6 sets the <code>PTE_W</code>,
<code>PTE_R</code>, <code>PTE_U</code>, and <code>PTE_V</code> flags in
these PTEs. Most processes do not use the entire user address space; xv6
leaves <code>PTE_V</code> clear in unused PTEs.</p>
<p>We see here a few nice examples of use of page tables. First,
different processes’ page tables translate user addresses to different
pages of physical memory, so that each process has private user memory.
Second, each process sees its memory as having contiguous virtual
addresses starting at zero, while the process’s physical memory can be
non-contiguous. Third, the kernel maps a page with trampoline code at
the top of the user address space (without <code>PTE_U</code>), thus a
single page of physical memory shows up in all address spaces, but can
be used only by the kernel.</p>
<figure id="fig:processlayout">
<img src="fig/processlayout.svg" />
<figcaption>A process’s user address space, with its initial
stack.</figcaption>
</figure>
</section>
<section id="code-sbrk" class="level2" data-number="3.7">
<h2 data-number="3.7"><span class="header-section-number">3.7</span>
Code: sbrk</h2>
<p><code>sbrk</code> is the system call for a process to shrink or grow
its memory. The system call is implemented by the function
<code>growproc</code> <a
href="xv6-riscv-src/kernel/proc.c#L260"><span>(kernel/proc.c:260)</span></a>.
<code>growproc</code> calls <code>uvmalloc</code> or
<code>uvmdealloc</code>, depending on whether <code>n</code> is positive
or negative. <code>uvmalloc</code> <a
href="xv6-riscv-src/kernel/vm.c#L226"><span>(kernel/vm.c:226)</span></a>
allocates physical memory with <span><code>kalloc</code></span>, and
adds PTEs to the user page table with
<span><code>mappages</code></span>. <code>uvmdealloc</code> calls
<span><code>uvmunmap</code></span> <a
href="xv6-riscv-src/kernel/vm.c#L171"><span>(kernel/vm.c:171)</span></a>,
which uses <span><code>walk</code></span> to find PTEs and
<span><code>kfree</code></span> to free the physical memory they refer
to.</p>
<p>Xv6 uses a process’s page table not just to tell the hardware how to
map user virtual addresses, but also as the only record of which
physical memory pages are allocated to that process. That is the reason
why freeing user memory (in <span><code>uvmunmap</code></span>) requires
examination of the user page table.</p>
</section>
<section id="code-exec" class="level2" data-number="3.8">
<h2 data-number="3.8"><span class="header-section-number">3.8</span>
Code: exec</h2>
<p><code>exec</code> is a system call that replaces a process’s user
address space with data read from a file, called a binary or executable
file. A binary is typically the output of the compiler and linker, and
holds machine instructions and program data. <code>exec</code> <a
href="xv6-riscv-src/kernel/exec.c#L23"><span>(kernel/exec.c:23)</span></a>
opens the named binary <code>path</code> using <code
id="namei_1">namei</code> <a
href="xv6-riscv-src/kernel/exec.c#L36"><span>(kernel/exec.c:36)</span></a>,
which is explained in Chapter <a href="#CH:FS" data-reference-type="ref"
data-reference="CH:FS">8</a>. Then, it reads the ELF header. Xv6
binaries are formatted in the widely-used <em><span
id="ELF_format_1">ELF format</span></em>, defined in <a
href="xv6-riscv-src/kernel/elf.h"><span>(kernel/elf.h)</span></a>. An
ELF binary consists of an ELF header, <code
id="struct_elfhdr_1">struct elfhdr</code> <a
href="xv6-riscv-src/kernel/elf.h#L6"><span>(kernel/elf.h:6)</span></a>,
followed by a sequence of program section headers,
<code>struct proghdr</code> <a
href="xv6-riscv-src/kernel/elf.h#L25"><span>(kernel/elf.h:25)</span></a>.
Each <code>progvhdr</code> describes a section of the application that
must be loaded into memory; xv6 programs have two program section
headers: one for instructions and one for data.</p>
<p>The first step is a quick check that the file probably contains an
ELF binary. An ELF binary starts with the four-byte “magic number”
<code>0x7F</code>, <code>`E'</code>, <code>`L'</code>, <code>`F'</code>,
or <code id="ELF_MAGIC_1">ELF_MAGIC</code> <a
href="xv6-riscv-src/kernel/elf.h#L3"><span>(kernel/elf.h:3)</span></a>.
If the ELF header has the right magic number, <code>exec</code> assumes
that the binary is well-formed.</p>
<p><code>exec</code> allocates a new page table with no user mappings
with <code id="proc_pagetable_1">proc_pagetable</code> <a
href="xv6-riscv-src/kernel/exec.c#L49"><span>(kernel/exec.c:49)</span></a>,
allocates memory for each ELF segment with <code
id="uvmalloc_1">uvmalloc</code> <a
href="xv6-riscv-src/kernel/exec.c#L65"><span>(kernel/exec.c:65)</span></a>,
and loads each segment into memory with <code
id="loadseg_1">loadseg</code> <a
href="xv6-riscv-src/kernel/exec.c#L10"><span>(kernel/exec.c:10)</span></a>.
<code>loadseg</code> uses <code id="walkaddr_1">walkaddr</code> to find
the physical address of the allocated memory at which to write each page
of the ELF segment, and <code id="readi_1">readi</code> to read from the
file.</p>
<p>The program section header for <code id="/init_2">/init</code>, the
first user program created with <code>exec</code>, looks like this:</p>
<div class="footnotesize">
<div class="sourceCode" id="cb18"><pre class="sourceCode C"><code class="sourceCode c"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="pp"># </span><span class="er">objdump -p user/_init</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>user<span class="op">/</span>_init<span class="op">:</span>     file format elf64<span class="op">-</span>little</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>Program Header<span class="op">:</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="bn">0x70000003</span> off    <span class="bn">0x0000000000006bb0</span> vaddr <span class="bn">0x0000000000000000</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>                                       paddr <span class="bn">0x0000000000000000</span> align <span class="dv">2</span><span class="op">**</span><span class="dv">0</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>         filesz <span class="bn">0x000000000000004a</span> memsz <span class="bn">0x0000000000000000</span> flags r<span class="op">--</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    LOAD off    <span class="bn">0x0000000000001000</span> vaddr <span class="bn">0x0000000000000000</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>                                       paddr <span class="bn">0x0000000000000000</span> align <span class="dv">2</span><span class="op">**</span><span class="dv">12</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>         filesz <span class="bn">0x0000000000001000</span> memsz <span class="bn">0x0000000000001000</span> flags r<span class="op">-</span>x</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    LOAD off    <span class="bn">0x0000000000002000</span> vaddr <span class="bn">0x0000000000001000</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>                                       paddr <span class="bn">0x0000000000001000</span> align <span class="dv">2</span><span class="op">**</span><span class="dv">12</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>         filesz <span class="bn">0x0000000000000010</span> memsz <span class="bn">0x0000000000000030</span> flags rw<span class="op">-</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>   STACK off    <span class="bn">0x0000000000000000</span> vaddr <span class="bn">0x0000000000000000</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>                                       paddr <span class="bn">0x0000000000000000</span> align <span class="dv">2</span><span class="op">**</span><span class="dv">4</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>         filesz <span class="bn">0x0000000000000000</span> memsz <span class="bn">0x0000000000000000</span> flags rw<span class="op">-</span></span></code></pre></div>
</div>
<p>We see that the text segment should be loaded at virtual address 0x0
in memory (without write permissions) from content at offset 0x1000 in
the file. We also see that the data should be loaded at address 0x1000,
which is at a page boundary, and without executable permissions.</p>
<p>A program section header’s <code>filesz</code> may be less than the
<code>memsz</code>, indicating that the gap between them should be
filled with zeroes (for C global variables) rather than read from the
file. For <code>/init</code>, the data <code>filesz</code> is 0x10 bytes
and <code>memsz</code> is 0x30 bytes, and thus <code
id="uvmalloc_2">uvmalloc</code> allocates enough physical memory to hold
0x30 bytes, but reads only 0x10 bytes from the file
<code>/init</code>.</p>
<p>Now <code id="exec_7">exec</code> allocates and initializes the user
stack. It allocates just one stack page. <code>exec</code> copies the
argument strings to the top of the stack one at a time, recording the
pointers to them in <code id="ustack_1">ustack</code>. It places a null
pointer at the end of what will be the <code id="argv_1">argv</code>
list passed to <code>main</code>. The first three entries in
<code>ustack</code> are the fake return program counter, <code
id="argc_1">argc</code>, and <code>argv</code> pointer.</p>
<p><code>exec</code> places an inaccessible page just below the stack
page, so that programs that try to use more than one page will fault.
This inaccessible page also allows <code>exec</code> to deal with
arguments that are too large; in that situation, the <code
id="copyout_1">copyout</code> <a
href="xv6-riscv-src/kernel/vm.c#L352"><span>(kernel/vm.c:352)</span></a>
function that <code>exec</code> uses to copy arguments to the stack will
notice that the destination page is not accessible, and will return
-1.</p>
<p>During the preparation of the new memory image, if <code>exec</code>
detects an error like an invalid program segment, it jumps to the label
<code>bad</code>, frees the new image, and returns -1. <code>exec</code>
must wait to free the old image until it is sure that the system call
will succeed: if the old image is gone, the system call cannot return -1
to it. The only error cases in <code>exec</code> happen during the
creation of the image. Once the image is complete, <code>exec</code> can
commit to the new page table <a
href="xv6-riscv-src/kernel/exec.c#L125"><span>(kernel/exec.c:125)</span></a>
and free the old one <a
href="xv6-riscv-src/kernel/exec.c#L129"><span>(kernel/exec.c:129)</span></a>.</p>
<p><code>exec</code> loads bytes from the ELF file into memory at
addresses specified by the ELF file. Users or processes can place
whatever addresses they want into an ELF file. Thus <code>exec</code> is
risky, because the addresses in the ELF file may refer to the kernel,
accidentally or on purpose. The consequences for an unwary kernel could
range from a crash to a malicious subversion of the kernel’s isolation
mechanisms (i.e., a security exploit). Xv6 performs a number of checks
to avoid these risks. For example
<code>if(ph.vaddr + ph.memsz &lt; ph.vaddr)</code> checks for whether
the sum overflows a 64-bit integer. The danger is that a user could
construct an ELF binary with a <code>ph.vaddr</code> that points to a
user-chosen address, and <code>ph.memsz</code> large enough that the sum
overflows to 0x1000, which will look like a valid value. In an older
version of xv6 in which the user address space also contained the kernel
(but not readable/writable in user mode), the user could choose an
address that corresponded to kernel memory and would thus copy data from
the ELF binary into the kernel. In the RISC-V version of xv6 this cannot
happen, because the kernel has its own separate page table;
<code>loadseg</code> loads into the process’s page table, not in the
kernel’s page table.</p>
<p>It is easy for a kernel developer to omit a crucial check, and
real-world kernels have a long history of missing checks whose absence
can be exploited by user programs to obtain kernel privileges. It is
likely that xv6 doesn’t do a complete job of validating user-level data
supplied to the kernel, which a malicious user program might be able to
exploit to circumvent xv6’s isolation.</p>
</section>
<section id="real-world-2" class="level2" data-number="3.9">
<h2 data-number="3.9"><span class="header-section-number">3.9</span>
Real world</h2>
<p>Like most operating systems, xv6 uses the paging hardware for memory
protection and mapping. Most operating systems make far more
sophisticated use of paging than xv6 by combining paging and page-fault
exceptions, which we will discuss in Chapter <a href="#CH:TRAP"
data-reference-type="ref" data-reference="CH:TRAP">4</a>.</p>
<p>Xv6 is simplified by the kernel’s use of a direct map between virtual
and physical addresses, and by its assumption that there is physical RAM
at address 0x8000000, where the kernel expects to be loaded. This works
with QEMU, but on real hardware it turns out to be a bad idea; real
hardware places RAM and devices at unpredictable physical addresses, so
that (for example) there might be no RAM at 0x8000000, where xv6 expect
to be able to store the kernel. More serious kernel designs exploit the
page table to turn arbitrary hardware physical memory layouts into
predictable kernel virtual address layouts.</p>
<p>RISC-V supports protection at the level of physical addresses, but
xv6 doesn’t use that feature.</p>
<p>On machines with lots of memory it might make sense to use RISC-V’s
support for “super pages.” Small pages make sense when physical memory
is small, to allow allocation and page-out to disk with fine
granularity. For example, if a program uses only 8 kilobytes of memory,
giving it a whole 4-megabyte super-page of physical memory is wasteful.
Larger pages make sense on machines with lots of RAM, and may reduce
overhead for page-table manipulation.</p>
<p>The xv6 kernel’s lack of a <span><code>malloc</code></span>-like
allocator that can provide memory for small objects prevents the kernel
from using sophisticated data structures that would require dynamic
allocation. A more elaborate kernel would likely allocate many different
sizes of small blocks, rather than (as in xv6) just 4096-byte blocks; a
real kernel allocator would need to handle small allocations as well as
large ones.</p>
<p>Memory allocation is a perennial hot topic, the basic problems being
efficient use of limited memory and preparing for unknown future
requests <span class="citation" data-cites="knuth">(<a href="#ref-knuth"
role="doc-biblioref">Knuth 1997</a>)</span>. Today people care more
about speed than space efficiency.</p>
</section>
<section id="exercises-2" class="level2" data-number="3.10">
<h2 data-number="3.10"><span class="header-section-number">3.10</span>
Exercises</h2>
<ol>
<li><p>Parse RISC-V’s device tree to find the amount of physical memory
the computer has.</p></li>
<li><p>Write a user program that grows its address space by one byte by
calling <code>sbrk(1)</code>. Run the program and investigate the page
table for the program before the call to <code>sbrk</code> and after the
call to <code>sbrk</code>. How much space has the kernel allocated? What
does the PTE for the new memory contain?</p></li>
<li><p>Modify xv6 to use super pages for the kernel.</p></li>
<li><p>Unix implementations of <code>exec</code> traditionally include
special handling for shell scripts. If the file to execute begins with
the text <code>#!</code>, then the first line is taken to be a program
to run to interpret the file. For example, if <code>exec</code> is
called to run <code>myprog</code> <code>arg1</code> and
<code>myprog</code> ’s first line is <code>#!/interp</code>, then
<code>exec</code> runs <code>/interp</code> with command line
<code>/interp</code> <code>myprog</code> <code>arg1</code>. Implement
support for this convention in xv6.</p></li>
<li><p>Implement address space layout randomization for the
kernel.</p></li>
</ol>
</section>
</section>
<section id="CH:TRAP" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Traps
and system calls</h1>
<p>There are three kinds of event which cause the CPU to set aside
ordinary execution of instructions and force a transfer of control to
special code that handles the event. One situation is a system call,
when a user program executes the <span><code>ecall</code></span>
instruction to ask the kernel to do something for it. Another situation
is an <em><span id="exception_1">exception</span></em>: an instruction
(user or kernel) does something illegal, such as divide by zero or use
an invalid virtual address. The third situation is a device <em><span
id="interrupt_1">interrupt</span></em>, when a device signals that it
needs attention, for example when the disk hardware finishes a read or
write request.</p>
<p>This book uses <em><span id="trap_1">trap</span></em> as a generic
term for these situations. Typically whatever code was executing at the
time of the trap will later need to resume, and shouldn’t need to be
aware that anything special happened. That is, we often want traps to be
transparent; this is particularly important for device interrupts, which
the interrupted code typically doesn’t expect. The usual sequence is
that a trap forces a transfer of control into the kernel; the kernel
saves registers and other state so that execution can be resumed; the
kernel executes appropriate handler code (e.g., a system call
implementation or device driver); the kernel restores the saved state
and returns from the trap; and the original code resumes where it left
off.</p>
<p>Xv6 handles all traps in the kernel; traps are not delivered to user
code. Handling traps in the kernel is natural for system calls. It makes
sense for interrupts since isolation demands that only the kernel be
allowed to use devices, and because the kernel is a convenient mechanism
with which to share devices among multiple processes. It also makes
sense for exceptions since xv6 responds to all exceptions from user
space by killing the offending program.</p>
<p>Xv6 trap handling proceeds in four stages: hardware actions taken by
the RISC-V CPU, some assembly instructions that prepare the way for
kernel C code, a C function that decides what to do with the trap, and
the system call or device-driver service routine. While commonality
among the three trap types suggests that a kernel could handle all traps
with a single code path, it turns out to be convenient to have separate
code for three distinct cases: traps from user space, traps from kernel
space, and timer interrupts. Kernel code (assembler or C) that processes
a trap is often called a <em><span id="handler_1">handler</span></em>;
the first handler instructions are usually written in assembler (rather
than C) and are sometimes called a <em><span
id="vector_1">vector</span></em>.</p>
<section id="risc-v-trap-machinery" class="level2" data-number="4.1">
<h2 data-number="4.1"><span class="header-section-number">4.1</span>
RISC-V trap machinery</h2>
<p>Each RISC-V CPU has a set of control registers that the kernel writes
to tell the CPU how to handle traps, and that the kernel can read to
find out about a trap that has occurred. The RISC-V documents contain
the full story <span class="citation" data-cites="riscv:priv">(<a
href="#ref-riscv:priv" role="doc-biblioref">Waterman, Asanovic, and
Hauser 2021</a>)</span>. <span><code>riscv.h</code></span> <a
href="xv6-riscv-src/kernel/riscv.h#L1"><span>(kernel/riscv.h:1)</span></a>
contains definitions that xv6 uses. Here’s an outline of the most
important registers:</p>
<ul>
<li><p><code id="stvec_1">stvec</code>: The kernel writes the address of
its trap handler here; the RISC-V jumps to the address in
<span><code>stvec</code></span> to handle a trap.</p></li>
<li><p><code id="sepc_1">sepc</code>: When a trap occurs, RISC-V saves
the program counter here (since the <span><code>pc</code></span> is then
overwritten with the value in <span><code>stvec</code></span>). The
<span><code>sret</code></span> (return from trap) instruction copies
<span><code>sepc</code></span> to the <span><code>pc</code></span>. The
kernel can write <span><code>sepc</code></span> to control where
<span><code> sret</code></span> goes.</p></li>
<li><p><code id="scause_1">scause</code>: RISC-V puts a number here that
describes the reason for the trap.</p></li>
<li><p><code id="sscratch_1">sscratch</code>: The trap handler code uses
<span><code>sscratch</code></span> to help it avoid overwriting user
registers before saving them.</p></li>
<li><p><code id="sstatus_1">sstatus</code>: The SIE bit in
<span><code>sstatus</code></span> controls whether device interrupts are
enabled. If the kernel clears SIE, the RISC-V will defer device
interrupts until the kernel sets SIE. The SPP bit indicates whether a
trap came from user mode or supervisor mode, and controls to what mode
<span><code>sret</code></span> returns.</p></li>
</ul>
<p>The above registers relate to traps handled in supervisor mode, and
they cannot be read or written in user mode. There is a similar set of
control registers for traps handled in machine mode; xv6 uses them only
for the special case of timer interrupts.</p>
<p>Each CPU on a multi-core chip has its own set of these registers, and
more than one CPU may be handling a trap at any given time.</p>
<p>When it needs to force a trap, the RISC-V hardware does the following
for all trap types (other than timer interrupts):</p>
<ol>
<li><p>If the trap is a device interrupt, and the
<span><code>sstatus</code></span> SIE bit is clear, don’t do any of the
following.</p></li>
<li><p>Disable interrupts by clearing the SIE bit in
<span><code>sstatus</code></span>.</p></li>
<li><p>Copy the <span><code>pc</code></span> to
<span><code>sepc</code></span>.</p></li>
<li><p>Save the current mode (user or supervisor) in the SPP bit in
<span><code>sstatus</code></span>.</p></li>
<li><p>Set <span><code>scause</code></span> to reflect the trap’s
cause.</p></li>
<li><p>Set the mode to supervisor.</p></li>
<li><p>Copy <span><code>stvec</code></span> to the
<span><code>pc</code></span>.</p></li>
<li><p>Start executing at the new <span><code>pc</code></span>.</p></li>
</ol>
<p>Note that the CPU doesn’t switch to the kernel page table, doesn’t
switch to a stack in the kernel, and doesn’t save any registers other
than the <span><code>pc</code></span>. Kernel software must perform
these tasks. One reason that the CPU does minimal work during a trap is
to provide flexibility to software; for example, some operating systems
omit a page table switch in some situations to increase trap
performance.</p>
<p>It’s worth thinking about whether any of the steps listed above could
be omitted, perhaps in search of faster traps. Though there are
situations in which a simpler sequence can work, many of the steps would
be dangerous to omit in general. For example, suppose that the CPU
didn’t switch program counters. Then a trap from user space could switch
to supervisor mode while still running user instructions. Those user
instructions could break user/kernel isolation, for example by modifying
the <span><code>satp</code></span> register to point to a page table
that allowed accessing all of physical memory. It is thus important that
the CPU switch to a kernel-specified instruction address, namely
<span><code> stvec</code></span>.</p>
</section>
<section id="traps-from-user-space" class="level2" data-number="4.2">
<h2 data-number="4.2"><span class="header-section-number">4.2</span>
Traps from user space</h2>
<p>Xv6 handles traps differently depending on whether the trap occurs
while executing in the kernel or in user code. Here is the story for
traps from user code; Section <a href="#s:ktraps"
data-reference-type="ref" data-reference="s:ktraps">4.5</a> describes
traps from kernel code.</p>
<p>A trap may occur while executing in user space if the user program
makes a system call (<span><code>ecall</code></span> instruction), or
does something illegal, or if a device interrupts. The high-level path
of a trap from user space is <span><code>uservec</code></span> <a
href="xv6-riscv-src/kernel/trampoline.S#L21"><span>(kernel/trampoline.S:21)</span></a>,
then <span><code>usertrap</code></span> <a
href="xv6-riscv-src/kernel/trap.c#L37"><span>(kernel/trap.c:37)</span></a>;
and when returning, <span><code>usertrapret</code></span> <a
href="xv6-riscv-src/kernel/trap.c#L90"><span>(kernel/trap.c:90)</span></a>
and then <span><code>userret</code></span> <a
href="xv6-riscv-src/kernel/trampoline.S#L101"><span>(kernel/trampoline.S:101)</span></a>.</p>
<p>A major constraint on the design of xv6’s trap handling is the fact
that the RISC-V hardware does not switch page tables when it forces a
trap. This means that the trap handler address in
<span><code>stvec</code></span> must have a valid mapping in the user
page table, since that’s the page table in force when the trap handling
code starts executing. Furthermore, xv6’s trap handling code needs to
switch to the kernel page table; in order to be able to continue
executing after that switch, the kernel page table must also have a
mapping for the handler pointed to by
<span><code>stvec</code></span>.</p>
<p>Xv6 satisfies these requirements using a <em><span
id="trampoline_2">trampoline</span></em> page. The trampoline page
contains <span><code>uservec</code></span>, the xv6 trap handling code
that <span><code>stvec</code></span> points to. The trampoline page is
mapped in every process’s page table at address <code
id="TRAMPOLINE_1">TRAMPOLINE</code>, which is at the top of the virtual
address space so that it will be above memory that programs use for
themselves. The trampoline page is also mapped at address
<span><code>TRAMPOLINE</code></span> in the kernel page table. See
Figure <a href="#fig:as" data-reference-type="ref"
data-reference="fig:as">2.3</a> and Figure <a href="#fig:xv6_layout"
data-reference-type="ref" data-reference="fig:xv6_layout">3.3</a>.
Because the trampoline page is mapped in the user page table, without
the <span><code>PTE_U</code></span> flag, traps can start executing
there in supervisor mode. Because the trampoline page is mapped at the
same address in the kernel address space, the trap handler can continue
to execute after it switches to the kernel page table.</p>
<p>The code for the <span><code>uservec</code></span> trap handler is in
<span><code>trampoline.S</code></span> <a
href="xv6-riscv-src/kernel/trampoline.S#L21"><span>(kernel/trampoline.S:21)</span></a>.
When <span><code>uservec</code></span> starts, all 32 registers contain
values owned by the interrupted user code. These 32 values need to be
saved somewhere in memory, so that they can be restored when the trap
returns to user space. Storing to memory requires use of a register to
hold the address, but at this point there are no general-purpose
registers available! Luckily RISC-V provides a helping hand in the form
of the <span><code>sscratch</code></span> register. The
<span><code>csrw</code></span> instruction at the start of
<span><code>uservec</code></span> saves <span><code>a0</code></span> in
<span><code> sscratch</code></span>. Now
<span><code>uservec</code></span> has one register
(<span><code>a0</code></span>) to play with.</p>
<p><span><code>uservec</code></span>’s next task is to save the 32 user
registers. The kernel allocates, for each process, a page of memory for
a <span><code>trapframe</code></span> structure that (among other
things) has space to save the 32 user registers <a
href="xv6-riscv-src/kernel/proc.h#L43"><span>(kernel/proc.h:43)</span></a>.
Because <span><code>satp</code></span> still refers to the user page
table, <span><code>uservec</code></span> needs the trapframe to be
mapped in the user address space. Xv6 maps each process’s trapframe at
virtual address <span><code>TRAPFRAME</code></span> in that process’s
user page table; <span><code>TRAPFRAME</code></span> is just below
<span><code>TRAMPOLINE</code></span>. The process’s
<span><code>p-&gt;trapframe</code></span> also points to the trapframe,
though at its physical address so the kernel can use it through the
kernel page table.</p>
<p>Thus <span><code>uservec</code></span> loads address
<span><code>TRAPFRAME</code></span> into <span><code>a0</code></span>
and saves all the user registers there, including the user’s
<span><code>a0</code></span>, read back from
<span><code>sscratch</code></span>.</p>
<p>The <span><code>trapframe</code></span> contains the address of the
current process’s kernel stack, the current CPU’s hartid, the address of
the <span><code>usertrap</code></span> function, and the address of the
kernel page table. <span><code>uservec</code></span> retrieves these
values, switches <span><code>satp</code></span> to the kernel page
table, and calls <span><code>usertrap</code></span>.</p>
<p>The job of <span><code>usertrap</code></span> is to determine the
cause of the trap, process it, and return <a
href="xv6-riscv-src/kernel/trap.c#L37"><span>(kernel/trap.c:37)</span></a>.
It first changes <span><code>stvec</code></span> so that a trap while in
the kernel will be handled by <span><code>kernelvec</code></span> rather
than <span><code>uservec</code></span>. It saves the
<span><code>sepc</code></span> register (the saved user program
counter), because <span><code>usertrap</code></span> might call
<code>yield</code> to switch to another process’s kernel thread, and
that process might return to user space, in the process of which it will
modify <code>sepc</code>. If the trap is a system call,
<span><code>usertrap</code></span> calls
<span><code>syscall</code></span> to handle it; if a device interrupt,
<span><code>devintr</code></span>; otherwise it’s an exception, and the
kernel kills the faulting process. The system call path adds four to the
saved user program counter because RISC-V, in the case of a system call,
leaves the program pointer pointing to the
<span><code>ecall</code></span> instruction but user code needs to
resume executing at the subsequent instruction. On the way out,
<span><code>usertrap</code></span> checks if the process has been killed
or should yield the CPU (if this trap is a timer interrupt).</p>
<p>The first step in returning to user space is the call to
<span><code>usertrapret</code></span> <a
href="xv6-riscv-src/kernel/trap.c#L90"><span>(kernel/trap.c:90)</span></a>.
This function sets up the RISC-V control registers to prepare for a
future trap from user space. This involves changing
<span><code>stvec</code></span> to refer to
<span><code>uservec</code></span>, preparing the trapframe fields that
<span><code>uservec</code></span> relies on, and setting
<span><code>sepc</code></span> to the previously saved user program
counter. At the end, <span><code>usertrapret</code></span> calls
<span><code>userret</code></span> on the trampoline page that is mapped
in both user and kernel page tables; the reason is that assembly code in
<span><code>userret</code></span> will switch page tables.</p>
<p><span><code>usertrapret</code></span>’s call to
<span><code>userret</code></span> passes a pointer to the process’s user
page table in <span><code>a0</code></span> <a
href="xv6-riscv-src/kernel/trampoline.S#L101"><span>(kernel/trampoline.S:101)</span></a>.
<span><code>userret</code></span> switches
<span><code>satp</code></span> to the process’s user page table. Recall
that the user page table maps both the trampoline page and
<span><code>TRAPFRAME</code></span>, but nothing else from the kernel.
The trampoline page mapping at the same virtual address in user and
kernel page tables allows <span><code>userret</code></span> to keep
executing after changing <span><code>satp</code></span>. From this point
on, the only data <span><code>userret</code></span> can use is the
register contents and the content of the trapframe.
<span><code>userret</code></span> loads the
<span><code>TRAPFRAME</code></span> address into
<span><code>a0</code></span>, restores saved user registers from the
trapframe via <span><code>a0</code></span>, restores the saved user
<span><code>a0</code></span>, and executes
<span><code>sret</code></span> to return to user space.</p>
</section>
<section id="code-calling-system-calls" class="level2"
data-number="4.3">
<h2 data-number="4.3"><span class="header-section-number">4.3</span>
Code: Calling system calls</h2>
<p>Chapter <a href="#CH:FIRST" data-reference-type="ref"
data-reference="CH:FIRST">2</a> ended with <code
id="initcode.S_2">initcode.S</code> invoking the
<span><code>exec</code></span> system call <a
href="xv6-riscv-src/user/initcode.S#L11"><span>(user/initcode.S:11)</span></a>.
Let’s look at how the user call makes its way to the
<span><code>exec</code></span> system call’s implementation in the
kernel.</p>
<p><span><code>initcode.S</code></span> places the arguments for <code
id="exec_8">exec</code> in registers <span><code>a0</code></span> and
<span><code>a1</code></span>, and puts the system call number in
<code>a7</code>. System call numbers match the entries in the
<span><code>syscalls</code></span> array, a table of function pointers
<a
href="xv6-riscv-src/kernel/syscall.c#L107"><span>(kernel/syscall.c:107)</span></a>.
The <code>ecall</code> instruction traps into the kernel and causes
<span><code>uservec</code></span>, <span><code>usertrap</code></span>,
and then <span><code>syscall</code></span> to execute, as we saw
above.</p>
<p><code id="syscall_1">syscall</code> <a
href="xv6-riscv-src/kernel/syscall.c#L132"><span>(kernel/syscall.c:132)</span></a>
retrieves the system call number from the saved <code>a7</code> in the
trapframe and uses it to index into <span><code>syscalls</code></span>.
For the first system call, <code>a7</code> contains <code
id="SYS_exec_1">SYS_exec</code> <a
href="xv6-riscv-src/kernel/syscall.h#L8"><span>(kernel/syscall.h:8)</span></a>,
resulting in a call to the system call implementation function
<code>sys_exec</code>.</p>
<p>When <code>sys_exec</code> returns, <code>syscall</code> records its
return value in <code>p-&gt;trapframe-&gt;a0</code>. This will cause the
original user-space call to <span><code>exec()</code></span> to return
that value, since the C calling convention on RISC-V places return
values in <span><code>a0</code></span>. System calls conventionally
return negative numbers to indicate errors, and zero or positive numbers
for success. If the system call number is invalid, <code>syscall</code>
prints an error and returns <span class="math inline"> − 1</span>.</p>
</section>
<section id="code-system-call-arguments" class="level2"
data-number="4.4">
<h2 data-number="4.4"><span class="header-section-number">4.4</span>
Code: System call arguments</h2>
<p>System call implementations in the kernel need to find the arguments
passed by user code. Because user code calls system call wrapper
functions, the arguments are initially where the RISC-V C calling
convention places them: in registers. The kernel trap code saves user
registers to the current process’s trap frame, where kernel code can
find them. The kernel functions <code>argint</code>,
<code>argaddr</code>, and <code>argfd</code> retrieve the <em>n</em> ’th
system call argument from the trap frame as an integer, pointer, or a
file descriptor. They all call <span><code>argraw</code></span> to
retrieve the appropriate saved user register <a
href="xv6-riscv-src/kernel/syscall.c#L34"><span>(kernel/syscall.c:34)</span></a>.</p>
<p>Some system calls pass pointers as arguments, and the kernel must use
those pointers to read or write user memory. The
<span><code>exec</code></span> system call, for example, passes the
kernel an array of pointers referring to string arguments in user space.
These pointers pose two challenges. First, the user program may be buggy
or malicious, and may pass the kernel an invalid pointer or a pointer
intended to trick the kernel into accessing kernel memory instead of
user memory. Second, the xv6 kernel page table mappings are not the same
as the user page table mappings, so the kernel cannot use ordinary
instructions to load or store from user-supplied addresses.</p>
<p>The kernel implements functions that safely transfer data to and from
user-supplied addresses. <span><code>fetchstr</code></span> is an
example <a
href="xv6-riscv-src/kernel/syscall.c#L25"><span>(kernel/syscall.c:25)</span></a>.
File system calls such as <span><code>exec</code></span> use
<span><code>fetchstr</code></span> to retrieve string file-name
arguments from user space. <code>fetchstr</code> calls
<code>copyinstr</code> to do the hard work.</p>
<p><code id="copyinstr_1">copyinstr</code> <a
href="xv6-riscv-src/kernel/vm.c#L403"><span>(kernel/vm.c:403)</span></a>
copies up to <code>max</code> bytes to <code>dst</code> from virtual
address <code>srcva</code> in the user page table
<code>pagetable</code>. Since <code>pagetable</code> is
<span><em>not</em></span> the current page table, <code>copyinstr</code>
uses <span><code>walkaddr</code></span> (which calls
<span><code>walk</code></span>) to look up <code>srcva</code> in
<code>pagetable</code>, yielding physical address <code>pa0</code>. The
kernel maps each physical RAM address to the corresponding kernel
virtual address, so <span><code>copyinstr</code></span> can directly
copy string bytes from <span><code>pa0</code></span> to
<span><code>dst</code></span>. <span><code>walkaddr</code></span> <a
href="xv6-riscv-src/kernel/vm.c#L109"><span>(kernel/vm.c:109)</span></a>
checks that the user-supplied virtual address is part of the process’s
user address space, so programs cannot trick the kernel into reading
other memory. A similar function, <span><code>copyout</code></span>,
copies data from the kernel to a user-supplied address.</p>
</section>
<section id="s:ktraps" class="level2" data-number="4.5">
<h2 data-number="4.5"><span class="header-section-number">4.5</span>
Traps from kernel space</h2>
<p>Xv6 configures the CPU trap registers somewhat differently depending
on whether user or kernel code is executing. When the kernel is
executing on a CPU, the kernel points <span><code>stvec</code></span> to
the assembly code at <span><code>kernelvec</code></span> <a
href="xv6-riscv-src/kernel/kernelvec.S#L12"><span>(kernel/kernelvec.S:12)</span></a>.
Since xv6 is already in the kernel, <span><code>kernelvec</code></span>
can rely on <span><code>satp</code></span> being set to the kernel page
table, and on the stack pointer referring to a valid kernel stack.
<span><code>kernelvec</code></span> pushes all 32 registers onto the
stack, from which it will later restore them so that the interrupted
kernel code can resume without disturbance.</p>
<p><span><code>kernelvec</code></span> saves the registers on the stack
of the interrupted kernel thread, which makes sense because the register
values belong to that thread. This is particularly important if the trap
causes a switch to a different thread – in that case the trap will
actually return from the stack of the new thread, leaving the
interrupted thread’s saved registers safely on its stack.</p>
<p><span><code>kernelvec</code></span> jumps to
<span><code>kerneltrap</code></span> <a
href="xv6-riscv-src/kernel/trap.c#L135"><span>(kernel/trap.c:135)</span></a>
after saving registers. <span><code>kerneltrap</code></span> is prepared
for two types of traps: device interrupts and exceptions. It calls
<span><code>devintr</code></span> <a
href="xv6-riscv-src/kernel/trap.c#L178"><span>(kernel/trap.c:178)</span></a>
to check for and handle the former. If the trap isn’t a device
interrupt, it must be an exception, and that is always a fatal error if
it occurs in the xv6 kernel; the kernel calls <code>panic</code> and
stops executing.</p>
<p>If <span><code>kerneltrap</code></span> was called due to a timer
interrupt, and a process’s kernel thread is running (as opposed to a
scheduler thread), <span><code>kerneltrap</code></span> calls
<span><code>yield</code></span> to give other threads a chance to run.
At some point one of those threads will yield, and let our thread and
its <span><code>kerneltrap</code></span> resume again. Chapter <a
href="#CH:SCHED" data-reference-type="ref"
data-reference="CH:SCHED">7</a> explains what happens in
<span><code>yield</code></span>.</p>
<p>When <span><code>kerneltrap</code></span>’s work is done, it needs to
return to whatever code was interrupted by the trap. Because a
<span><code>yield</code></span> may have disturbed
<span><code>sepc</code></span> and the previous mode in
<span><code>sstatus</code></span>, <span><code>kerneltrap</code></span>
saves them when it starts. It now restores those control registers and
returns to <span><code>kernelvec</code></span> <a
href="xv6-riscv-src/kernel/kernelvec.S#L50"><span>(kernel/kernelvec.S:50)</span></a>.
<span><code>kernelvec</code></span> pops the saved registers from the
stack and executes <span><code>sret</code></span>, which copies
<span><code>sepc</code></span> to <span><code>pc</code></span> and
resumes the interrupted kernel code.</p>
<p>It’s worth thinking through how the trap return happens if
<span><code>kerneltrap</code></span> called
<span><code>yield</code></span> due to a timer interrupt.</p>
<p>Xv6 sets a CPU’s <span><code>stvec</code></span> to
<span><code>kernelvec</code></span> when that CPU enters the kernel from
user space; you can see this in <span><code>usertrap</code></span> <a
href="xv6-riscv-src/kernel/trap.c#L29"><span>(kernel/trap.c:29)</span></a>.
There’s a window of time when the kernel has started executing but
<span><code>stvec</code></span> is still set to
<span><code>uservec</code></span>, and it’s crucial that no device
interrupt occur during that window. Luckily the RISC-V always disables
interrupts when it starts to take a trap, and xv6 doesn’t enable them
again until after it sets <span><code>stvec</code></span>.</p>
</section>
<section id="sec:pagefaults" class="level2" data-number="4.6">
<h2 data-number="4.6"><span class="header-section-number">4.6</span>
Page-fault exceptions</h2>
<p>Xv6’s response to exceptions is quite boring: if an exception happens
in user space, the kernel kills the faulting process. If an exception
happens in the kernel, the kernel panics. Real operating systems often
respond in much more interesting ways.</p>
<p>As an example, many kernels use page faults to implement <em><span
id="copy-on-write_(COW)_fork_1">copy-on-write (COW) fork</span></em>. To
explain copy-on-write fork, consider xv6’s <code>fork</code>, described
in Chapter <a href="#CH:MEM" data-reference-type="ref"
data-reference="CH:MEM">3</a>. <code>fork</code> causes the child’s
initial memory content to be the same as the parent’s at the time of the
fork. Xv6 implements fork with <code>uvmcopy</code> <a
href="xv6-riscv-src/kernel/vm.c#L306"><span>(kernel/vm.c:306)</span></a>,
which allocates physical memory for the child and copies the parent’s
memory into it. It would be more efficient if the child and parent could
share the parent’s physical memory. A straightforward implementation of
this would not work, however, since it would cause the parent and child
to disrupt each other’s execution with their writes to the shared stack
and heap.</p>
<p>Parent and child can safely share physical memory by appropriate use
of page-table permissions and page faults. The CPU raises a <em><span
id="page-fault_exception_2">page-fault exception</span></em> when a
virtual address is used that has no mapping in the page table, or has a
mapping whose <code>PTE_V</code> flag is clear, or a mapping whose
permission bits (<code>PTE_R</code>, <code>PTE_W</code>,
<code>PTE_X</code>, <code>PTE_U</code>) forbid the operation being
attempted. RISC-V distinguishes three kinds of page fault: load page
faults (when a load instruction cannot translate its virtual address),
store page faults (when a store instruction cannot translate its virtual
address), and instruction page faults (when the address in the program
counter doesn’t translate). The <code>scause</code> register indicates
the type of the page fault and the <code id="stval_1">stval</code>
register contains the address that couldn’t be translated.</p>
<p>The basic plan in COW fork is for the parent and child to initially
share all physical pages, but for each to map them read-only (with the
<code>PTE_W</code> flag clear). Parent and child can read from the
shared physical memory. If either writes a given page, the RISC-V CPU
raises a page-fault exception. The kernel’s trap handler responds by
allocating a new page of physical memory and copying into it the
physical page that the faulted address maps to. The kernel changes the
relevant PTE in the faulting process’s page table to point to the copy
and to allow writes as well as reads, and then resumes the faulting
process at the instruction that caused the fault. Because the PTE allows
writes, the re-executed instruction will now execute without a fault.
Copy-on-write requires book-keeping to help decide when physical pages
can be freed, since each page can be referenced by a varying number of
page tables depending on the history of forks, page faults, execs, and
exits. This book-keeping allows an important optimization: if a process
incurs a store page fault and the physical page is only referred to from
that process’s page table, no copy is needed.</p>
<p>Copy-on-write makes <code>fork</code> faster, since <code>fork</code>
need not copy memory. Some of the memory will have to be copied later,
when written, but it’s often the case that most of the memory never has
to be copied. A common example is <code>fork</code> followed by
<code>exec</code>: a few pages may be written after the
<code>fork</code>, but then the child’s <code>exec</code> releases the
bulk of the memory inherited from the parent. Copy-on-write
<code>fork</code> eliminates the need to ever copy this memory.
Furthermore, COW fork is transparent: no modifications to applications
are necessary for them to benefit.</p>
<p>The combination of page tables and page faults opens up a wide range
of interesting possibilities in addition to COW fork. Another
widely-used feature is called <em><span id="lazy_allocation_1">lazy
allocation</span></em>, which has two parts. First, when an application
asks for more memory by calling <code>sbrk</code>, the kernel notes the
increase in size, but does not allocate physical memory and does not
create PTEs for the new range of virtual addresses. Second, on a page
fault on one of those new addresses, the kernel allocates a page of
physical memory and maps it into the page table. Like COW fork, the
kernel can implement lazy allocation transparently to applications.</p>
<p>Since applications often ask for more memory than they need, lazy
allocation is a win: the kernel doesn’t have to do any work at all for
pages that the application never uses. Furthermore, if the application
is asking to grow the address space by a lot, then <code>sbrk</code>
without lazy allocation is expensive: if an application asks for a
gigabyte of memory, the kernel has to allocate and zero 262,144
4096-byte pages. Lazy allocation allows this cost to be spread over
time. On the other hand, lazy allocation incurs the extra overhead of
page faults, which involve a kernel/user transition. Operating systems
can reduce this cost by allocating a batch of consecutive pages per page
fault instead of one page and by specializing the kernel entry/exit code
for such page-faults.</p>
<p>Yet another widely-used feature that exploits page faults is
<em><span id="demand_paging_1">demand paging</span></em>. In
<code>exec</code>, xv6 loads all text and data of an application eagerly
into memory. Since applications can be large and reading from disk is
expensive, this startup cost may be noticeable to users: when the user
starts a large application from the shell, it may take a long time
before user sees a response. To improve response time, a modern kernel
creates the page table for the user address space, but marks the PTEs
for the pages invalid. On a page fault, the kernel reads the content of
the page from disk and maps it into the user address space. Like COW
fork and lazy allocation, the kernel can implement this feature
transparently to applications.</p>
<p>The programs running on a computer may need more memory than the
computer has RAM. To cope gracefully, the operating system may implement
<em><span id="paging_to_disk_1">paging to disk</span></em>. The idea is
to store only a fraction of user pages in RAM, and to store the rest on
disk in a <em><span id="paging_area_1">paging area</span></em>. The
kernel marks PTEs that correspond to memory stored in the paging area
(and thus not in RAM) as invalid. If an application tries to use one of
the pages that has been <span><em>paged out</em></span> to disk, the
application will incur a page fault, and the page must be
<span><em>paged in</em></span>: the kernel trap handler will allocate a
page of physical RAM, read the page from disk into the RAM, and modify
the relevant PTE to point to the RAM.</p>
<p>What happens if a page needs to be paged in, but there is no free
physical RAM? In that case, the kernel must first free a physical page
by paging it out or <span><em>evicting</em></span> it to the paging area
on disk, and marking the PTEs referring to that physical page as
invalid. Eviction is expensive, so paging performs best if it’s
infrequent: if applications use only a subset of their memory pages and
the union of the subsets fits in RAM. This property is often referred to
as having good locality of reference. As with many virtual memory
techniques, kernels usually implement paging to disk in a way that’s
transparent to applications.</p>
<p>Computers often operate with little or no <span><em>free</em></span>
physical memory, regardless of how much RAM the hardware provides. For
example, cloud providers multiplex many customers on a single machine to
use their hardware cost-effectively. As another example, users run many
applications on smart phones in a small amount of physical memory. In
such settings allocating a page may require first evicting an existing
page. Thus, when free physical memory is scarce, allocation is
expensive.</p>
<p>Lazy allocation and demand paging are particularly advantageous when
free memory is scarce. Eagerly allocating memory in <code>sbrk</code> or
<code>exec</code> incurs the extra cost of eviction to make memory
available. Furthermore, there is a risk that the eager work is wasted,
because before the application uses the page, the operating system may
have evicted it.</p>
<p>Other features that combine paging and page-fault exceptions include
automatically extending stacks and memory-mapped files.</p>
</section>
<section id="real-world-3" class="level2" data-number="4.7">
<h2 data-number="4.7"><span class="header-section-number">4.7</span>
Real world</h2>
<p>The trampoline and trapframe may seem excessively complex. A driving
force is that the RISC-V intentionally does as little as it can when
forcing a trap, to allow the possibility of very fast trap handling,
which turns out to be important. As a result, the first few instructions
of the kernel trap handler effectively have to execute in the user
environment: the user page table, and user register contents. And the
trap handler is initially ignorant of useful facts such as the identity
of the process that’s running or the address of the kernel page table. A
solution is possible because RISC-V provides protected places in which
the kernel can stash away information before entering user space: the
<span><code>sscratch</code></span> register, and user page table entries
that point to kernel memory but are protected by lack of
<code>PTE_U</code>. Xv6’s trampoline and trapframe exploit these RISC-V
features.</p>
<p>The need for special trampoline pages could be eliminated if kernel
memory were mapped into every process’s user page table (with
appropriate PTE permission flags). That would also eliminate the need
for a page table switch when trapping from user space into the kernel.
That in turn would allow system call implementations in the kernel to
take advantage of the current process’s user memory being mapped,
allowing kernel code to directly dereference user pointers. Many
operating systems have used these ideas to increase efficiency. Xv6
avoids them in order to reduce the chances of security bugs in the
kernel due to inadvertent use of user pointers, and to reduce some
complexity that would be required to ensure that user and kernel virtual
addresses don’t overlap.</p>
<p>Production operating systems implement copy-on-write fork, lazy
allocation, demand paging, paging to disk, memory-mapped files, etc.
Furthermore, production operating systems will try to use all of
physical memory, either for applications or caches (e.g., the buffer
cache of the file system, which we will cover later in Section <a
href="#s:bcache" data-reference-type="ref"
data-reference="s:bcache">8.2</a>). Xv6 is naïve in this regard: you
want your operating system to use the physical memory you paid for, but
xv6 doesn’t. Furthermore, if xv6 runs out of memory, it returns an error
to the running application or kills it, instead of, for example,
evicting a page of another application.</p>
</section>
<section id="exercises-3" class="level2" data-number="4.8">
<h2 data-number="4.8"><span class="header-section-number">4.8</span>
Exercises</h2>
<ol>
<li><p>The functions <span><code>copyin</code></span> and
<span><code>copyinstr</code></span> walk the user page table in
software. Set up the kernel page table so that the kernel has the user
program mapped, and <span><code>copyin</code></span> and
<span><code> copyinstr</code></span> can use
<span><code>memcpy</code></span> to copy system call arguments into
kernel space, relying on the hardware to do the page table
walk.</p></li>
<li><p>Implement lazy memory allocation.</p></li>
<li><p>Implement COW fork.</p></li>
<li><p>Is there a way to eliminate the special
<span><code> TRAPFRAME</code></span> page mapping in every user address
space? For example, could <span><code>uservec</code></span> be modified
to simply push the 32 user registers onto the kernel stack, or store
them in the <span><code>proc</code></span> structure?</p></li>
<li><p>Could xv6 be modified to eliminate the special
<span><code> TRAMPOLINE</code></span> page mapping?</p></li>
</ol>
</section>
</section>
<section id="CH:INTERRUPT" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span>
Interrupts and device drivers</h1>
<p>A <em><span id="driver_1">driver</span></em> is the code in an
operating system that manages a particular device: it configures the
device hardware, tells the device to perform operations, handles the
resulting interrupts, and interacts with processes that may be waiting
for I/O from the device. Driver code can be tricky because a driver
executes concurrently with the device that it manages. In addition, the
driver must understand the device’s hardware interface, which can be
complex and poorly documented.</p>
<p>Devices that need attention from the operating system can usually be
configured to generate interrupts, which are one type of trap. The
kernel trap handling code recognizes when a device has raised an
interrupt and calls the driver’s interrupt handler; in xv6, this
dispatch happens in <span><code>devintr</code></span> <a
href="xv6-riscv-src/kernel/trap.c#L178"><span>(kernel/trap.c:178)</span></a>.</p>
<p>Many device drivers execute code in two contexts: a <em><span
id="top_half_1">top half</span></em> that runs in a process’s kernel
thread, and a <em><span id="bottom_half_1">bottom half</span></em> that
executes at interrupt time. The top half is called via system calls such
as <span><code>read</code></span> and <span><code>write</code></span>
that want the device to perform I/O. This code may ask the hardware to
start an operation (e.g., ask the disk to read a block); then the code
waits for the operation to complete. Eventually the device completes the
operation and raises an interrupt. The driver’s interrupt handler,
acting as the bottom half, figures out what operation has completed,
wakes up a waiting process if appropriate, and tells the hardware to
start work on any waiting next operation.</p>
<section id="code-console-input" class="level2" data-number="5.1">
<h2 data-number="5.1"><span class="header-section-number">5.1</span>
Code: Console input</h2>
<p>The console driver <a
href="xv6-riscv-src/kernel/console.c"><span>(kernel/console.c)</span></a>
is a simple illustration of driver structure. The console driver accepts
characters typed by a human, via the <em><span
id="UART_1">UART</span></em> serial-port hardware attached to the
RISC-V. The console driver accumulates a line of input at a time,
processing special input characters such as backspace and control-u.
User processes, such as the shell, use the
<span><code>read</code></span> system call to fetch lines of input from
the console. When you type input to xv6 in QEMU, your keystrokes are
delivered to xv6 by way of QEMU’s simulated UART hardware.</p>
<p>The UART hardware that the driver talks to is a 16550 chip <span
class="citation" data-cites="ns16550a">(<a href="#ref-ns16550a"
role="doc-biblioref">Michael and Durich 1987</a>)</span> emulated by
QEMU. On a real computer, a 16550 would manage an RS232 serial link
connecting to a terminal or other computer. When running QEMU, it’s
connected to your keyboard and display.</p>
<p>The UART hardware appears to software as a set of <em><span
id="memory-mapped_2">memory-mapped</span></em> control registers. That
is, there are some physical addresses that RISC-V hardware connects to
the UART device, so that loads and stores interact with the device
hardware rather than RAM. The memory-mapped addresses for the UART start
at 0x10000000, or <span><code>UART0</code></span> <a
href="xv6-riscv-src/kernel/memlayout.h#L21"><span>(kernel/memlayout.h:21)</span></a>.
There are a handful of UART control registers, each the width of a byte.
Their offsets from <span><code>UART0</code></span> are defined in <a
href="xv6-riscv-src/kernel/uart.c#L22"><span>(kernel/uart.c:22)</span></a>.
For example, the <span><code>LSR</code></span> register contain bits
that indicate whether input characters are waiting to be read by the
software. These characters (if any) are available for reading from the
<span><code>RHR</code></span> register. Each time one is read, the UART
hardware deletes it from an internal FIFO of waiting characters, and
clears the “ready” bit in <span><code>LSR</code></span> when the FIFO is
empty. The UART transmit hardware is largely independent of the receive
hardware; if software writes a byte to the
<span><code>THR</code></span>, the UART transmit that byte.</p>
<p>Xv6’s <span><code>main</code></span> calls
<span><code>consoleinit</code></span> <a
href="xv6-riscv-src/kernel/console.c#L182"><span>(kernel/console.c:182)</span></a>
to initialize the UART hardware. This code configures the UART to
generate a receive interrupt when the UART receives each byte of input,
and a <em><span id="transmit_complete_1">transmit complete</span></em>
interrupt each time the UART finishes sending a byte of output <a
href="xv6-riscv-src/kernel/uart.c#L53"><span>(kernel/uart.c:53)</span></a>.</p>
<p>The xv6 shell reads from the console by way of a file descriptor
opened by <span><code>init.c</code></span> <a
href="xv6-riscv-src/user/init.c#L19"><span>(user/init.c:19)</span></a>.
Calls to the <span><code>read</code></span> system call make their way
through the kernel to <span><code> consoleread</code></span> <a
href="xv6-riscv-src/kernel/console.c#L80"><span>(kernel/console.c:80)</span></a>.
<span><code> consoleread</code></span> waits for input to arrive (via
interrupts) and be buffered in <span><code>cons.buf</code></span>,
copies the input to user space, and (after a whole line has arrived)
returns to the user process. If the user hasn’t typed a full line yet,
any reading processes will wait in the <span><code>sleep</code></span>
call <a
href="xv6-riscv-src/kernel/console.c#L96"><span>(kernel/console.c:96)</span></a>
(Chapter <a href="#CH:SCHED" data-reference-type="ref"
data-reference="CH:SCHED">7</a> explains the details of
<span><code>sleep</code></span>).</p>
<p>When the user types a character, the UART hardware asks the RISC-V to
raise an interrupt, which activates xv6’s trap handler. The trap handler
calls <span><code>devintr</code></span> <a
href="xv6-riscv-src/kernel/trap.c#L178"><span>(kernel/trap.c:178)</span></a>,
which looks at the RISC-V <span><code>scause</code></span> register to
discover that the interrupt is from an external device. Then it asks a
hardware unit called the PLIC <span class="citation"
data-cites="riscv:priv">(<a href="#ref-riscv:priv"
role="doc-biblioref">Waterman, Asanovic, and Hauser 2021</a>)</span> to
tell it which device interrupted <a
href="xv6-riscv-src/kernel/trap.c#L187"><span>(kernel/trap.c:187)</span></a>.
If it was the UART, <span><code>devintr</code></span> calls
<span><code>uartintr</code></span>.</p>
<p><span><code>uartintr</code></span> <a
href="xv6-riscv-src/kernel/uart.c#L176"><span>(kernel/uart.c:176)</span></a>
reads any waiting input characters from the UART hardware and hands them
to <span><code>consoleintr</code></span> <a
href="xv6-riscv-src/kernel/console.c#L136"><span>(kernel/console.c:136)</span></a>;
it doesn’t wait for characters, since future input will raise a new
interrupt. The job of <span><code>consoleintr</code></span> is to
accumulate input characters in <span><code>cons.buf</code></span> until
a whole line arrives. <span><code>consoleintr</code></span> treats
backspace and a few other characters specially. When a newline arrives,
<span><code>consoleintr</code></span> wakes up a waiting
<span><code>consoleread</code></span> (if there is one).</p>
<p>Once woken, <span><code>consoleread</code></span> will observe a full
line in <span><code> cons.buf</code></span>, copy it to user space, and
return (via the system call machinery) to user space.</p>
</section>
<section id="code-console-output" class="level2" data-number="5.2">
<h2 data-number="5.2"><span class="header-section-number">5.2</span>
Code: Console output</h2>
<p>A <span><code>write</code></span> system call on a file descriptor
connected to the console eventually arrives at
<span><code>uartputc</code></span> <a
href="xv6-riscv-src/kernel/uart.c#L87"><span>(kernel/uart.c:87)</span></a>.
The device driver maintains an output buffer
(<span><code>uart_tx_buf</code></span>) so that writing processes do not
have to wait for the UART to finish sending; instead,
<span><code>uartputc</code></span> appends each character to the buffer,
calls <span><code>uartstart</code></span> to start the device
transmitting (if it isn’t already), and returns. The only situation in
which <span><code>uartputc</code></span> waits is if the buffer is
already full.</p>
<p>Each time the UART finishes sending a byte, it generates an
interrupt. <span><code>uartintr</code></span> calls
<span><code>uartstart</code></span>, which checks that the device really
has finished sending, and hands the device the next buffered output
character. Thus if a process writes multiple bytes to the console,
typically the first byte will be sent by
<span><code>uartputc</code></span>’s call to
<span><code>uartstart</code></span>, and the remaining buffered bytes
will be sent by <span><code>uartstart</code></span> calls from
<span><code>uartintr</code></span> as transmit complete interrupts
arrive.</p>
<p>A general pattern to note is the decoupling of device activity from
process activity via buffering and interrupts. The console driver can
process input even when no process is waiting to read it; a subsequent
read will see the input. Similarly, processes can send output without
having to wait for the device. This decoupling can increase performance
by allowing processes to execute concurrently with device I/O, and is
particularly important when the device is slow (as with the UART) or
needs immediate attention (as with echoing typed characters). This idea
is sometimes called <em><span id="I/O_concurrency_1">I/O
concurrency</span></em>.</p>
</section>
<section id="concurrency-in-drivers" class="level2" data-number="5.3">
<h2 data-number="5.3"><span class="header-section-number">5.3</span>
Concurrency in drivers</h2>
<p>You may have noticed calls to <span><code>acquire</code></span> in
<span><code>consoleread</code></span> and in
<span><code>consoleintr</code></span>. These calls acquire a lock, which
protects the console driver’s data structures from concurrent access.
There are three concurrency dangers here: two processes on different
CPUs might call <span><code>consoleread</code></span> at the same time;
the hardware might ask a CPU to deliver a console (really UART)
interrupt while that CPU is already executing inside
<span><code>consoleread</code></span>; and the hardware might deliver a
console interrupt on a different CPU while
<span><code>consoleread</code></span> is executing. These dangers may
result in races or deadlocks. Chapter <a href="#CH:LOCK"
data-reference-type="ref" data-reference="CH:LOCK">6</a> explores these
problems and how locks can address them.</p>
<p>Another way in which concurrency requires care in drivers is that one
process may be waiting for input from a device, but the interrupt
signaling arrival of the input may arrive when a different process (or
no process at all) is running. Thus interrupt handlers are not allowed
to think about the process or code that they have interrupted. For
example, an interrupt handler cannot safely call
<span><code>copyout</code></span> with the current process’s page table.
Interrupt handlers typically do relatively little work (e.g., just copy
the input data to a buffer), and wake up top-half code to do the
rest.</p>
</section>
<section id="timer-interrupts" class="level2" data-number="5.4">
<h2 data-number="5.4"><span class="header-section-number">5.4</span>
Timer interrupts</h2>
<p>Xv6 uses timer interrupts to maintain its clock and to enable it to
switch among compute-bound processes; the
<span><code>yield</code></span> calls in
<span><code> usertrap</code></span> and
<span><code>kerneltrap</code></span> cause this switching. Timer
interrupts come from clock hardware attached to each RISC-V CPU. Xv6
programs this clock hardware to interrupt each CPU periodically.</p>
<p>RISC-V requires that timer interrupts be taken in machine mode, not
supervisor mode. RISC-V machine mode executes without paging, and with a
separate set of control registers, so it’s not practical to run ordinary
xv6 kernel code in machine mode. As a result, xv6 handles timer
interrupts completely separately from the trap mechanism laid out
above.</p>
<p>Code executed in machine mode in <span><code>start.c</code></span>,
before <span><code>main</code></span>, sets up to receive timer
interrupts <a
href="xv6-riscv-src/kernel/start.c#L63"><span>(kernel/start.c:63)</span></a>.
Part of the job is to program the CLINT hardware (core-local
interruptor) to generate an interrupt after a certain delay. Another
part is to set up a scratch area, analogous to the trapframe, to help
the timer interrupt handler save registers and the address of the CLINT
registers. Finally, <span><code>start</code></span> sets
<span><code>mtvec</code></span> to <span><code>timervec</code></span>
and enables timer interrupts.</p>
<p>A timer interrupt can occur at any point when user or kernel code is
executing; there’s no way for the kernel to disable timer interrupts
during critical operations. Thus the timer interrupt handler must do its
job in a way guaranteed not to disturb interrupted kernel code. The
basic strategy is for the handler to ask the RISC-V to raise a “software
interrupt” and immediately return. The RISC-V delivers software
interrupts to the kernel with the ordinary trap mechanism, and allows
the kernel to disable them. The code to handle the software interrupt
generated by a timer interrupt can be seen in
<span><code>devintr</code></span> <a
href="xv6-riscv-src/kernel/trap.c#L205"><span>(kernel/trap.c:205)</span></a>.</p>
<p>The machine-mode timer interrupt handler is
<span><code>timervec</code></span> <a
href="xv6-riscv-src/kernel/kernelvec.S#L95"><span>(kernel/kernelvec.S:95)</span></a>.
It saves a few registers in the scratch area prepared by
<span><code>start</code></span>, tells the CLINT when to generate the
next timer interrupt, asks the RISC-V to raise a software interrupt,
restores registers, and returns. There’s no C code in the timer
interrupt handler.</p>
</section>
<section id="real-world-4" class="level2" data-number="5.5">
<h2 data-number="5.5"><span class="header-section-number">5.5</span>
Real world</h2>
<p>Xv6 allows device and timer interrupts while executing in the kernel,
as well as when executing user programs. Timer interrupts force a thread
switch (a call to <span><code>yield</code></span>) from the timer
interrupt handler, even when executing in the kernel. The ability to
time-slice the CPU fairly among kernel threads is useful if kernel
threads sometimes spend a lot of time computing, without returning to
user space. However, the need for kernel code to be mindful that it
might be suspended (due to a timer interrupt) and later resume on a
different CPU is the source of some complexity in xv6 (see Section <a
href="#s:lockinter" data-reference-type="ref"
data-reference="s:lockinter">6.6</a>). The kernel could be made somewhat
simpler if device and timer interrupts only occurred while executing
user code.</p>
<p>Supporting all the devices on a typical computer in its full glory is
much work, because there are many devices, the devices have many
features, and the protocol between device and driver can be complex and
poorly documented. In many operating systems, the drivers account for
more code than the core kernel.</p>
<p>The UART driver retrieves data a byte at a time by reading the UART
control registers; this pattern is called <em><span
id="programmed_I/O_1">programmed I/O</span></em>, since software is
driving the data movement. Programmed I/O is simple, but too slow to be
used at high data rates. Devices that need to move lots of data at high
speed typically use <em><span id="direct_memory_access_(DMA)_1">direct
memory access (DMA)</span></em>. DMA device hardware directly writes
incoming data to RAM, and reads outgoing data from RAM. Modern disk and
network devices use DMA. A driver for a DMA device would prepare data in
RAM, and then use a single write to a control register to tell the
device to process the prepared data.</p>
<p>Interrupts make sense when a device needs attention at unpredictable
times, and not too often. But interrupts have high CPU overhead. Thus
high speed devices, such as network and disk controllers, use tricks
that reduce the need for interrupts. One trick is to raise a single
interrupt for a whole batch of incoming or outgoing requests. Another
trick is for the driver to disable interrupts entirely, and to check the
device periodically to see if it needs attention. This technique is
called <em><span id="polling_1">polling</span></em>. Polling makes sense
if the device performs operations very quickly, but it wastes CPU time
if the device is mostly idle. Some drivers dynamically switch between
polling and interrupts depending on the current device load.</p>
<p>The UART driver copies incoming data first to a buffer in the kernel,
and then to user space. This makes sense at low data rates, but such a
double copy can significantly reduce performance for devices that
generate or consume data very quickly. Some operating systems are able
to directly move data between user-space buffers and device hardware,
often with DMA.</p>
<p>As mentioned in Chapter <a href="#CH:UNIX" data-reference-type="ref"
data-reference="CH:UNIX">1</a>, the console appears to applications as a
regular file, and applications read input and write output using the
<code>read</code> and <code>write</code> system calls. Applications may
want to control aspects of a device that cannot be expressed through the
standard file system calls (e.g., enabling/disabling line buffering in
the console driver). Unix operating systems support the
<code>ioctl</code> system call for such cases.</p>
<p>Some usages of computers require that the system must respond in a
bounded time. For example, in safety-critical systems missing a deadline
can lead to disasters. Xv6 is not suitable for hard real-time settings.
Operating systems for hard real-time tend to be libraries that link with
the application in a way that allows for an analysis to determine the
worst-case response time. Xv6 is also not suitable for soft real-time
applications, when missing a deadline occasionally is acceptable,
because xv6’s scheduler is too simplistic and it has kernel code path
where interrupts are disabled for a long time.</p>
</section>
<section id="exercises-4" class="level2" data-number="5.6">
<h2 data-number="5.6"><span class="header-section-number">5.6</span>
Exercises</h2>
<ol>
<li><p>Modify <span><code>uart.c</code></span> to not use interrupts at
all. You may need to modify <span><code>console.c</code></span> as
well.</p></li>
<li><p>Add a driver for an Ethernet card.</p></li>
</ol>
</section>
</section>
<section id="CH:LOCK" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span>
Locking</h1>
<p>Most kernels, including xv6, interleave the execution of multiple
activities. One source of interleaving is multiprocessor hardware:
computers with multiple CPUs executing independently, such as xv6’s
RISC-V. These multiple CPUs share physical RAM, and xv6 exploits the
sharing to maintain data structures that all CPUs read and write. This
sharing raises the possibility of one CPU reading a data structure while
another CPU is mid-way through updating it, or even multiple CPUs
updating the same data simultaneously; without careful design such
parallel access is likely to yield incorrect results or a broken data
structure. Even on a uniprocessor, the kernel may switch the CPU among a
number of threads, causing their execution to be interleaved. Finally, a
device interrupt handler that modifies the same data as some
interruptible code could damage the data if the interrupt occurs at just
the wrong time. The word <em><span
id="concurrency_1">concurrency</span></em> refers to situations in which
multiple instruction streams are interleaved, due to multiprocessor
parallelism, thread switching, or interrupts.</p>
<p>Kernels are full of concurrently-accessed data. For example, two CPUs
could simultaneously call <span><code>kalloc</code></span>, thereby
concurrently popping from the head of the free list. Kernel designers
like to allow for lots of concurrency, since it can yield increased
performance through parallelism, and increased responsiveness. However,
as a result kernel designers must convince themselves of correctness
despite such concurrency. There are many ways to arrive at correct code,
some easier to reason about than others. Strategies aimed at correctness
under concurrency, and abstractions that support them, are called
<em><span id="concurrency_control_1">concurrency control</span></em>
techniques.</p>
<p>Xv6 uses a number of concurrency control techniques, depending on the
situation; many more are possible. This chapter focuses on a widely used
technique: the <em><span id="lock_1">lock</span></em>. A lock provides
mutual exclusion, ensuring that only one CPU at a time can hold the
lock. If the programmer associates a lock with each shared data item,
and the code always holds the associated lock when using an item, then
the item will be used by only one CPU at a time. In this situation, we
say that the lock protects the data item. Although locks are an
easy-to-understand concurrency control mechanism, the downside of locks
is that they can limit performance, because they serialize concurrent
operations.</p>
<p>The rest of this chapter explains why xv6 needs locks, how xv6
implements them, and how it uses them.</p>
<section id="races" class="level2" data-number="6.1">
<h2 data-number="6.1"><span class="header-section-number">6.1</span>
Races</h2>
<figure id="fig:smp">
<img src="fig/smp.svg" />
<figcaption>Simplified SMP architecture</figcaption>
</figure>
<p>As an example of why we need locks, consider two processes with
exited children calling <span><code>wait</code></span> on two different
CPUs. <span><code>wait</code></span> frees the child’s memory. Thus on
each CPU, the kernel will call <span><code>kfree</code></span> to free
the children’s memory pages. The kernel allocator maintains a linked
list: <code>kalloc()</code> <a
href="xv6-riscv-src/kernel/kalloc.c#L69"><span>(kernel/kalloc.c:69)</span></a>
pops a page of memory from a list of free pages, and
<code>kfree()</code> <a
href="xv6-riscv-src/kernel/kalloc.c#L47"><span>(kernel/kalloc.c:47)</span></a>
pushes a page onto the free list. For best performance, we might hope
that the <span><code>kfree</code></span>s of the two parent processes
would execute in parallel without either having to wait for the other,
but this would not be correct given xv6’s
<span><code>kfree</code></span> implementation.</p>
<p>Figure <a href="#fig:smp" data-reference-type="ref"
data-reference="fig:smp">6.1</a> illustrates the setting in more detail:
the linked list of free pages is in memory that is shared by the two
CPUs, which manipulate the list using load and store instructions. (In
reality, the processors have caches, but conceptually multiprocessor
systems behave as if there were a single, shared memory.) If there were
no concurrent requests, you might implement a list <code>push</code>
operation as follows:</p>
<span id="line:next" label="line:next"></span>
<span id="line:list" label="line:list"></span>
<div class="sourceCode" id="cb19" data-numbers="left"
data-startFrom="1"><pre
class="sourceCode numberSource numberLines C"><code class="sourceCode c"><span id="cb19-1"><a href="#cb19-1"></a>   <span class="kw">struct</span> element <span class="op">{</span></span>
<span id="cb19-2"><a href="#cb19-2"></a>      <span class="dt">int</span> data<span class="op">;</span></span>
<span id="cb19-3"><a href="#cb19-3"></a>      <span class="kw">struct</span> element <span class="op">*</span>next<span class="op">;</span></span>
<span id="cb19-4"><a href="#cb19-4"></a>    <span class="op">};</span></span>
<span id="cb19-5"><a href="#cb19-5"></a>    </span>
<span id="cb19-6"><a href="#cb19-6"></a>    <span class="kw">struct</span> element <span class="op">*</span>list <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb19-7"><a href="#cb19-7"></a>    </span>
<span id="cb19-8"><a href="#cb19-8"></a>    <span class="dt">void</span></span>
<span id="cb19-9"><a href="#cb19-9"></a>    push<span class="op">(</span><span class="dt">int</span> data<span class="op">)</span></span>
<span id="cb19-10"><a href="#cb19-10"></a>    <span class="op">{</span></span>
<span id="cb19-11"><a href="#cb19-11"></a>      <span class="kw">struct</span> element <span class="op">*</span>l<span class="op">;</span></span>
<span id="cb19-12"><a href="#cb19-12"></a>   </span>
<span id="cb19-13"><a href="#cb19-13"></a>      l <span class="op">=</span> malloc<span class="op">(</span><span class="kw">sizeof</span> <span class="op">*</span>l<span class="op">);</span></span>
<span id="cb19-14"><a href="#cb19-14"></a>      l<span class="op">-&gt;</span>data <span class="op">=</span> data<span class="op">;</span></span>
<span id="cb19-15"><a href="#cb19-15"></a>      l<span class="op">-&gt;</span>next <span class="op">=</span> list<span class="op">;</span> </span>
<span id="cb19-16"><a href="#cb19-16"></a>      list <span class="op">=</span> l<span class="op">;</span>  </span>
<span id="cb19-17"><a href="#cb19-17"></a>   <span class="op">}</span></span></code></pre></div>
<figure id="fig:race">
<img src="fig/race.svg" />
<figcaption>Example race</figcaption>
</figure>
<p>This implementation is correct if executed in isolation. However, the
code is not correct if more than one copy executes concurrently. If two
CPUs execute <code>push</code> at the same time, both might execute
line <a href="#line:next" data-reference-type="ref"
data-reference="line:next">15</a> as shown in Fig <a href="#fig:smp"
data-reference-type="ref" data-reference="fig:smp">6.1</a>, before
either executes line <a href="#line:list" data-reference-type="ref"
data-reference="line:list">16</a>, which results in an incorrect outcome
as illustrated by Figure <a href="#fig:race" data-reference-type="ref"
data-reference="fig:race">6.2</a>. There would then be two list elements
with <code>next</code> set to the former value of <code>list</code>.
When the two assignments to <code>list</code> happen at line  <a
href="#line:list" data-reference-type="ref"
data-reference="line:list">16</a>, the second one will overwrite the
first; the element involved in the first assignment will be lost.</p>
<p>The lost update at line <a href="#line:list"
data-reference-type="ref" data-reference="line:list">16</a> is an
example of a <em><span id="race_1">race</span></em>. A race is a
situation in which a memory location is accessed concurrently, and at
least one access is a write. A race is often a sign of a bug, either a
lost update (if the accesses are writes) or a read of an
incompletely-updated data structure. The outcome of a race depends on
the machine code generated by the compiler, the timing of the two CPUs
involved, and how their memory operations are ordered by the memory
system, which can make race-induced errors difficult to reproduce and
debug. For example, adding print statements while debugging
<code>push</code> might change the timing of the execution enough to
make the race disappear.</p>
<p>The usual way to avoid races is to use a lock. Locks ensure <em><span
id="mutual_exclusion_1">mutual exclusion</span></em>, so that only one
CPU at a time can execute the sensitive lines of <code>push</code>; this
makes the scenario above impossible. The correctly locked version of the
above code adds just a few lines (highlighted in yellow):</p>
<span id="line:malloc" label="line:malloc"></span>
<span id="line:next1" label="line:next1"></span>
<span id="line:list1" label="line:list1"></span>
<div class="sourceCode" id="cb20" data-numbers="left"
data-startFrom="6"><pre
class="sourceCode numberSource numberLines C"><code class="sourceCode c" style="counter-reset: source-line 5;"><span id="cb20-6"><a href="#cb20-6"></a>   <span class="kw">struct</span> element <span class="op">*</span>list <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb20-7"><a href="#cb20-7"></a>   <span class="kw">struct</span> lock listlock<span class="op">;</span></span>
<span id="cb20-8"><a href="#cb20-8"></a>    	</span>
<span id="cb20-9"><a href="#cb20-9"></a>   <span class="dt">void</span></span>
<span id="cb20-10"><a href="#cb20-10"></a>   push<span class="op">(</span><span class="dt">int</span> data<span class="op">)</span></span>
<span id="cb20-11"><a href="#cb20-11"></a>   <span class="op">{</span></span>
<span id="cb20-12"><a href="#cb20-12"></a>     <span class="kw">struct</span> element <span class="op">*</span>l<span class="op">;</span></span>
<span id="cb20-13"><a href="#cb20-13"></a>     l <span class="op">=</span> malloc<span class="op">(</span><span class="kw">sizeof</span> <span class="op">*</span>l<span class="op">);</span> </span>
<span id="cb20-14"><a href="#cb20-14"></a>     l<span class="op">-&gt;</span>data <span class="op">=</span> data<span class="op">;</span></span>
<span id="cb20-15"><a href="#cb20-15"></a>   </span>
<span id="cb20-16"><a href="#cb20-16"></a>     acquire<span class="op">(&amp;</span>listlock<span class="op">);</span></span>
<span id="cb20-17"><a href="#cb20-17"></a>     l<span class="op">-&gt;</span>next <span class="op">=</span> list<span class="op">;</span>     </span>
<span id="cb20-18"><a href="#cb20-18"></a>     list <span class="op">=</span> l<span class="op">;</span>           </span>
<span id="cb20-19"><a href="#cb20-19"></a>     release<span class="op">(&amp;</span>listlock<span class="op">)</span></span>
<span id="cb20-20"><a href="#cb20-20"></a>   <span class="op">}</span></span></code></pre></div>
<p>The sequence of instructions between <code>acquire</code> and
<code>release</code> is often called a <em><span
id="critical_section_1">critical section</span></em>. The lock is
typically said to be protecting <code>list</code>.</p>
<p>When we say that a lock protects data, we really mean that the lock
protects some collection of invariants that apply to the data.
Invariants are properties of data structures that are maintained across
operations. Typically, an operation’s correct behavior depends on the
invariants being true when the operation begins. The operation may
temporarily violate the invariants but must reestablish them before
finishing. For example, in the linked list case, the invariant is that
<code>list</code> points at the first element in the list and that each
element’s <code>next</code> field points at the next element. The
implementation of <code>push</code> violates this invariant temporarily:
in line <a href="#line:next1" data-reference-type="ref"
data-reference="line:next1">17</a>, <code>l</code> points to the next
list element, but <code>list</code> does not point at <code>l</code> yet
(reestablished at line <a href="#line:list1" data-reference-type="ref"
data-reference="line:list1">18</a>). The race we examined above happened
because a second CPU executed code that depended on the list invariants
while they were (temporarily) violated. Proper use of a lock ensures
that only one CPU at a time can operate on the data structure in the
critical section, so that no CPU will execute a data structure operation
when the data structure’s invariants do not hold.</p>
<p>You can think of a lock as <em><span
id="serializing_1">serializing</span></em> concurrent critical sections
so that they run one at a time, and thus preserve invariants (assuming
the critical sections are correct in isolation). You can also think of
critical sections guarded by the same lock as being atomic with respect
to each other, so that each sees only the complete set of changes from
earlier critical sections, and never sees partially-completed
updates.</p>
<p>Though useful for correctness, locks inherently limit performance.
For example, if two processes call <span><code>kfree</code></span>
concurrently, the locks will serialize the two critical sections, so
that there is no benefit from running them on different CPUs. We say
that multiple processes <em><span id="conflict_1">conflict</span></em>
if they want the same lock at the same time, or that the lock
experiences <em><span id="contention_1">contention</span></em>. A major
challenge in kernel design is avoidance of lock contention in pursuit of
parallelism. Xv6 does little of that, but sophisticated kernels organize
data structures and algorithms specifically to avoid lock contention. In
the list example, a kernel may maintain a separate free list per CPU and
only touch another CPU’s free list if the current CPU’s list is empty
and it must steal memory from another CPU. Other use cases may require
more complicated designs.</p>
<p>The placement of locks is also important for performance. For
example, it would be correct to move <code>acquire</code> earlier in
<code>push</code>, before line <a href="#line:malloc"
data-reference-type="ref" data-reference="line:malloc">13</a>. But this
would likely reduce performance because then the calls to
<code>malloc</code> would be serialized. The section “Using locks” below
provides some guidelines for where to insert <code>acquire</code> and
<code>release</code> invocations.</p>
</section>
<section id="code-locks" class="level2" data-number="6.2">
<h2 data-number="6.2"><span class="header-section-number">6.2</span>
Code: Locks</h2>
<p>Xv6 has two types of locks: spinlocks and sleep-locks. We’ll start
with spinlocks. Xv6 represents a spinlock as a <code
id="struct_spinlock_1">struct spinlock</code> <a
href="xv6-riscv-src/kernel/spinlock.h#L2"><span>(kernel/spinlock.h:2)</span></a>.
The important field in the structure is <code>locked</code>, a word that
is zero when the lock is available and non-zero when it is held.
Logically, xv6 should acquire a lock by executing code like</p>
<span id="line:testlocked" label="line:testlocked"></span>
<span id="line:assign" label="line:assign"></span>
<div class="sourceCode" id="cb21" data-numbers="left"
data-startFrom="21"><pre
class="sourceCode numberSource numberLines C"><code class="sourceCode c" style="counter-reset: source-line 20;"><span id="cb21-21"><a href="#cb21-21"></a>   <span class="dt">void</span></span>
<span id="cb21-22"><a href="#cb21-22"></a>   acquire<span class="op">(</span><span class="kw">struct</span> spinlock <span class="op">*</span>lk<span class="op">)</span> <span class="co">// does not work!</span></span>
<span id="cb21-23"><a href="#cb21-23"></a>   <span class="op">{</span></span>
<span id="cb21-24"><a href="#cb21-24"></a>     <span class="cf">for</span><span class="op">(;;)</span> <span class="op">{</span></span>
<span id="cb21-25"><a href="#cb21-25"></a>       <span class="cf">if</span><span class="op">(</span>lk<span class="op">-&gt;</span>locked <span class="op">==</span> <span class="dv">0</span><span class="op">)</span> <span class="op">{</span>  </span>
<span id="cb21-26"><a href="#cb21-26"></a>         lk<span class="op">-&gt;</span>locked <span class="op">=</span> <span class="dv">1</span><span class="op">;</span>      </span>
<span id="cb21-27"><a href="#cb21-27"></a>         <span class="cf">break</span><span class="op">;</span></span>
<span id="cb21-28"><a href="#cb21-28"></a>       <span class="op">}</span></span>
<span id="cb21-29"><a href="#cb21-29"></a>     <span class="op">}</span></span>
<span id="cb21-30"><a href="#cb21-30"></a>   <span class="op">}</span></span></code></pre></div>
<p>Unfortunately, this implementation does not guarantee mutual
exclusion on a multiprocessor. It could happen that two CPUs
simultaneously reach line <a href="#line:testlocked"
data-reference-type="ref" data-reference="line:testlocked">25</a>, see
that <code>lk-&gt;locked</code> is zero, and then both grab the lock by
executing line <a href="#line:assign" data-reference-type="ref"
data-reference="line:assign">26</a>. At this point, two different CPUs
hold the lock, which violates the mutual exclusion property. What we
need is a way to make lines <a href="#line:testlocked"
data-reference-type="ref" data-reference="line:testlocked">25</a> and <a
href="#line:assign" data-reference-type="ref"
data-reference="line:assign">26</a> execute as an <em><span
id="atomic_1">atomic</span></em> (i.e., indivisible) step.</p>
<p>Because locks are widely used, multi-core processors usually provide
instructions that implement an atomic version of lines <a
href="#line:testlocked" data-reference-type="ref"
data-reference="line:testlocked">25</a> and <a href="#line:assign"
data-reference-type="ref" data-reference="line:assign">26</a>. On the
RISC-V this instruction is <code>amoswap r, a</code>.
<code>amoswap</code> reads the value at the memory address
<span><code>a</code></span>, writes the contents of register
<span><code>r</code></span> to that address, and puts the value it read
into <span><code>r</code></span>. That is, it swaps the contents of the
register and the memory address. It performs this sequence atomically,
using special hardware to prevent any other CPU from using the memory
address between the read and the write.</p>
<p>Xv6’s <code id="acquire_1">acquire</code> <a
href="xv6-riscv-src/kernel/spinlock.c#L22"><span>(kernel/spinlock.c:22)</span></a>
uses the portable C library call <code>__sync_lock_test_and_set</code>,
which boils down to the <code>amoswap</code> instruction; the return
value is the old (swapped) contents of <code>lk-&gt;locked</code>. The
<code>acquire</code> function wraps the swap in a loop, retrying
(spinning) until it has acquired the lock. Each iteration swaps one into
<code>lk-&gt;locked</code> and checks the previous value; if the
previous value is zero, then we’ve acquired the lock, and the swap will
have set <code>lk-&gt;locked</code> to one. If the previous value is
one, then some other CPU holds the lock, and the fact that we atomically
swapped one into <code>lk-&gt;locked</code> didn’t change its value.</p>
<p>Once the lock is acquired, <code>acquire</code> records, for
debugging, the CPU that acquired the lock. The <code>lk-&gt;cpu</code>
field is protected by the lock and must only be changed while holding
the lock.</p>
<p>The function <code id="release_1">release</code> <a
href="xv6-riscv-src/kernel/spinlock.c#L47"><span>(kernel/spinlock.c:47)</span></a>
is the opposite of <code>acquire</code>: it clears the
<code>lk-&gt;cpu</code> field and then releases the lock. Conceptually,
the release just requires assigning zero to <code>lk-&gt;locked</code>.
The C standard allows compilers to implement an assignment with multiple
store instructions, so a C assignment might be non-atomic with respect
to concurrent code. Instead, <code>release</code> uses the C library
function <code>__sync_lock_release</code> that performs an atomic
assignment. This function also boils down to a RISC-V
<code>amoswap</code> instruction.</p>
</section>
<section id="code-using-locks" class="level2" data-number="6.3">
<h2 data-number="6.3"><span class="header-section-number">6.3</span>
Code: Using locks</h2>
<p>Xv6 uses locks in many places to avoid races. As described above,
<code>kalloc</code> <a
href="xv6-riscv-src/kernel/kalloc.c#L69"><span>(kernel/kalloc.c:69)</span></a>
and <code>kfree</code> <a
href="xv6-riscv-src/kernel/kalloc.c#L47"><span>(kernel/kalloc.c:47)</span></a>
form a good example. Try Exercises 1 and 2 to see what happens if those
functions omit the locks. You’ll likely find that it’s difficult to
trigger incorrect behavior, suggesting that it’s hard to reliably test
whether code is free from locking errors and races. Xv6 may well have
as-yet-undiscovered races.</p>
<p>A hard part about using locks is deciding how many locks to use and
which data and invariants each lock should protect. There are a few
basic principles. First, any time a variable can be written by one CPU
at the same time that another CPU can read or write it, a lock should be
used to keep the two operations from overlapping. Second, remember that
locks protect invariants: if an invariant involves multiple memory
locations, typically all of them need to be protected by a single lock
to ensure the invariant is maintained.</p>
<p>The rules above say when locks are necessary but say nothing about
when locks are unnecessary, and it is important for efficiency not to
lock too much, because locks reduce parallelism. If parallelism isn’t
important, then one could arrange to have only a single thread and not
worry about locks. A simple kernel can do this on a multiprocessor by
having a single lock that must be acquired on entering the kernel and
released on exiting the kernel (though blocking system calls such as
pipe reads or <code>wait</code> would pose a problem). Many uniprocessor
operating systems have been converted to run on multiprocessors using
this approach, sometimes called a “big kernel lock,” but the approach
sacrifices parallelism: only one CPU can execute in the kernel at a
time. If the kernel does any heavy computation, it would be more
efficient to use a larger set of more fine-grained locks, so that the
kernel could execute on multiple CPUs simultaneously.</p>
<p>As an example of coarse-grained locking, xv6’s <code>kalloc.c</code>
allocator has a single free list protected by a single lock. If multiple
processes on different CPUs try to allocate pages at the same time, each
will have to wait for its turn by spinning in
<span><code> acquire</code></span>. Spinning wastes CPU time, since it’s
not useful work. If contention for the lock wasted a significant
fraction of CPU time, perhaps performance could be improved by changing
the allocator design to have multiple free lists, each with its own
lock, to allow truly parallel allocation.</p>
<p>As an example of fine-grained locking, xv6 has a separate lock for
each file, so that processes that manipulate different files can often
proceed without waiting for each other’s locks. The file locking scheme
could be made even more fine-grained if one wanted to allow processes to
simultaneously write different areas of the same file. Ultimately lock
granularity decisions need to be driven by performance measurements as
well as complexity considerations.</p>
<p>As subsequent chapters explain each part of xv6, they will mention
examples of xv6’s use of locks to deal with concurrency. As a preview,
Figure <a href="#fig:locktable" data-reference-type="ref"
data-reference="fig:locktable">6.3</a> lists all of the locks in
xv6.</p>
<figure id="fig:locktable">
<table>
<thead>
<tr class="header">
<th style="text-align: left;"><span><strong>Lock</strong></span></th>
<th
style="text-align: left;"><span><strong>Description</strong></span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">bcache.lock</td>
<td style="text-align: left;">Protects allocation of block buffer cache
entries</td>
</tr>
<tr class="even">
<td style="text-align: left;">cons.lock</td>
<td style="text-align: left;">Serializes access to console hardware,
avoids intermixed output</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ftable.lock</td>
<td style="text-align: left;">Serializes allocation of a struct file in
file table</td>
</tr>
<tr class="even">
<td style="text-align: left;">itable.lock</td>
<td style="text-align: left;">Protects allocation of in-memory inode
entries</td>
</tr>
<tr class="odd">
<td style="text-align: left;">vdisk_lock</td>
<td style="text-align: left;">Serializes access to disk hardware and
queue of DMA descriptors</td>
</tr>
<tr class="even">
<td style="text-align: left;">kmem.lock</td>
<td style="text-align: left;">Serializes allocation of memory</td>
</tr>
<tr class="odd">
<td style="text-align: left;">log.lock</td>
<td style="text-align: left;">Serializes operations on the transaction
log</td>
</tr>
<tr class="even">
<td style="text-align: left;">pipe’s pi-&gt;lock</td>
<td style="text-align: left;">Serializes operations on each pipe</td>
</tr>
<tr class="odd">
<td style="text-align: left;">pid_lock</td>
<td style="text-align: left;">Serializes increments of next_pid</td>
</tr>
<tr class="even">
<td style="text-align: left;">proc’s p-&gt;lock</td>
<td style="text-align: left;">Serializes changes to process’s state</td>
</tr>
<tr class="odd">
<td style="text-align: left;">wait_lock</td>
<td style="text-align: left;">Helps wait avoid lost wakeups</td>
</tr>
<tr class="even">
<td style="text-align: left;">tickslock</td>
<td style="text-align: left;">Serializes operations on the ticks
counter</td>
</tr>
<tr class="odd">
<td style="text-align: left;">inode’s ip-&gt;lock</td>
<td style="text-align: left;">Serializes operations on each inode and
its content</td>
</tr>
<tr class="even">
<td style="text-align: left;">buf’s b-&gt;lock</td>
<td style="text-align: left;">Serializes operations on each block
buffer</td>
</tr>
</tbody>
</table>
<figcaption>Locks in xv6</figcaption>
</figure>
</section>
<section id="deadlock-and-lock-ordering" class="level2"
data-number="6.4">
<h2 data-number="6.4"><span class="header-section-number">6.4</span>
Deadlock and lock ordering</h2>
<p>If a code path through the kernel must hold several locks at the same
time, it is important that all code paths acquire those locks in the
same order. If they don’t, there is a risk of <em><span
id="deadlock_1">deadlock</span></em>. Let’s say two code paths in xv6
need locks A and B, but code path 1 acquires locks in the order A then
B, and the other path acquires them in the order B then A. Suppose
thread T1 executes code path 1 and acquires lock A, and thread T2
executes code path 2 and acquires lock B. Next T1 will try to acquire
lock B, and T2 will try to acquire lock A. Both acquires will block
indefinitely, because in both cases the other thread holds the needed
lock, and won’t release it until its acquire returns. To avoid such
deadlocks, all code paths must acquire locks in the same order. The need
for a global lock acquisition order means that locks are effectively
part of each function’s specification: callers must invoke functions in
a way that causes locks to be acquired in the agreed-on order.</p>
<p>Xv6 has many lock-order chains of length two involving per-process
locks (the lock in each <code>struct proc</code>) due to the way that
<code>sleep</code> works (see Chapter <a href="#CH:SCHED"
data-reference-type="ref" data-reference="CH:SCHED">7</a>). For example,
<code>consoleintr</code> <a
href="xv6-riscv-src/kernel/console.c#L136"><span>(kernel/console.c:136)</span></a>
is the interrupt routine which handles typed characters. When a newline
arrives, any process that is waiting for console input should be woken
up. To do this, <code>consoleintr</code> holds <code>cons.lock</code>
while calling <code id="wakeup_1">wakeup</code>, which acquires the
waiting process’s lock in order to wake it up. In consequence, the
global deadlock-avoiding lock order includes the rule that
<code>cons.lock</code> must be acquired before any process lock. The
file-system code contains xv6’s longest lock chains. For example,
creating a file requires simultaneously holding a lock on the directory,
a lock on the new file’s inode, a lock on a disk block buffer, the disk
driver’s <code>vdisk_lock</code>, and the calling process’s
<code>p-&gt;lock</code>. To avoid deadlock, file-system code always
acquires locks in the order mentioned in the previous sentence.</p>
<p>Honoring a global deadlock-avoiding order can be surprisingly
difficult. Sometimes the lock order conflicts with logical program
structure, e.g., perhaps code module M1 calls module M2, but the lock
order requires that a lock in M2 be acquired before a lock in M1.
Sometimes the identities of locks aren’t known in advance, perhaps
because one lock must be held in order to discover the identity of the
lock to be acquired next. This kind of situation arises in the file
system as it looks up successive components in a path name, and in the
code for <span><code>wait</code></span> and
<span><code>exit</code></span> as they search the table of processes
looking for child processes. Finally, the danger of deadlock is often a
constraint on how fine-grained one can make a locking scheme, since more
locks often means more opportunity for deadlock. The need to avoid
deadlock is often a major factor in kernel implementation.</p>
</section>
<section id="re-entrant-locks" class="level2" data-number="6.5">
<h2 data-number="6.5"><span class="header-section-number">6.5</span>
Re-entrant locks</h2>
<p>It might appear that some deadlocks and lock-ordering challenges
could be avoided by using <em><span id="re-entrant_locks_1">re-entrant
locks</span></em>, which are also called <em><span
id="recursive_locks_1">recursive locks</span></em>. The idea is that if
the lock is held by a process and if that process attempts to acquire
the lock again, then the kernel could just allow this (since the process
already has the lock), instead of calling panic, as the xv6 kernel
does.</p>
<p>It turns out, however, that re-entrant locks make it harder to reason
about concurrency: re-entrant locks break the intuition that locks cause
critical sections to be atomic with respect to other critical sections.
Consider the following two functions <code>f</code> and
<code>g</code>:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode C"><code class="sourceCode c"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">struct</span> spinlock lock<span class="op">;</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> data <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> <span class="co">// protected by lock</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>f<span class="op">()</span> <span class="op">{</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>  acquire<span class="op">(&amp;</span>lock<span class="op">);</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span><span class="op">(</span>data <span class="op">==</span> <span class="dv">0</span><span class="op">){</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    call_once<span class="op">();</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    h<span class="op">();</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>  release<span class="op">(&amp;</span>lock<span class="op">);</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>g<span class="op">()</span> <span class="op">{</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>  aquire<span class="op">(&amp;</span>lock<span class="op">);</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span><span class="op">(</span>data <span class="op">==</span> <span class="dv">0</span><span class="op">){</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>    call_once<span class="op">();</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>  release<span class="op">(&amp;</span>lock<span class="op">);</span></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>Looking at this code fragment, the intuition is that
<code>call_once</code> will be called only once: either by
<code>f</code>, or by <code>g</code>, but not by both.</p>
<p>But if re-entrant locks are allowed, and <code>h</code> happens to
call <code>g</code>, <code>call_once</code> will be called
<em>twice</em>.</p>
<p>If re-entrant locks aren’t allowed, then <code>h</code> calling
<code>g</code> results in a deadlock, which is not great either. But,
assuming it would be a serious error to call <code>call_once</code>, a
deadlock is preferable. The kernel developer will observe the deadlock
(the kernel panics) and can fix the code to avoid it, while calling
<code>call_once</code> twice may silently result in an error that is
difficult to track down.</p>
<p>For this reason, xv6 uses the simpler to understand non-re-entrant
locks. As long as programmers keep the locking rules in mind, however,
either approach can be made to work. If xv6 were to use re-entrant
locks, one would have to modify <code>acquire</code> to notice that the
lock is currently held by the calling thread. One would also have to add
a count of nested acquires to struct spinlock, in similar style to
<code>push_off</code>, which is discussed next.</p>
</section>
<section id="s:lockinter" class="level2" data-number="6.6">
<h2 data-number="6.6"><span class="header-section-number">6.6</span>
Locks and interrupt handlers</h2>
<p>Some xv6 spinlocks protect data that is used by both threads and
interrupt handlers. For example, the <code>clockintr</code> timer
interrupt handler might increment <code id="ticks_1">ticks</code> <a
href="xv6-riscv-src/kernel/trap.c#L164"><span>(kernel/trap.c:164)</span></a>
at about the same time that a kernel thread reads <code>ticks</code> in
<code id="sys_sleep_1">sys_sleep</code> <a
href="xv6-riscv-src/kernel/sysproc.c#L59"><span>(kernel/sysproc.c:59)</span></a>.
The lock <code id="tickslock_1">tickslock</code> serializes the two
accesses.</p>
<p>The interaction of spinlocks and interrupts raises a potential
danger. Suppose <code id="sys_sleep_2">sys_sleep</code> holds <code
id="tickslock_2">tickslock</code>, and its CPU is interrupted by a timer
interrupt. <code>clockintr</code> would try to acquire
<code>tickslock</code>, see it was held, and wait for it to be released.
In this situation, <code>tickslock</code> will never be released: only
<code>sys_sleep</code> can release it, but <code>sys_sleep</code> will
not continue running until <code>clockintr</code> returns. So the CPU
will deadlock, and any code that needs either lock will also freeze.</p>
<p>To avoid this situation, if a spinlock is used by an interrupt
handler, a CPU must never hold that lock with interrupts enabled. Xv6 is
more conservative: when a CPU acquires any lock, xv6 always disables
interrupts on that CPU. Interrupts may still occur on other CPUs, so an
interrupt’s <code>acquire</code> can wait for a thread to release a
spinlock; just not on the same CPU.</p>
<p>Xv6 re-enables interrupts when a CPU holds no spinlocks; it must do a
little book-keeping to cope with nested critical sections.
<code>acquire</code> calls <code id="push_off_1">push_off</code> <a
href="xv6-riscv-src/kernel/spinlock.c#L89"><span>(kernel/spinlock.c:89)</span></a>
and <code>release</code> calls <code id="pop_off_1">pop_off</code> <a
href="xv6-riscv-src/kernel/spinlock.c#L100"><span>(kernel/spinlock.c:100)</span></a>
to track the nesting level of locks on the current CPU. When that count
reaches zero, <code>pop_off</code> restores the interrupt enable state
that existed at the start of the outermost critical section. The
<code>intr_off</code> and <code>intr_on</code> functions execute RISC-V
instructions to disable and enable interrupts, respectively.</p>
<p>It is important that <code id="acquire_2">acquire</code> call
<code>push_off</code> strictly before setting <code>lk-&gt;locked</code>
<a
href="xv6-riscv-src/kernel/spinlock.c#L28"><span>(kernel/spinlock.c:28)</span></a>.
If the two were reversed, there would be a brief window when the lock
was held with interrupts enabled, and an unfortunately timed interrupt
would deadlock the system. Similarly, it is important that <code
id="release_2">release</code> call <code id="pop_off_2">pop_off</code>
only after releasing the lock <a
href="xv6-riscv-src/kernel/spinlock.c#L66"><span>(kernel/spinlock.c:66)</span></a>.</p>
</section>
<section id="instruction-and-memory-ordering" class="level2"
data-number="6.7">
<h2 data-number="6.7"><span class="header-section-number">6.7</span>
Instruction and memory ordering</h2>
<p>It is natural to think of programs executing in the order in which
source code statements appear. That’s a reasonable mental model for
single-threaded code, but is incorrect when multiple threads interact
through shared memory <span class="citation"
data-cites="riscv:user boehm04">(<a href="#ref-riscv:user"
role="doc-biblioref">Waterman and Asanovic 2019</a>; <a
href="#ref-boehm04" role="doc-biblioref">Boehm 2005</a>)</span>. One
reason is that compilers emit load and store instructions in orders
different from those implied by the source code, and may entirely omit
them (for example by caching data in registers). Another reason is that
the CPU may execute instructions out of order to increase performance.
For example, a CPU may notice that in a serial sequence of instructions
A and B are not dependent on each other. The CPU may start instruction B
first, either because its inputs are ready before A’s inputs, or in
order to overlap execution of A and B.</p>
<p>As an example of what could go wrong, in this code for
<code>push</code>, it would be a disaster if the compiler or CPU moved
the store corresponding to line <a href="#line:next2"
data-reference-type="ref" data-reference="line:next2">4</a> to a point
after the <code>release</code> on line <a href="#line:release"
data-reference-type="ref" data-reference="line:release">6</a>:</p>
<span id="line:next2" label="line:next2"></span>
<span id="line:release" label="line:release"></span>
<div class="sourceCode" id="cb23" data-numbers="left"
data-startFrom="1"><pre
class="sourceCode numberSource numberLines C"><code class="sourceCode c"><span id="cb23-1"><a href="#cb23-1"></a>      l <span class="op">=</span> malloc<span class="op">(</span><span class="kw">sizeof</span> <span class="op">*</span>l<span class="op">);</span></span>
<span id="cb23-2"><a href="#cb23-2"></a>      l<span class="op">-&gt;</span>data <span class="op">=</span> data<span class="op">;</span></span>
<span id="cb23-3"><a href="#cb23-3"></a>      acquire<span class="op">(&amp;</span>listlock<span class="op">);</span></span>
<span id="cb23-4"><a href="#cb23-4"></a>      l<span class="op">-&gt;</span>next <span class="op">=</span> list<span class="op">;</span>   </span>
<span id="cb23-5"><a href="#cb23-5"></a>      list <span class="op">=</span> l<span class="op">;</span>      </span>
<span id="cb23-6"><a href="#cb23-6"></a>      release<span class="op">(&amp;</span>listlock<span class="op">);</span>  </span></code></pre></div>
<p>If such a re-ordering occurred, there would be a window during which
another CPU could acquire the lock and observe the updated
<code>list</code>, but see an uninitialized
<code>list-&gt;next</code>.</p>
<p>The good news is that compilers and CPUs help concurrent programmers
by following a set of rules called the <em><span
id="memory_model_1">memory model</span></em>, and by providing some
primitives to help programmers control re-ordering.</p>
<p>To tell the hardware and compiler not to re-order, xv6 uses
<code>__sync_synchronize()</code> in both <code>acquire</code> <a
href="xv6-riscv-src/kernel/spinlock.c#L22"><span>(kernel/spinlock.c:22)</span></a>
and <code>release</code> <a
href="xv6-riscv-src/kernel/spinlock.c#L47"><span>(kernel/spinlock.c:47)</span></a>.
<code>__sync_synchronize()</code> is a <em><span
id="memory_barrier_1">memory barrier</span></em>: it tells the compiler
and CPU to not reorder loads or stores across the barrier. The barriers
in xv6’s <code>acquire</code> and <code>release</code> force order in
almost all cases where it matters, since xv6 uses locks around accesses
to shared data. Chapter <a href="#CH:LOCK2" data-reference-type="ref"
data-reference="CH:LOCK2">9</a> discusses a few exceptions.</p>
</section>
<section id="sleep-locks" class="level2" data-number="6.8">
<h2 data-number="6.8"><span class="header-section-number">6.8</span>
Sleep locks</h2>
<p>Sometimes xv6 needs to hold a lock for a long time. For example, the
file system (Chapter <a href="#CH:FS" data-reference-type="ref"
data-reference="CH:FS">8</a>) keeps a file locked while reading and
writing its content on the disk, and these disk operations can take tens
of milliseconds. Holding a spinlock that long would lead to waste if
another process wanted to acquire it, since the acquiring process would
waste CPU for a long time while spinning. Another drawback of spinlocks
is that a process cannot yield the CPU while retaining a spinlock; we’d
like to do this so that other processes can use the CPU while the
process with the lock waits for the disk. Yielding while holding a
spinlock is illegal because it might lead to deadlock if a second thread
then tried to acquire the spinlock; since
<span><code>acquire</code></span> doesn’t yield the CPU, the second
thread’s spinning might prevent the first thread from running and
releasing the lock. Yielding while holding a lock would also violate the
requirement that interrupts must be off while a spinlock is held. Thus
we’d like a type of lock that yields the CPU while waiting to acquire,
and allows yields (and interrupts) while the lock is held.</p>
<p>Xv6 provides such locks in the form of <em><span
id="sleep-locks_1">sleep-locks</span></em>. <code>acquiresleep</code> <a
href="xv6-riscv-src/kernel/sleeplock.c#L22"><span>(kernel/sleeplock.c:22)</span></a>
yields the CPU while waiting, using techniques that will be explained in
Chapter <a href="#CH:SCHED" data-reference-type="ref"
data-reference="CH:SCHED">7</a>. At a high level, a sleep-lock has a
<code>locked</code> field that is protected by a spinlock, and
<code>acquiresleep</code> ’s call to <code>sleep</code> atomically
yields the CPU and releases the spinlock. The result is that other
threads can execute while <code>acquiresleep</code> waits.</p>
<p>Because sleep-locks leave interrupts enabled, they cannot be used in
interrupt handlers. Because <code>acquiresleep</code> may yield the CPU,
sleep-locks cannot be used inside spinlock critical sections (though
spinlocks can be used inside sleep-lock critical sections).</p>
<p>Spin-locks are best suited to short critical sections, since waiting
for them wastes CPU time; sleep-locks work well for lengthy
operations.</p>
</section>
<section id="real-world-5" class="level2" data-number="6.9">
<h2 data-number="6.9"><span class="header-section-number">6.9</span>
Real world</h2>
<p>Programming with locks remains challenging despite years of research
into concurrency primitives and parallelism. It is often best to conceal
locks within higher-level constructs like synchronized queues, although
xv6 does not do this. If you program with locks, it is wise to use a
tool that attempts to identify races, because it is easy to miss an
invariant that requires a lock.</p>
<p>Most operating systems support POSIX threads (Pthreads), which allow
a user process to have several threads running concurrently on different
CPUs. Pthreads has support for user-level locks, barriers, etc. Pthreads
also allows a programmer to optionally specify that a lock should be
re-entrant.</p>
<p>Supporting Pthreads at user level requires support from the operating
system. For example, it should be the case that if one pthread blocks in
a system call, another pthread of the same process should be able to run
on that CPU. As another example, if a pthread changes its process’s
address space (e.g., maps or unmaps memory), the kernel must arrange
that other CPUs that run threads of the same process update their
hardware page tables to reflect the change in the address space.</p>
<p>It is possible to implement locks without atomic instructions <span
class="citation" data-cites="lamport:bakery">(<a
href="#ref-lamport:bakery" role="doc-biblioref">Lamport
1974</a>)</span>, but it is expensive, and most operating systems use
atomic instructions.</p>
<p>Locks can be expensive if many CPUs try to acquire the same lock at
the same time. If one CPU has a lock cached in its local cache, and
another CPU must acquire the lock, then the atomic instruction to update
the cache line that holds the lock must move the line from the one CPU’s
cache to the other CPU’s cache, and perhaps invalidate any other copies
of the cache line. Fetching a cache line from another CPU’s cache can be
orders of magnitude more expensive than fetching a line from a local
cache.</p>
<p>To avoid the expenses associated with locks, many operating systems
use lock-free data structures and algorithms <span class="citation"
data-cites="herlihy:art mckenney:rcuusage">(<a href="#ref-herlihy:art"
role="doc-biblioref">Herlihy and Shavit 2012</a>; <a
href="#ref-mckenney:rcuusage" role="doc-biblioref">Mckenney,
Boyd-wickizer, and Walpole 2013</a>)</span>. For example, it is possible
to implement a linked list like the one in the beginning of the chapter
that requires no locks during list searches, and one atomic instruction
to insert an item in a list. Lock-free programming is more complicated,
however, than programming locks; for example, one must worry about
instruction and memory reordering. Programming with locks is already
hard, so xv6 avoids the additional complexity of lock-free
programming.</p>
</section>
<section id="exercises-5" class="level2" data-number="6.10">
<h2 data-number="6.10"><span class="header-section-number">6.10</span>
Exercises</h2>
<ol>
<li><p>Comment out the calls to <code>acquire</code> and
<code>release</code> in <code>kalloc</code> <a
href="xv6-riscv-src/kernel/kalloc.c#L69"><span>(kernel/kalloc.c:69)</span></a>.
This seems like it should cause problems for kernel code that calls
<code>kalloc</code>; what symptoms do you expect to see? When you run
xv6, do you see these symptoms? How about when running
<code>usertests</code>? If you don’t see a problem, why not? See if you
can provoke a problem by inserting dummy loops into the critical section
of <code>kalloc</code>.</p></li>
<li><p>Suppose that you instead commented out the locking in
<code>kfree</code> (after restoring locking in <code>kalloc</code>).
What might now go wrong? Is lack of locks in <code>kfree</code> less
harmful than in <code>kalloc</code>?</p></li>
<li><p>If two CPUs call <code>kalloc</code> at the same time, one will
have to wait for the other, which is bad for performance. Modify
<code>kalloc.c</code> to have more parallelism, so that simultaneous
calls to <code>kalloc</code> from different CPUs can proceed without
waiting for each other.</p></li>
<li><p>Write a parallel program using POSIX threads, which is supported
on most operating systems. For example, implement a parallel hash table
and measure if the number of puts/gets scales with increasing number of
cores.</p></li>
<li><p>Implement a subset of Pthreads in xv6. That is, implement a
user-level thread library so that a user process can have more than 1
thread and arrange that these threads can run in parallel on different
CPUs. Come up with a design that correctly handles a thread making a
blocking system call and changing its shared address space.</p></li>
</ol>
</section>
</section>
<section id="CH:SCHED" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span>
Scheduling</h1>
<p>Any operating system is likely to run with more processes than the
computer has CPUs, so a plan is needed to time-share the CPUs among the
processes. Ideally the sharing would be transparent to user processes. A
common approach is to provide each process with the illusion that it has
its own virtual CPU by <em><span
id="multiplexing_1">multiplexing</span></em> the processes onto the
hardware CPUs. This chapter explains how xv6 achieves this
multiplexing.</p>
<section id="multiplexing" class="level2" data-number="7.1">
<h2 data-number="7.1"><span class="header-section-number">7.1</span>
Multiplexing</h2>
<p>Xv6 multiplexes by switching each CPU from one process to another in
two situations. First, xv6’s <code>sleep</code> and <code>wakeup</code>
mechanism switches when a process waits for device or pipe I/O to
complete, or waits for a child to exit, or waits in the
<code>sleep</code> system call. Second, xv6 periodically forces a switch
to cope with processes that compute for long periods without sleeping.
This multiplexing creates the illusion that each process has its own
CPU, much as xv6 uses the memory allocator and hardware page tables to
create the illusion that each process has its own memory.</p>
<p>Implementing multiplexing poses a few challenges. First, how to
switch from one process to another? Although the idea of context
switching is simple, the implementation is some of the most opaque code
in xv6. Second, how to force switches in a way that is transparent to
user processes? Xv6 uses the standard technique in which a hardware
timer’s interrupts drive context switches. Third, all of the CPUs switch
among the same shared set of processes, and a locking plan is necessary
to avoid races. Fourth, a process’s memory and other resources must be
freed when the process exits, but it cannot do all of this itself
because (for example) it can’t free its own kernel stack while still
using it. Fifth, each core of a multi-core machine must remember which
process it is executing so that system calls affect the correct
process’s kernel state. Finally, <code>sleep</code> and
<code>wakeup</code> allow a process to give up the CPU and wait to be
woken up by another process or interrupt. Care is needed to avoid races
that result in the loss of wakeup notifications. Xv6 tries to solve
these problems as simply as possible, but nevertheless the resulting
code is tricky.</p>
</section>
<section id="code-context-switching" class="level2" data-number="7.2">
<h2 data-number="7.2"><span class="header-section-number">7.2</span>
Code: Context switching</h2>
<figure id="fig:switch">
<img src="fig/switch.svg" />
<figcaption>Switching from one user process to another. In this example,
xv6 runs with one CPU (and thus one scheduler thread).</figcaption>
</figure>
<p>Figure <a href="#fig:switch" data-reference-type="ref"
data-reference="fig:switch">7.1</a> outlines the steps involved in
switching from one user process to another: a user-kernel transition
(system call or interrupt) to the old process’s kernel thread, a context
switch to the current CPU’s scheduler thread, a context switch to a new
process’s kernel thread, and a trap return to the user-level process.
The xv6 scheduler has a dedicated thread (saved registers and stack) per
CPU because it is not safe for the scheduler to execute on the old
process’s kernel stack: some other core might wake the process up and
run it, and it would be a disaster to use the same stack on two
different cores. In this section we’ll examine the mechanics of
switching between a kernel thread and a scheduler thread.</p>
<p>Switching from one thread to another involves saving the old thread’s
CPU registers, and restoring the previously-saved registers of the new
thread; the fact that the stack pointer and program counter are saved
and restored means that the CPU will switch stacks and switch what code
it is executing.</p>
<p>The function <code id="swtch_1">swtch</code> performs the saves and
restores for a kernel thread switch. <code>swtch</code> doesn’t directly
know about threads; it just saves and restores sets of 32 RISC-V
registers, called <em><span id="contexts_1">contexts</span></em>. When
it is time for a process to give up the CPU, the process’s kernel thread
calls <code>swtch</code> to save its own context and return to the
scheduler context. Each context is contained in a <code>struct</code>
<code>context</code> <a
href="xv6-riscv-src/kernel/proc.h#L2"><span>(kernel/proc.h:2)</span></a>,
itself contained in a process’s <code>struct proc</code> or a CPU’s
<code>struct cpu</code>. <code>swtch</code> takes two arguments: <code
id="struct_context_1">struct context</code> <code>*old</code> and
<code>struct</code> <code>context</code> <code>*new</code>. It saves the
current registers in <code>old</code>, loads registers from
<code>new</code>, and returns.</p>
<p>Let’s follow a process through <code>swtch</code> into the scheduler.
We saw in Chapter <a href="#CH:TRAP" data-reference-type="ref"
data-reference="CH:TRAP">4</a> that one possibility at the end of an
interrupt is that <code id="usertrap_1">usertrap</code> calls <code
id="yield_1">yield</code>. <code>yield</code> in turn calls <code
id="sched_1">sched</code>, which calls <code id="swtch_2">swtch</code>
to save the current context in <code>p-&gt;context</code> and switch to
the scheduler context previously saved in <code
id="cpu-&gt;context_1">cpu-&gt;context</code> <a
href="xv6-riscv-src/kernel/proc.c#L497"><span>(kernel/proc.c:497)</span></a>.</p>
<p><code>swtch</code> <a
href="xv6-riscv-src/kernel/swtch.S#L3"><span>(kernel/swtch.S:3)</span></a>
saves only callee-saved registers; the C compiler generates code in the
caller to save caller-saved registers on the stack. <code>swtch</code>
knows the offset of each register’s field in
<code>struct context</code>. It does not save the program counter.
Instead, <code>swtch</code> saves the <code>ra</code> register, which
holds the return address from which <code>swtch</code> was called. Now
<code>swtch</code> restores registers from the new context, which holds
register values saved by a previous <code>swtch</code>. When
<code>swtch</code> returns, it returns to the instructions pointed to by
the restored <code>ra</code> register, that is, the instruction from
which the new thread previously called <code>swtch</code>. In addition,
it returns on the new thread’s stack, since that’s where the restored
<code>sp</code> points.</p>
<p>In our example, <code id="sched_2">sched</code> called <code
id="swtch_3">swtch</code> to switch to <code
id="cpu-&gt;context_2">cpu-&gt;context</code>, the per-CPU scheduler
context. That context was saved at the point in the past when
<code>scheduler</code> called <code>swtch</code> <a
href="xv6-riscv-src/kernel/proc.c#L463"><span>(kernel/proc.c:463)</span></a>
to switch to the process that’s now giving up the CPU. When the <code
id="swtch_4">swtch</code> we have been tracing returns, it returns not
to <code>sched</code> but to <code id="scheduler_1">scheduler</code>,
with the stack pointer in the current CPU’s scheduler stack.</p>
</section>
<section id="code-scheduling" class="level2" data-number="7.3">
<h2 data-number="7.3"><span class="header-section-number">7.3</span>
Code: Scheduling</h2>
<p>The last section looked at the low-level details of <code
id="swtch_5">swtch</code>; now let’s take <code>swtch</code> as a given
and examine switching from one process’s kernel thread through the
scheduler to another process. The scheduler exists in the form of a
special thread per CPU, each running the <code>scheduler</code>
function. This function is in charge of choosing which process to run
next. A process that wants to give up the CPU must acquire its own
process lock <code id="p-&gt;lock_1">p-&gt;lock</code>, release any
other locks it is holding, update its own state
(<code>p-&gt;state</code>), and then call <code
id="sched_3">sched</code>. You can see this sequence in
<code>yield</code> <a
href="xv6-riscv-src/kernel/proc.c#L503"><span>(kernel/proc.c:503)</span></a>,
<code>sleep</code> and <code>exit</code>. <code>sched</code>
double-checks some of those requirements <a
href="xv6-riscv-src/kernel/proc.c#L487-L492">(kernel/proc.c:487-492)</a>
and then checks an implication: since a lock is held, interrupts should
be disabled. Finally, <code id="sched_4">sched</code> calls <code
id="swtch_6">swtch</code> to save the current context in
<code>p-&gt;context</code> and switch to the scheduler context in <code
id="cpu-&gt;context_3">cpu-&gt;context</code>. <code>swtch</code>
returns on the scheduler’s stack as though <code
id="scheduler_2">scheduler</code>’s <code>swtch</code> had returned <a
href="xv6-riscv-src/kernel/proc.c#L463"><span>(kernel/proc.c:463)</span></a>.
The scheduler continues its <code>for</code> loop, finds a process to
run, switches to it, and the cycle repeats.</p>
<p>We just saw that xv6 holds <code id="p-&gt;lock_2">p-&gt;lock</code>
across calls to <code>swtch</code>: the caller of <code
id="swtch_7">swtch</code> must already hold the lock, and control of the
lock passes to the switched-to code. This convention is unusual with
locks; usually the thread that acquires a lock is also responsible for
releasing the lock, which makes it easier to reason about correctness.
For context switching it is necessary to break this convention because
<code id="p-&gt;lock_3">p-&gt;lock</code> protects invariants on the
process’s <code>state</code> and <code>context</code> fields that are
not true while executing in <code>swtch</code>. One example of a problem
that could arise if <code>p-&gt;lock</code> were not held during <code
id="swtch_8">swtch</code>: a different CPU might decide to run the
process after <code id="yield_2">yield</code> had set its state to
<code>RUNNABLE</code>, but before <code>swtch</code> caused it to stop
using its own kernel stack. The result would be two CPUs running on the
same stack, which would cause chaos.</p>
<p>The only place a kernel thread gives up its CPU is in
<code>sched</code>, and it always switches to the same location in
<code>scheduler</code>, which (almost) always switches to some kernel
thread that previously called <code>sched</code>. Thus, if one were to
print out the line numbers where xv6 switches threads, one would observe
the following simple pattern: <a
href="xv6-riscv-src/kernel/proc.c#L463"><span>(kernel/proc.c:463)</span></a>,
<a
href="xv6-riscv-src/kernel/proc.c#L497"><span>(kernel/proc.c:497)</span></a>,
<a
href="xv6-riscv-src/kernel/proc.c#L463"><span>(kernel/proc.c:463)</span></a>,
<a
href="xv6-riscv-src/kernel/proc.c#L497"><span>(kernel/proc.c:497)</span></a>,
and so on. Procedures that intentionally transfer control to each other
via thread switch are sometimes referred to as <em><span
id="coroutines_1">coroutines</span></em>; in this example, <code
id="sched_5">sched</code> and <code id="scheduler_3">scheduler</code>
are co-routines of each other.</p>
<p>There is one case when the scheduler’s call to <code
id="swtch_9">swtch</code> does not end up in <code
id="sched_6">sched</code>. <code>allocproc</code> sets the context
<code>ra</code> register of a new process to <code
id="forkret_1">forkret</code> <a
href="xv6-riscv-src/kernel/proc.c#L515"><span>(kernel/proc.c:515)</span></a>,
so that its first <code>swtch</code> “returns” to the start of that
function. <code>forkret</code> exists to release the <code
id="p-&gt;lock_4">p-&gt;lock</code>; otherwise, since the new process
needs to return to user space as if returning from <code>fork</code>, it
could instead start at <code>usertrapret</code>.</p>
<p><code>scheduler</code> <a
href="xv6-riscv-src/kernel/proc.c#L445"><span>(kernel/proc.c:445)</span></a>
runs a loop: find a process to run, run it until it yields, repeat. The
scheduler loops over the process table looking for a runnable process,
one that has <code>p-&gt;state</code> <code>==</code>
<code>RUNNABLE</code>. Once it finds a process, it sets the per-CPU
current process variable <code>c-&gt;proc</code>, marks the process as
<code>RUNNING</code>, and then calls <code id="swtch_10">swtch</code> to
start running it <a
href="xv6-riscv-src/kernel/proc.c#L458-L463">(kernel/proc.c:458-463)</a>.</p>
<p>One way to think about the structure of the scheduling code is that
it enforces a set of invariants about each process, and holds <code
id="p-&gt;lock_5">p-&gt;lock</code> whenever those invariants are not
true. One invariant is that if a process is <code>RUNNING</code>, a
timer interrupt’s <code id="yield_3">yield</code> must be able to safely
switch away from the process; this means that the CPU registers must
hold the process’s register values (i.e. <code>swtch</code> hasn’t moved
them to a <code>context</code>), and <code>c-&gt;proc</code> must refer
to the process. Another invariant is that if a process is <code
id="RUNNABLE_1">RUNNABLE</code>, it must be safe for an idle CPU’s <code
id="scheduler_4">scheduler</code> to run it; this means that <code
id="p-&gt;context_1">p-&gt;context</code> must hold the process’s
registers (i.e., they are not actually in the real registers), that no
CPU is executing on the process’s kernel stack, and that no CPU’s
<code>c-&gt;proc</code> refers to the process. Observe that these
properties are often not true while <code>p-&gt;lock</code> is held.</p>
<p>Maintaining the above invariants is the reason why xv6 often acquires
<code id="p-&gt;lock_6">p-&gt;lock</code> in one thread and releases it
in another, for example acquiring in <code>yield</code> and releasing in
<code>scheduler</code>. Once <code>yield</code> has started to modify a
running process’s state to make it <code>RUNNABLE</code>, the lock must
remain held until the invariants are restored: the earliest correct
release point is after <code>scheduler</code> (running on its own stack)
clears <code>c-&gt;proc</code>. Similarly, once <code>scheduler</code>
starts to convert a <code>RUNNABLE</code> process to
<code>RUNNING</code>, the lock cannot be released until the kernel
thread is completely running (after the <code>swtch</code>, for example
in <code>yield</code>).</p>
</section>
<section id="code-mycpu-and-myproc" class="level2" data-number="7.4">
<h2 data-number="7.4"><span class="header-section-number">7.4</span>
Code: mycpu and myproc</h2>
<p>Xv6 often needs a pointer to the current process’s <code>proc</code>
structure. On a uniprocessor one could have a global variable pointing
to the current <code>proc</code>. This doesn’t work on a multi-core
machine, since each core executes a different process. The way to solve
this problem is to exploit the fact that each core has its own set of
registers; we can use one of those registers to help find per-core
information.</p>
<p>Xv6 maintains a <code id="struct_cpu_1">struct cpu</code> for each
CPU <a
href="xv6-riscv-src/kernel/proc.h#L22"><span>(kernel/proc.h:22)</span></a>,
which records the process currently running on that CPU (if any), saved
registers for the CPU’s scheduler thread, and the count of nested
spinlocks needed to manage interrupt disabling. The function <code
id="mycpu_1">mycpu</code> <a
href="xv6-riscv-src/kernel/proc.c#L74"><span>(kernel/proc.c:74)</span></a>
returns a pointer to the current CPU’s <code>struct cpu</code>. RISC-V
numbers its CPUs, giving each a <em><span
id="hartid_1">hartid</span></em>. Xv6 ensures that each CPU’s hartid is
stored in that CPU’s <code>tp</code> register while in the kernel. This
allows <code>mycpu</code> to use <code>tp</code> to index an array of
<code>cpu</code> structures to find the right one.</p>
<p>Ensuring that a CPU’s <code>tp</code> always holds the CPU’s hartid
is a little involved. <code>start</code> sets the <code>tp</code>
register early in the CPU’s boot sequence, while still in machine mode
<a
href="xv6-riscv-src/kernel/start.c#L51"><span>(kernel/start.c:51)</span></a>.
<code>usertrapret</code> saves <code>tp</code> in the trampoline page,
because the user process might modify <code>tp</code>. Finally,
<code>uservec</code> restores that saved <code>tp</code> when entering
the kernel from user space <a
href="xv6-riscv-src/kernel/trampoline.S#L77"><span>(kernel/trampoline.S:77)</span></a>.
The compiler guarantees never to use the <code>tp</code> register. It
would be more convenient if xv6 could ask the RISC-V hardware for the
current hartid whenever needed, but RISC-V allows that only in machine
mode, not in supervisor mode.</p>
<p>The return values of <code>cpuid</code> and <code>mycpu</code> are
fragile: if the timer were to interrupt and cause the thread to yield
and then move to a different CPU, a previously returned value would no
longer be correct. To avoid this problem, xv6 requires that callers
disable interrupts, and only enable them after they finish using the
returned <code>struct cpu</code>.</p>
<p>The function <code id="myproc_1">myproc</code> <a
href="xv6-riscv-src/kernel/proc.c#L83"><span>(kernel/proc.c:83)</span></a>
returns the <code>struct proc</code> pointer for the process that is
running on the current CPU. <code>myproc</code> disables interrupts,
invokes <code>mycpu</code>, fetches the current process pointer
(<code>c-&gt;proc</code>) out of the <code>struct cpu</code>, and then
enables interrupts. The return value of <code>myproc</code> is safe to
use even if interrupts are enabled: if a timer interrupt moves the
calling process to a different CPU, its <code>struct proc</code> pointer
will stay the same.</p>
</section>
<section id="sec:sleep" class="level2" data-number="7.5">
<h2 data-number="7.5"><span class="header-section-number">7.5</span>
Sleep and wakeup</h2>
<p>Scheduling and locks help conceal the actions of one thread from
another, but we also need abstractions that help threads intentionally
interact. For example, the reader of a pipe in xv6 may need to wait for
a writing process to produce data; a parent’s call to <code>wait</code>
may need to wait for a child to exit; and a process reading the disk
needs to wait for the disk hardware to finish the read. The xv6 kernel
uses a mechanism called sleep and wakeup in these situations (and many
others). Sleep allows a kernel thread to wait for a specific event;
another thread can call wakeup to indicate that threads waiting for an
event should resume. Sleep and wakeup are often called <em><span
id="sequence_coordination_1">sequence coordination</span></em> or
<em><span id="conditional_synchronization_1">conditional
synchronization</span></em> mechanisms.</p>
<p>Sleep and wakeup provide a relatively low-level synchronization
interface. To motivate the way they work in xv6, we’ll use them to build
a higher-level synchronization mechanism called a <em><span
id="semaphore_1">semaphore</span></em> <span class="citation"
data-cites="dijkstra65">(<a href="#ref-dijkstra65"
role="doc-biblioref">Dijkstra 1965</a>)</span> that coordinates
producers and consumers (xv6 does not use semaphores). A semaphore
maintains a count and provides two operations. The “V” operation (for
the producer) increments the count. The “P” operation (for the consumer)
waits until the count is non-zero, and then decrements it and returns.
If there were only one producer thread and one consumer thread, and they
executed on different CPUs, and the compiler didn’t optimize too
aggressively, this implementation would be correct:</p>
<div class="sourceCode" id="cb24" data-numbers="left"
data-startFrom="100"><pre
class="sourceCode numberSource numberLines C"><code class="sourceCode c" style="counter-reset: source-line 99;"><span id="cb24-100"><a href="#cb24-100"></a>  <span class="kw">struct</span> semaphore <span class="op">{</span></span>
<span id="cb24-101"><a href="#cb24-101"></a>    <span class="kw">struct</span> spinlock lock<span class="op">;</span></span>
<span id="cb24-102"><a href="#cb24-102"></a>    <span class="dt">int</span> count<span class="op">;</span></span>
<span id="cb24-103"><a href="#cb24-103"></a>  <span class="op">};</span></span>
<span id="cb24-104"><a href="#cb24-104"></a></span>
<span id="cb24-105"><a href="#cb24-105"></a>  <span class="dt">void</span></span>
<span id="cb24-106"><a href="#cb24-106"></a>  V<span class="op">(</span><span class="kw">struct</span> semaphore <span class="op">*</span>s<span class="op">)</span></span>
<span id="cb24-107"><a href="#cb24-107"></a>  <span class="op">{</span></span>
<span id="cb24-108"><a href="#cb24-108"></a>     acquire<span class="op">(&amp;</span>s<span class="op">-&gt;</span>lock<span class="op">);</span></span>
<span id="cb24-109"><a href="#cb24-109"></a>     s<span class="op">-&gt;</span>count <span class="op">+=</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb24-110"><a href="#cb24-110"></a>     release<span class="op">(&amp;</span>s<span class="op">-&gt;</span>lock<span class="op">);</span></span>
<span id="cb24-111"><a href="#cb24-111"></a>  <span class="op">}</span></span>
<span id="cb24-112"><a href="#cb24-112"></a></span>
<span id="cb24-113"><a href="#cb24-113"></a>  <span class="dt">void</span></span>
<span id="cb24-114"><a href="#cb24-114"></a>  P<span class="op">(</span><span class="kw">struct</span> semaphore <span class="op">*</span>s<span class="op">)</span></span>
<span id="cb24-115"><a href="#cb24-115"></a>  <span class="op">{</span></span>
<span id="cb24-116"><a href="#cb24-116"></a>     <span class="cf">while</span><span class="op">(</span>s<span class="op">-&gt;</span>count <span class="op">==</span> <span class="dv">0</span><span class="op">)</span></span>
<span id="cb24-117"><a href="#cb24-117"></a>       <span class="op">;</span></span>
<span id="cb24-118"><a href="#cb24-118"></a>     acquire<span class="op">(&amp;</span>s<span class="op">-&gt;</span>lock<span class="op">);</span></span>
<span id="cb24-119"><a href="#cb24-119"></a>     s<span class="op">-&gt;</span>count <span class="op">-=</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb24-120"><a href="#cb24-120"></a>     release<span class="op">(&amp;</span>s<span class="op">-&gt;</span>lock<span class="op">);</span></span>
<span id="cb24-121"><a href="#cb24-121"></a>  <span class="op">}</span></span></code></pre></div>
<p>The implementation above is expensive. If the producer acts rarely,
the consumer will spend most of its time spinning in the
<code>while</code> loop hoping for a non-zero count. The consumer’s CPU
could probably find more productive work than <em><span
id="busy_waiting_1">busy waiting</span></em> by repeatedly <em><span
id="polling_2">polling</span></em> <code>s-&gt;count</code>. Avoiding
busy waiting requires a way for the consumer to yield the CPU and resume
only after <code>V</code> increments the count.</p>
<p>Here’s a step in that direction, though as we will see it is not
enough. Let’s imagine a pair of calls, <code id="sleep_1">sleep</code>
and <code id="wakeup_2">wakeup</code>, that work as follows.
<code>sleep(chan)</code> sleeps on the arbitrary value <code
id="chan_1">chan</code>, called the <em><span id="wait_channel_1">wait
channel</span></em>. <code>sleep</code> puts the calling process to
sleep, releasing the CPU for other work. <code>wakeup(chan)</code> wakes
all processes sleeping on <code>chan</code> (if any), causing their
<code>sleep</code> calls to return. If no processes are waiting on
<code>chan</code>, <code>wakeup</code> does nothing. We can change the
semaphore implementation to use <code>sleep</code> and
<code>wakeup</code> (changes highlighted in yellow):</p>
<span id="line:test" label="line:test"></span>
<span id="line:sleep" label="line:sleep"></span>
<div class="sourceCode" id="cb25" data-numbers="left"
data-startFrom="200"><pre
class="sourceCode numberSource numberLines C"><code class="sourceCode c" style="counter-reset: source-line 199;"><span id="cb25-200"><a href="#cb25-200"></a>  <span class="dt">void</span></span>
<span id="cb25-201"><a href="#cb25-201"></a>  V<span class="op">(</span><span class="kw">struct</span> semaphore <span class="op">*</span>s<span class="op">)</span></span>
<span id="cb25-202"><a href="#cb25-202"></a>  <span class="op">{</span></span>
<span id="cb25-203"><a href="#cb25-203"></a>     acquire<span class="op">(&amp;</span>s<span class="op">-&gt;</span>lock<span class="op">);</span></span>
<span id="cb25-204"><a href="#cb25-204"></a>     s<span class="op">-&gt;</span>count <span class="op">+=</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb25-205"><a href="#cb25-205"></a>     wakeup<span class="op">(</span>s<span class="op">);</span></span>
<span id="cb25-206"><a href="#cb25-206"></a>     release<span class="op">(&amp;</span>s<span class="op">-&gt;</span>lock<span class="op">);</span></span>
<span id="cb25-207"><a href="#cb25-207"></a>  <span class="op">}</span></span>
<span id="cb25-208"><a href="#cb25-208"></a>  </span>
<span id="cb25-209"><a href="#cb25-209"></a>  <span class="dt">void</span></span>
<span id="cb25-210"><a href="#cb25-210"></a>  P<span class="op">(</span><span class="kw">struct</span> semaphore <span class="op">*</span>s<span class="op">)</span></span>
<span id="cb25-211"><a href="#cb25-211"></a>  <span class="op">{</span></span>
<span id="cb25-212"><a href="#cb25-212"></a>    <span class="cf">while</span><span class="op">(</span>s<span class="op">-&gt;</span>count <span class="op">==</span> <span class="dv">0</span><span class="op">)</span>    </span>
<span id="cb25-213"><a href="#cb25-213"></a>      sleep<span class="op">(</span>s<span class="op">);</span></span>
<span id="cb25-214"><a href="#cb25-214"></a>    acquire<span class="op">(&amp;</span>s<span class="op">-&gt;</span>lock<span class="op">);</span></span>
<span id="cb25-215"><a href="#cb25-215"></a>    s<span class="op">-&gt;</span>count <span class="op">-=</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb25-216"><a href="#cb25-216"></a>    release<span class="op">(&amp;</span>s<span class="op">-&gt;</span>lock<span class="op">);</span></span>
<span id="cb25-217"><a href="#cb25-217"></a>  <span class="op">}</span></span></code></pre></div>
<p><code>P</code> now gives up the CPU instead of spinning, which is
nice. However, it turns out not to be straightforward to design
<code>sleep</code> and <code>wakeup</code> with this interface without
suffering from what is known as the <em><span id="lost_wake-up_1">lost
wake-up</span></em> problem. Suppose that <code>P</code> finds that
<code>s-&gt;count</code> <code>==</code> <code>0</code> on line <a
href="#line:test" data-reference-type="ref"
data-reference="line:test">212</a>. While <code>P</code> is between
lines <a href="#line:test" data-reference-type="ref"
data-reference="line:test">212</a> and <a href="#line:sleep"
data-reference-type="ref" data-reference="line:sleep">213</a>,
<code>V</code> runs on another CPU: it changes <code>s-&gt;count</code>
to be nonzero and calls <code>wakeup</code>, which finds no processes
sleeping and thus does nothing. Now <code>P</code> continues executing
at line <a href="#line:sleep" data-reference-type="ref"
data-reference="line:sleep">213</a>: it calls <code>sleep</code> and
goes to sleep. This causes a problem: <code>P</code> is asleep waiting
for a <code>V</code> call that has already happened. Unless we get lucky
and the producer calls <code>V</code> again, the consumer will wait
forever even though the count is non-zero.</p>
<p>The root of this problem is that the invariant that <code>P</code>
only sleeps when <code>s-&gt;count</code> <code>==</code> <code>0</code>
is violated by <code>V</code> running at just the wrong moment. An
incorrect way to protect the invariant would be to move the lock
acquisition (highlighted in yellow below) in <code>P</code> so that its
check of the count and its call to <code>sleep</code> are atomic:</p>
<span id="line:test1" label="line:test1"></span>
<span id="line:sleep1" label="line:sleep1"></span>
<div class="sourceCode" id="cb26" data-numbers="left"
data-startFrom="300"><pre
class="sourceCode numberSource numberLines C"><code class="sourceCode c" style="counter-reset: source-line 299;"><span id="cb26-300"><a href="#cb26-300"></a>  <span class="dt">void</span></span>
<span id="cb26-301"><a href="#cb26-301"></a>  V<span class="op">(</span><span class="kw">struct</span> semaphore <span class="op">*</span>s<span class="op">)</span></span>
<span id="cb26-302"><a href="#cb26-302"></a>  <span class="op">{</span></span>
<span id="cb26-303"><a href="#cb26-303"></a>    acquire<span class="op">(&amp;</span>s<span class="op">-&gt;</span>lock<span class="op">);</span></span>
<span id="cb26-304"><a href="#cb26-304"></a>    s<span class="op">-&gt;</span>count <span class="op">+=</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb26-305"><a href="#cb26-305"></a>    wakeup<span class="op">(</span>s<span class="op">);</span></span>
<span id="cb26-306"><a href="#cb26-306"></a>    release<span class="op">(&amp;</span>s<span class="op">-&gt;</span>lock<span class="op">);</span></span>
<span id="cb26-307"><a href="#cb26-307"></a>  <span class="op">}</span></span>
<span id="cb26-308"><a href="#cb26-308"></a>  </span>
<span id="cb26-309"><a href="#cb26-309"></a>  <span class="dt">void</span></span>
<span id="cb26-310"><a href="#cb26-310"></a>  P<span class="op">(</span><span class="kw">struct</span> semaphore <span class="op">*</span>s<span class="op">)</span></span>
<span id="cb26-311"><a href="#cb26-311"></a>  <span class="op">{</span></span>
<span id="cb26-312"><a href="#cb26-312"></a>    acquire<span class="op">(&amp;</span>s<span class="op">-&gt;</span>lock<span class="op">);</span></span>
<span id="cb26-313"><a href="#cb26-313"></a>    <span class="cf">while</span><span class="op">(</span>s<span class="op">-&gt;</span>count <span class="op">==</span> <span class="dv">0</span><span class="op">)</span>    </span>
<span id="cb26-314"><a href="#cb26-314"></a>      sleep<span class="op">(</span>s<span class="op">);</span>             </span>
<span id="cb26-315"><a href="#cb26-315"></a>    s<span class="op">-&gt;</span>count <span class="op">-=</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb26-316"><a href="#cb26-316"></a>    release<span class="op">(&amp;</span>s<span class="op">-&gt;</span>lock<span class="op">);</span></span>
<span id="cb26-317"><a href="#cb26-317"></a>  <span class="op">}</span></span></code></pre></div>
<p>One might hope that this version of <code>P</code> would avoid the
lost wakeup because the lock prevents <code>V</code> from executing
between lines <a href="#line:test1" data-reference-type="ref"
data-reference="line:test1">313</a> and <a href="#line:sleep1"
data-reference-type="ref" data-reference="line:sleep1">314</a>. It does
that, but it also deadlocks: <code>P</code> holds the lock while it
sleeps, so <code>V</code> will block forever waiting for the lock.</p>
<p>We’ll fix the preceding scheme by changing <code>sleep</code>’s
interface: the caller must pass the <em><span
id="condition_lock_1">condition lock</span></em> to <code>sleep</code>
so it can release the lock after the calling process is marked as asleep
and waiting on the sleep channel. The lock will force a concurrent
<code>V</code> to wait until <code>P</code> has finished putting itself
to sleep, so that the <code>wakeup</code> will find the sleeping
consumer and wake it up. Once the consumer is awake again <code
id="sleep_2">sleep</code> reacquires the lock before returning. Our new
correct sleep/wakeup scheme is usable as follows (change highlighted in
yellow):</p>
<div class="sourceCode" id="cb27" data-numbers="left"
data-startFrom="400"><pre
class="sourceCode numberSource numberLines C"><code class="sourceCode c" style="counter-reset: source-line 399;"><span id="cb27-400"><a href="#cb27-400"></a>  <span class="dt">void</span></span>
<span id="cb27-401"><a href="#cb27-401"></a>  V<span class="op">(</span><span class="kw">struct</span> semaphore <span class="op">*</span>s<span class="op">)</span></span>
<span id="cb27-402"><a href="#cb27-402"></a>  <span class="op">{</span></span>
<span id="cb27-403"><a href="#cb27-403"></a>    acquire<span class="op">(&amp;</span>s<span class="op">-&gt;</span>lock<span class="op">);</span></span>
<span id="cb27-404"><a href="#cb27-404"></a>    s<span class="op">-&gt;</span>count <span class="op">+=</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb27-405"><a href="#cb27-405"></a>    wakeup<span class="op">(</span>s<span class="op">);</span></span>
<span id="cb27-406"><a href="#cb27-406"></a>    release<span class="op">(&amp;</span>s<span class="op">-&gt;</span>lock<span class="op">);</span></span>
<span id="cb27-407"><a href="#cb27-407"></a>  <span class="op">}</span></span>
<span id="cb27-408"><a href="#cb27-408"></a></span>
<span id="cb27-409"><a href="#cb27-409"></a>  <span class="dt">void</span></span>
<span id="cb27-410"><a href="#cb27-410"></a>  P<span class="op">(</span><span class="kw">struct</span> semaphore <span class="op">*</span>s<span class="op">)</span></span>
<span id="cb27-411"><a href="#cb27-411"></a>  <span class="op">{</span></span>
<span id="cb27-412"><a href="#cb27-412"></a>    acquire<span class="op">(&amp;</span>s<span class="op">-&gt;</span>lock<span class="op">);</span></span>
<span id="cb27-413"><a href="#cb27-413"></a>    <span class="cf">while</span><span class="op">(</span>s<span class="op">-&gt;</span>count <span class="op">==</span> <span class="dv">0</span><span class="op">)</span></span>
<span id="cb27-414"><a href="#cb27-414"></a>       sleep<span class="op">(</span>s<span class="op">,</span> <span class="op">&amp;</span>s<span class="op">-&gt;</span>lock<span class="op">);</span></span>
<span id="cb27-415"><a href="#cb27-415"></a>    s<span class="op">-&gt;</span>count <span class="op">-=</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb27-416"><a href="#cb27-416"></a>    release<span class="op">(&amp;</span>s<span class="op">-&gt;</span>lock<span class="op">);</span></span>
<span id="cb27-417"><a href="#cb27-417"></a>  <span class="op">}</span></span></code></pre></div>
<p>The fact that <code>P</code> holds <code>s-&gt;lock</code> prevents
<code>V</code> from trying to wake it up between <code>P</code>’s check
of <code>s-&gt;count</code> and its call to <code>sleep</code>. Note,
however, that we need <code>sleep</code> to atomically release
<code>s-&gt;lock</code> and put the consuming process to sleep, in order
to avoid lost wakeups.</p>
</section>
<section id="code-sleep-and-wakeup" class="level2" data-number="7.6">
<h2 data-number="7.6"><span class="header-section-number">7.6</span>
Code: Sleep and wakeup</h2>
<p>Xv6’s <code id="sleep_3">sleep</code> <a
href="xv6-riscv-src/kernel/proc.c#L536"><span>(kernel/proc.c:536)</span></a>
and <code id="wakeup_3">wakeup</code> <a
href="xv6-riscv-src/kernel/proc.c#L567"><span>(kernel/proc.c:567)</span></a>
provide the interface shown in the last example above, and their
implementation (plus rules for how to use them) ensures that there are
no lost wakeups. The basic idea is to have <code>sleep</code> mark the
current process as <code id="SLEEPING_1">SLEEPING</code> and then call
<code id="sched_7">sched</code> to release the CPU; <code>wakeup</code>
looks for a process sleeping on the given wait channel and marks it as
<code id="RUNNABLE_2">RUNNABLE</code>. Callers of <code>sleep</code> and
<code>wakeup</code> can use any mutually convenient number as the
channel. Xv6 often uses the address of a kernel data structure involved
in the waiting.</p>
<p><code>sleep</code> acquires <code id="p-&gt;lock_7">p-&gt;lock</code>
<a
href="xv6-riscv-src/kernel/proc.c#L547"><span>(kernel/proc.c:547)</span></a>.
Now the process going to sleep holds both <code>p-&gt;lock</code> and
<code>lk</code>. Holding <code>lk</code> was necessary in the caller (in
the example, <code>P</code>): it ensured that no other process (in the
example, one running <code>V</code>) could start a call to
<code>wakeup(chan)</code>. Now that <code>sleep</code> holds
<code>p-&gt;lock</code>, it is safe to release <code>lk</code>: some
other process may start a call to <code>wakeup(chan)</code>, but <code
id="wakeup_4">wakeup</code> will wait to acquire <code
id="p-&gt;lock_8">p-&gt;lock</code>, and thus will wait until
<code>sleep</code> has finished putting the process to sleep, keeping
the <code>wakeup</code> from missing the <code>sleep</code>.</p>
<p>Now that <code>sleep</code> holds <code>p-&gt;lock</code> and no
others, it can put the process to sleep by recording the sleep channel,
changing the process state to <code>SLEEPING</code>, and calling
<code>sched</code> <a
href="xv6-riscv-src/kernel/proc.c#L551-L554">(kernel/proc.c:551-554)</a>.
In a moment it will be clear why it’s critical that
<code>p-&gt;lock</code> is not released (by <code>scheduler</code>)
until after the process is marked <code>SLEEPING</code>.</p>
<p>At some point, a process will acquire the condition lock, set the
condition that the sleeper is waiting for, and call
<code>wakeup(chan)</code>. It’s important that <code>wakeup</code> is
called while holding the condition lock<a href="#fn3"
class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.
<code>wakeup</code> loops over the process table <a
href="xv6-riscv-src/kernel/proc.c#L567"><span>(kernel/proc.c:567)</span></a>.
It acquires the <code>p-&gt;lock</code> of each process it inspects,
both because it may manipulate that process’s state and because
<code>p-&gt;lock</code> ensures that <code>sleep</code> and
<code>wakeup</code> do not miss each other. When <code>wakeup</code>
finds a process in state <code id="SLEEPING_2">SLEEPING</code> with a
matching <code id="chan_2">chan</code>, it changes that process’s state
to <code id="RUNNABLE_3">RUNNABLE</code>. The next time the scheduler
runs, it will see that the process is ready to be run.</p>
<p>Why do the locking rules for <code>sleep</code> and
<code>wakeup</code> ensure a sleeping process won’t miss a wakeup? The
sleeping process holds either the condition lock or its own
<code>p-&gt;lock</code> or both from a point before it checks the
condition to a point after it is marked <code>SLEEPING</code>. The
process calling <code>wakeup</code> holds <em>both</em> of those locks
in <code>wakeup</code>’s loop. Thus the waker either makes the condition
true before the consuming thread checks the condition; or the waker’s
<code>wakeup</code> examines the sleeping thread strictly after it has
been marked <code>SLEEPING</code>. Then <code>wakeup</code> will see the
sleeping process and wake it up (unless something else wakes it up
first).</p>
<p>It is sometimes the case that multiple processes are sleeping on the
same channel; for example, more than one process reading from a pipe. A
single call to <code>wakeup</code> will wake them all up. One of them
will run first and acquire the lock that <code>sleep</code> was called
with, and (in the case of pipes) read whatever data is waiting in the
pipe. The other processes will find that, despite being woken up, there
is no data to be read. From their point of view the wakeup was
“spurious,” and they must sleep again. For this reason
<code>sleep</code> is always called inside a loop that checks the
condition.</p>
<p>No harm is done if two uses of sleep/wakeup accidentally choose the
same channel: they will see spurious wakeups, but looping as described
above will tolerate this problem. Much of the charm of sleep/wakeup is
that it is both lightweight (no need to create special data structures
to act as sleep channels) and provides a layer of indirection (callers
need not know which specific process they are interacting with).</p>
</section>
<section id="code-pipes" class="level2" data-number="7.7">
<h2 data-number="7.7"><span class="header-section-number">7.7</span>
Code: Pipes</h2>
<p>A more complex example that uses <code>sleep</code> and
<code>wakeup</code> to synchronize producers and consumers is xv6’s
implementation of pipes. We saw the interface for pipes in Chapter <a
href="#CH:UNIX" data-reference-type="ref"
data-reference="CH:UNIX">1</a>: bytes written to one end of a pipe are
copied to an in-kernel buffer and then can be read from the other end of
the pipe. Future chapters will examine the file descriptor support
surrounding pipes, but let’s look now at the implementations of <code
id="pipewrite_1">pipewrite</code> and <code
id="piperead_1">piperead</code>.</p>
<p>Each pipe is represented by a <code
id="struct_pipe_1">struct pipe</code>, which contains a
<code>lock</code> and a <code>data</code> buffer. The fields
<code>nread</code> and <code>nwrite</code> count the total number of
bytes read from and written to the buffer. The buffer wraps around: the
next byte written after <code>buf[PIPESIZE-1]</code> is
<code>buf[0]</code>. The counts do not wrap. This convention lets the
implementation distinguish a full buffer (<code>nwrite</code>
<code>==</code> <code>nread+PIPESIZE</code>) from an empty buffer
(<code>nwrite</code> <code>==</code> <code>nread</code>), but it means
that indexing into the buffer must use <code>buf[nread</code>
<code>%</code> <code>PIPESIZE]</code> instead of just
<code>buf[nread]</code> (and similarly for <code>nwrite</code>).</p>
<p>Let’s suppose that calls to <code>piperead</code> and
<code>pipewrite</code> happen simultaneously on two different CPUs.
<code>pipewrite</code> <a
href="xv6-riscv-src/kernel/pipe.c#L77"><span>(kernel/pipe.c:77)</span></a>
begins by acquiring the pipe’s lock, which protects the counts, the
data, and their associated invariants. <code>piperead</code> <a
href="xv6-riscv-src/kernel/pipe.c#L106"><span>(kernel/pipe.c:106)</span></a>
then tries to acquire the lock too, but cannot. It spins in
<code>acquire</code> <a
href="xv6-riscv-src/kernel/spinlock.c#L22"><span>(kernel/spinlock.c:22)</span></a>
waiting for the lock. While <code>piperead</code> waits,
<code>pipewrite</code> loops over the bytes being written
(<code>addr[0..n-1]</code>), adding each to the pipe in turn <a
href="xv6-riscv-src/kernel/pipe.c#L95"><span>(kernel/pipe.c:95)</span></a>.
During this loop, it could happen that the buffer fills <a
href="xv6-riscv-src/kernel/pipe.c#L88"><span>(kernel/pipe.c:88)</span></a>.
In this case, <code>pipewrite</code> calls <code>wakeup</code> to alert
any sleeping readers to the fact that there is data waiting in the
buffer and then sleeps on <code>&amp;pi-&gt;nwrite</code> to wait for a
reader to take some bytes out of the buffer. <code>sleep</code> releases
<code>pi-&gt;lock</code> as part of putting <code>pipewrite</code>’s
process to sleep.</p>
<p>Now that <code>pi-&gt;lock</code> is available, <code>piperead</code>
manages to acquire it and enters its critical section: it finds that
<code>pi-&gt;nread</code> <code>!=</code> <code>pi-&gt;nwrite</code> <a
href="xv6-riscv-src/kernel/pipe.c#L113"><span>(kernel/pipe.c:113)</span></a>
(<code>pipewrite</code> went to sleep because <code>pi-&gt;nwrite</code>
<code>==</code> <code>pi-&gt;nread+PIPESIZE</code> <a
href="xv6-riscv-src/kernel/pipe.c#L88"><span>(kernel/pipe.c:88)</span></a>),
so it falls through to the <code>for</code> loop, copies data out of the
pipe <a
href="xv6-riscv-src/kernel/pipe.c#L120"><span>(kernel/pipe.c:120)</span></a>,
and increments <code>nread</code> by the number of bytes copied. That
many bytes are now available for writing, so <code>piperead</code> calls
<code>wakeup</code> <a
href="xv6-riscv-src/kernel/pipe.c#L127"><span>(kernel/pipe.c:127)</span></a>
to wake any sleeping writers before it returns. <code>wakeup</code>
finds a process sleeping on <code>&amp;pi-&gt;nwrite</code>, the process
that was running <code>pipewrite</code> but stopped when the buffer
filled. It marks that process as <code
id="RUNNABLE_4">RUNNABLE</code>.</p>
<p>The pipe code uses separate sleep channels for reader and writer
(<code>pi-&gt;nread</code> and <code>pi-&gt;nwrite</code>); this might
make the system more efficient in the unlikely event that there are lots
of readers and writers waiting for the same pipe. The pipe code sleeps
inside a loop checking the sleep condition; if there are multiple
readers or writers, all but the first process to wake up will see the
condition is still false and sleep again.</p>
</section>
<section id="code-wait-exit-and-kill" class="level2" data-number="7.8">
<h2 data-number="7.8"><span class="header-section-number">7.8</span>
Code: Wait, exit, and kill</h2>
<p><code>sleep</code> and <code>wakeup</code> can be used for many kinds
of waiting. An interesting example, introduced in Chapter <a
href="#CH:UNIX" data-reference-type="ref"
data-reference="CH:UNIX">1</a>, is the interaction between a child’s
<code id="exit_2">exit</code> and its parent’s <code
id="wait_4">wait</code>. At the time of the child’s death, the parent
may already be sleeping in <span><code>wait</code></span>, or may be
doing something else; in the latter case, a subsequent call to
<span><code>wait</code></span> must observe the child’s death, perhaps
long after it calls <span><code>exit</code></span>. The way that xv6
records the child’s demise until <span><code>wait</code></span> observes
it is for <span><code>exit</code></span> to put the caller into the
<code id="ZOMBIE_1">ZOMBIE</code> state, where it stays until the
parent’s <span><code>wait</code></span> notices it, changes the child’s
state to <span><code>UNUSED</code></span>, copies the child’s exit
status, and returns the child’s process ID to the parent. If the parent
exits before the child, the parent gives the child to the
<code>init</code> process, which perpetually calls
<span><code>wait</code></span>; thus every child has a parent to clean
up after it. A challenge is to avoid races and deadlock between
simultaneous parent and child <code>wait</code> and <code>exit</code>,
as well as simultaneous <code>exit</code> and <code>exit</code>.</p>
<p><code>wait</code> starts by acquiring <code>wait_lock</code> <a
href="xv6-riscv-src/kernel/proc.c#L391"><span>(kernel/proc.c:391)</span></a>.
The reason is that <code>wait_lock</code> acts as the condition lock
that helps ensure that the parent doesn’t miss a <code>wakeup</code>
from an exiting child. Then <code>wait</code> scans the process table.
If it finds a child in <code>ZOMBIE</code> state, it frees that child’s
resources and its <code>proc</code> structure, copies the child’s exit
status to the address supplied to <code>wait</code> (if it is not 0),
and returns the child’s process ID. If <code>wait</code> finds children
but none have exited, it calls <code>sleep</code> to wait for any of
them to exit <a
href="xv6-riscv-src/kernel/proc.c#L433"><span>(kernel/proc.c:433)</span></a>,
then scans again. <code>wait</code> often holds two locks,
<code>wait_lock</code> and some process’s <code>pp-&gt;lock</code>; the
deadlock-avoiding order is first <code>wait_lock</code> and then
<code>pp-&gt;lock</code>.</p>
<p><code>exit</code> <a
href="xv6-riscv-src/kernel/proc.c#L347"><span>(kernel/proc.c:347)</span></a>
records the exit status, frees some resources, calls
<code>reparent</code> to give its children to the <code>init</code>
process, wakes up the parent in case it is in <code>wait</code>, marks
the caller as a zombie, and permanently yields the CPU.
<code>exit</code> holds both <code>wait_lock</code> and
<code>p-&gt;lock</code> during this sequence. It holds
<code>wait_lock</code> because it’s the condition lock for the
<code>wakeup(p-&gt;parent)</code>, preventing a parent in
<code>wait</code> from losing the wakeup. <code>exit</code> must hold
<code>p-&gt;lock</code> for this sequence also, to prevent a parent in
<code>wait</code> from seeing that the child is in state
<code>ZOMBIE</code> before the child has finally called
<code>swtch</code>. <code>exit</code> acquires these locks in the same
order as <code>wait</code> to avoid deadlock.</p>
<p>It may look incorrect for <code>exit</code> to wake up the parent
before setting its state to <code>ZOMBIE</code>, but that is safe:
although <code>wakeup</code> may cause the parent to run, the loop in
<code>wait</code> cannot examine the child until the child’s
<code>p-&gt;lock</code> is released by
<span><code>scheduler</code></span>, so <code>wait</code> can’t look at
the exiting process until well after <code>exit</code> has set its state
to <code>ZOMBIE</code> <a
href="xv6-riscv-src/kernel/proc.c#L379"><span>(kernel/proc.c:379)</span></a>.</p>
<p>While <code>exit</code> allows a process to terminate itself,
<code>kill</code> <a
href="xv6-riscv-src/kernel/proc.c#L586"><span>(kernel/proc.c:586)</span></a>
lets one process request that another terminate. It would be too complex
for <code>kill</code> to directly destroy the victim process, since the
victim might be executing on another CPU, perhaps in the middle of a
sensitive sequence of updates to kernel data structures. Thus
<code>kill</code> does very little: it just sets the victim’s <code
id="p-&gt;killed_1">p-&gt;killed</code> and, if it is sleeping, wakes it
up. Eventually the victim will enter or leave the kernel, at which point
code in <code>usertrap</code> will call <code>exit</code> if
<code>p-&gt;killed</code> is set (it checks by calling
<code>killed</code> <a
href="xv6-riscv-src/kernel/proc.c#L615"><span>(kernel/proc.c:615)</span></a>).
If the victim is running in user space, it will soon enter the kernel by
making a system call or because the timer (or some other device)
interrupts.</p>
<p>If the victim process is in <code>sleep</code>, <code>kill</code>’s
call to <code>wakeup</code> will cause the victim to return from
<code>sleep</code>. This is potentially dangerous because the condition
being waiting for may not be true. However, xv6 calls to
<code>sleep</code> are always wrapped in a <code>while</code> loop that
re-tests the condition after <code>sleep</code> returns. Some calls to
<code>sleep</code> also test <code>p-&gt;killed</code> in the loop, and
abandon the current activity if it is set. This is only done when such
abandonment would be correct. For example, the pipe read and write code
returns if the killed flag is set; eventually the code will return back
to trap, which will again check <code>p-&gt;killed</code> and exit.</p>
<p>Some xv6 <code>sleep</code> loops do not check
<code>p-&gt;killed</code> because the code is in the middle of a
multi-step system call that should be atomic. The virtio driver <a
href="xv6-riscv-src/kernel/virtio_disk.c#L285"><span>(kernel/virtio_disk.c:285)</span></a>
is an example: it does not check <code>p-&gt;killed</code> because a
disk operation may be one of a set of writes that are all needed in
order for the file system to be left in a correct state. A process that
is killed while waiting for disk I/O won’t exit until it completes the
current system call and <code>usertrap</code> sees the killed flag.</p>
</section>
<section id="process-locking" class="level2" data-number="7.9">
<h2 data-number="7.9"><span class="header-section-number">7.9</span>
Process Locking</h2>
<p>The lock associated with each process (<code>p-&gt;lock</code>) is
the most complex lock in xv6. A simple way to think about
<code>p-&gt;lock</code> is that it must be held while reading or writing
any of the following <code>struct proc</code> fields:
<code>p-&gt;state</code>, <code>p-&gt;chan</code>,
<code>p-&gt;killed</code>, <code>p-&gt;xstate</code>, and
<code>p-&gt;pid</code>. These fields can be used by other processes, or
by scheduler threads on other cores, so it’s natural that they must be
protected by a lock.</p>
<p>However, most uses of <code>p-&gt;lock</code> are protecting
higher-level aspects of xv6’s process data structures and algorithms.
Here’s the full set of things that <code>p-&gt;lock</code> does:</p>
<ul>
<li><p>Along with <code>p-&gt;state</code>, it prevents races in
allocating <code>proc[]</code> slots for new processes.</p></li>
<li><p>It conceals a process from view while it is being created or
destroyed.</p></li>
<li><p>It prevents a parent’s <code>wait</code> from collecting a
process that has set its state to <code>ZOMBIE</code> but has not yet
yielded the CPU.</p></li>
<li><p>It prevents another core’s scheduler from deciding to run a
yielding process after it sets its state to <code>RUNNABLE</code> but
before it finishes <code>swtch</code>.</p></li>
<li><p>It ensures that only one core’s scheduler decides to run a
<code>RUNNABLE</code> processes.</p></li>
<li><p>It prevents a timer interrupt from causing a process to yield
while it is in <code>swtch</code>.</p></li>
<li><p>Along with the condition lock, it helps prevent
<code>wakeup</code> from overlooking a process that is calling
<code>sleep</code> but has not finished yielding the CPU.</p></li>
<li><p>It prevents the victim process of <code>kill</code> from exiting
and perhaps being re-allocated between <code>kill</code>’s check of
<code>p-&gt;pid</code> and setting <code>p-&gt;killed</code>.</p></li>
<li><p>It makes <code>kill</code>’s check and write of
<code>p-&gt;state</code> atomic.</p></li>
</ul>
<p>The <code>p-&gt;parent</code> field is protected by the global lock
<code>wait_lock</code> rather than by <code>p-&gt;lock</code>. Only a
process’s parent modifies <code>p-&gt;parent</code>, though the field is
read both by the process itself and by other processes searching for
their children. The purpose of <code>wait_lock</code> is to act as the
condition lock when <code>wait</code> sleeps waiting for any child to
exit. An exiting child holds either <code>wait_lock</code> or
<code>p-&gt;lock</code> until after it has set its state to
<code>ZOMBIE</code>, woken up its parent, and yielded the CPU.
<code>wait_lock</code> also serializes concurrent <code>exit</code>s by
a parent and child, so that the <code>init</code> process (which
inherits the child) is guaranteed to be woken up from its
<code>wait</code>. <code>wait_lock</code> is a global lock rather than a
per-process lock in each parent, because, until a process acquires it,
it cannot know who its parent is.</p>
</section>
<section id="real-world-6" class="level2" data-number="7.10">
<h2 data-number="7.10"><span class="header-section-number">7.10</span>
Real world</h2>
<p>The xv6 scheduler implements a simple scheduling policy, which runs
each process in turn. This policy is called <em><span
id="round_robin_1">round robin</span></em>. Real operating systems
implement more sophisticated policies that, for example, allow processes
to have priorities. The idea is that a runnable high-priority process
will be preferred by the scheduler over a runnable low-priority process.
These policies can become complex quickly because there are often
competing goals: for example, the operating system might also want to
guarantee fairness and high throughput. In addition, complex policies
may lead to unintended interactions such as <em><span
id="priority_inversion_1">priority inversion</span></em> and <em><span
id="convoys_1">convoys</span></em>. Priority inversion can happen when a
low-priority and high-priority process both use a particular lock, which
when acquired by the low-priority process can prevent the high-priority
process from making progress. A long convoy of waiting processes can
form when many high-priority processes are waiting for a low-priority
process that acquires a shared lock; once a convoy has formed it can
persist for long time. To avoid these kinds of problems additional
mechanisms are necessary in sophisticated schedulers.</p>
<p><code>sleep</code> and <code>wakeup</code> are a simple and effective
synchronization method, but there are many others. The first challenge
in all of them is to avoid the “lost wakeups” problem we saw at the
beginning of the chapter. The original Unix kernel’s <code>sleep</code>
simply disabled interrupts, which sufficed because Unix ran on a
single-CPU system. Because xv6 runs on multiprocessors, it adds an
explicit lock to <code>sleep</code>. FreeBSD’s <code>msleep</code> takes
the same approach. Plan 9’s <code>sleep</code> uses a callback function
that runs with the scheduling lock held just before going to sleep; the
function serves as a last-minute check of the sleep condition, to avoid
lost wakeups. The Linux kernel’s <code>sleep</code> uses an explicit
process queue, called a wait queue, instead of a wait channel; the queue
has its own internal lock.</p>
<p>Scanning the entire set of processes in <code>wakeup</code> is
inefficient. A better solution is to replace the <code>chan</code> in
both <code>sleep</code> and <code>wakeup</code> with a data structure
that holds a list of processes sleeping on that structure, such as
Linux’s wait queue. Plan 9’s <code>sleep</code> and <code>wakeup</code>
call that structure a rendezvous point. Many thread libraries refer to
the same structure as a condition variable; in that context, the
operations <code>sleep</code> and <code>wakeup</code> are called
<code>wait</code> and <code>signal</code>. All of these mechanisms share
the same flavor: the sleep condition is protected by some kind of lock
dropped atomically during sleep.</p>
<p>The implementation of <code>wakeup</code> wakes up all processes that
are waiting on a particular channel, and it might be the case that many
processes are waiting for that particular channel. The operating system
will schedule all these processes and they will race to check the sleep
condition. Processes that behave in this way are sometimes called a
<em><span id="thundering_herd_1">thundering herd</span></em>, and it is
best avoided. Most condition variables have two primitives for
<code>wakeup</code>: <code>signal</code>, which wakes up one process,
and <code>broadcast</code>, which wakes up all waiting processes.</p>
<p>Semaphores are often used for synchronization. The count typically
corresponds to something like the number of bytes available in a pipe
buffer or the number of zombie children that a process has. Using an
explicit count as part of the abstraction avoids the “lost wakeup”
problem: there is an explicit count of the number of wakeups that have
occurred. The count also avoids the spurious wakeup and thundering herd
problems.</p>
<p>Terminating processes and cleaning them up introduces much complexity
in xv6. In most operating systems it is even more complex, because, for
example, the victim process may be deep inside the kernel sleeping, and
unwinding its stack requires care, since each function on the call stack
may need to do some clean-up. Some languages help out by providing an
exception mechanism, but not C. Furthermore, there are other events that
can cause a sleeping process to be woken up, even though the event it is
waiting for has not happened yet. For example, when a Unix process is
sleeping, another process may send a <code id="signal_1">signal</code>
to it. In this case, the process will return from the interrupted system
call with the value -1 and with the error code set to EINTR. The
application can check for these values and decide what to do. Xv6
doesn’t support signals and this complexity doesn’t arise.</p>
<p>Xv6’s support for <code>kill</code> is not entirely satisfactory:
there are sleep loops which probably should check for
<code>p-&gt;killed</code>. A related problem is that, even for
<code>sleep</code> loops that check <code>p-&gt;killed</code>, there is
a race between <code>sleep</code> and <code>kill</code>; the latter may
set <code>p-&gt;killed</code> and try to wake up the victim just after
the victim’s loop checks <code>p-&gt;killed</code> but before it calls
<code>sleep</code>. If this problem occurs, the victim won’t notice the
<code>p-&gt;killed</code> until the condition it is waiting for occurs.
This may be quite a bit later or even never (e.g., if the victim is
waiting for input from the console, but the user doesn’t type any
input).</p>
<p>A real operating system would find free <code>proc</code> structures
with an explicit free list in constant time instead of the linear-time
search in <code>allocproc</code>; xv6 uses the linear scan for
simplicity.</p>
</section>
<section id="exercises-6" class="level2" data-number="7.11">
<h2 data-number="7.11"><span class="header-section-number">7.11</span>
Exercises</h2>
<ol>
<li><p>Sleep has to check <code>lk != &amp;p-&gt;lock</code> to avoid a
deadlock Suppose the special case were eliminated by replacing</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode C"><code class="sourceCode c"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span><span class="op">(</span>lk <span class="op">!=</span> <span class="op">&amp;</span>p<span class="op">-&gt;</span>lock<span class="op">){</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>  acquire<span class="op">(&amp;</span>p<span class="op">-&gt;</span>lock<span class="op">);</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>  release<span class="op">(</span>lk<span class="op">);</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>with</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode C"><code class="sourceCode c"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>release<span class="op">(</span>lk<span class="op">);</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>acquire<span class="op">(&amp;</span>p<span class="op">-&gt;</span>lock<span class="op">);</span></span></code></pre></div>
<p>Doing this would break <code>sleep</code>. How?</p></li>
<li><p>Implement semaphores in xv6 without using <code>sleep</code> and
<code>wakeup</code> (but it is OK to use spin locks). Replace the uses
of sleep and wakeup in xv6 with semaphores. Judge the result.</p></li>
<li><p>Fix the race mentioned above between <code>kill</code> and
<code>sleep</code>, so that a <code>kill</code> that occurs after the
victim’s sleep loop checks <code>p-&gt;killed</code> but before it calls
<code>sleep</code> results in the victim abandoning the current system
call.</p></li>
<li><p>Design a plan so that every sleep loop checks
<code>p-&gt;killed</code> so that, for example, a process that is in the
virtio driver can return quickly from the while loop if it is killed by
another process.</p></li>
<li><p>Modify xv6 to use only one context switch when switching from one
process’s kernel thread to another, rather than switching through the
scheduler thread. The yielding thread will need to select the next
thread itself and call <code>swtch</code>. The challenges will be to
prevent multiple cores from executing the same thread accidentally; to
get the locking right; and to avoid deadlocks.</p></li>
<li><p>Modify xv6’s <code>scheduler</code> to use the RISC-V
<code>WFI</code> (wait for interrupt) instruction when no processes are
runnable. Try to ensure that, any time there are runnable processes
waiting to run, no cores are pausing in <code>WFI</code>.</p></li>
</ol>
</section>
</section>
<section id="CH:FS" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> File
system</h1>
<p>The purpose of a file system is to organize and store data. File
systems typically support sharing of data among users and applications,
as well as <em><span id="persistence_1">persistence</span></em> so that
data is still available after a reboot.</p>
<p>The xv6 file system provides Unix-like files, directories, and
pathnames (see Chapter <a href="#CH:UNIX" data-reference-type="ref"
data-reference="CH:UNIX">1</a>), and stores its data on a virtio disk
for persistence. The file system addresses several challenges:</p>
<ul>
<li><p>The file system needs on-disk data structures to represent the
tree of named directories and files, to record the identities of the
blocks that hold each file’s content, and to record which areas of the
disk are free.</p></li>
<li><p>The file system must support <em><span
id="crash_recovery_1">crash recovery</span></em>. That is, if a crash
(e.g., power failure) occurs, the file system must still work correctly
after a restart. The risk is that a crash might interrupt a sequence of
updates and leave inconsistent on-disk data structures (e.g., a block
that is both used in a file and marked free).</p></li>
<li><p>Different processes may operate on the file system at the same
time, so the file-system code must coordinate to maintain
invariants.</p></li>
<li><p>Accessing a disk is orders of magnitude slower than accessing
memory, so the file system must maintain an in-memory cache of popular
blocks.</p></li>
</ul>
<p>The rest of this chapter explains how xv6 addresses these
challenges.</p>
<section id="overview" class="level2" data-number="8.1">
<h2 data-number="8.1"><span class="header-section-number">8.1</span>
Overview</h2>
<p>The xv6 file system implementation is organized in seven layers,
shown in Figure <a href="#fig:fslayer" data-reference-type="ref"
data-reference="fig:fslayer">8.1</a>. The disk layer reads and writes
blocks on an virtio hard drive. The buffer cache layer caches disk
blocks and synchronizes access to them, making sure that only one kernel
process at a time can modify the data stored in any particular block.
The logging layer allows higher layers to wrap updates to several blocks
in a <em><span id="transaction_1">transaction</span></em>, and ensures
that the blocks are updated atomically in the face of crashes (i.e., all
of them are updated or none). The inode layer provides individual files,
each represented as an <em><span id="inode_2">inode</span></em> with a
unique i-number and some blocks holding the file’s data. The directory
layer implements each directory as a special kind of inode whose content
is a sequence of directory entries, each of which contains a file’s name
and i-number. The pathname layer provides hierarchical path names like
<code>/usr/rtm/xv6/fs.c</code>, and resolves them with recursive lookup.
The file descriptor layer abstracts many Unix resources (e.g., pipes,
devices, files, etc.) using the file system interface, simplifying the
lives of application programmers.</p>
<figure id="fig:fslayer">
<img src="fig/fslayer.svg" />
<figcaption>Layers of the xv6 file system.</figcaption>
</figure>
<p>Disk hardware traditionally presents the data on the disk as a
numbered sequence of 512-byte <em>blocks</em> (also called
<em>sectors</em>): sector 0 is the first 512 bytes, sector 1 is the
next, and so on. The block size that an operating system uses for its
file system maybe different than the sector size that a disk uses, but
typically the block size is a multiple of the sector size. Xv6 holds
copies of blocks that it has read into memory in objects of type
<code>struct buf</code> <a
href="xv6-riscv-src/kernel/buf.h#L1"><span>(kernel/buf.h:1)</span></a>.
The data stored in this structure is sometimes out of sync with the
disk: it might have not yet been read in from disk (the disk is working
on it but hasn’t returned the sector’s content yet), or it might have
been updated by software but not yet written to the disk.</p>
<p>The file system must have a plan for where it stores inodes and
content blocks on the disk. To do so, xv6 divides the disk into several
sections, as Figure <a href="#fig:fslayout" data-reference-type="ref"
data-reference="fig:fslayout">8.2</a> shows. The file system does not
use block 0 (it holds the boot sector). Block 1 is called the <em><span
id="superblock_1">superblock</span></em>; it contains metadata about the
file system (the file system size in blocks, the number of data blocks,
the number of inodes, and the number of blocks in the log). Blocks
starting at 2 hold the log. After the log are the inodes, with multiple
inodes per block. After those come bitmap blocks tracking which data
blocks are in use. The remaining blocks are data blocks; each is either
marked free in the bitmap block, or holds content for a file or
directory. The superblock is filled in by a separate program, called
<code id="mkfs_1">mkfs</code>, which builds an initial file system.</p>
<p>The rest of this chapter discusses each layer, starting with the
buffer cache. Look out for situations where well-chosen abstractions at
lower layers ease the design of higher ones.</p>
</section>
<section id="s:bcache" class="level2" data-number="8.2">
<h2 data-number="8.2"><span class="header-section-number">8.2</span>
Buffer cache layer</h2>
<p>The buffer cache has two jobs: (1) synchronize access to disk blocks
to ensure that only one copy of a block is in memory and that only one
kernel thread at a time uses that copy; (2) cache popular blocks so that
they don’t need to be re-read from the slow disk. The code is in
<code>bio.c</code>.</p>
<p>The main interface exported by the buffer cache consists of <code
id="bread_1">bread</code> and <code id="bwrite_1">bwrite</code>; the
former obtains a <em><span id="buf_1">buf</span></em> containing a copy
of a block which can be read or modified in memory, and the latter
writes a modified buffer to the appropriate block on the disk. A kernel
thread must release a buffer by calling <code
id="brelse_1">brelse</code> when it is done with it. The buffer cache
uses a per-buffer sleep-lock to ensure that only one thread at a time
uses each buffer (and thus each disk block); <code>bread</code> returns
a locked buffer, and <code>brelse</code> releases the lock.</p>
<p>Let’s return to the buffer cache. The buffer cache has a fixed number
of buffers to hold disk blocks, which means that if the file system asks
for a block that is not already in the cache, the buffer cache must
recycle a buffer currently holding some other block. The buffer cache
recycles the least recently used buffer for the new block. The
assumption is that the least recently used buffer is the one least
likely to be used again soon.</p>
<figure id="fig:fslayout">
<img src="fig/fslayout.svg" />
<figcaption>Structure of the xv6 file system. </figcaption>
</figure>
</section>
<section id="code-buffer-cache" class="level2" data-number="8.3">
<h2 data-number="8.3"><span class="header-section-number">8.3</span>
Code: Buffer cache</h2>
<p>The buffer cache is a doubly-linked list of buffers. The function
<code id="binit_1">binit</code>, called by <code id="main_4">main</code>
<a
href="xv6-riscv-src/kernel/main.c#L27"><span>(kernel/main.c:27)</span></a>,
initializes the list with the <code id="NBUF_1">NBUF</code> buffers in
the static array <code>buf</code> <a
href="xv6-riscv-src/kernel/bio.c#L43-L52">(kernel/bio.c:43-52)</a>. All
other access to the buffer cache refer to the linked list via <code
id="bcache.head_1">bcache.head</code>, not the <code>buf</code>
array.</p>
<p>A buffer has two state fields associated with it. The field <code
id="valid_1">valid</code> indicates that the buffer contains a copy of
the block. The field <code id="disk_1">disk</code> indicates that the
buffer content has been handed to the disk, which may change the buffer
(e.g., write data from the disk into <code>data</code>).</p>
<p><code>bread</code> <a
href="xv6-riscv-src/kernel/bio.c#L93"><span>(kernel/bio.c:93)</span></a>
calls <code id="bget_1">bget</code> to get a buffer for the given sector
<a
href="xv6-riscv-src/kernel/bio.c#L97"><span>(kernel/bio.c:97)</span></a>.
If the buffer needs to be read from disk, <code>bread</code> calls <code
id="virtio_disk_rw_1">virtio_disk_rw</code> to do that before returning
the buffer.</p>
<p><code>bget</code> <a
href="xv6-riscv-src/kernel/bio.c#L59"><span>(kernel/bio.c:59)</span></a>
scans the buffer list for a buffer with the given device and sector
numbers <a
href="xv6-riscv-src/kernel/bio.c#L65-L73">(kernel/bio.c:65-73)</a>. If
there is such a buffer, <code id="bget_2">bget</code> acquires the
sleep-lock for the buffer. <code>bget</code> then returns the locked
buffer.</p>
<p>If there is no cached buffer for the given sector, <code
id="bget_3">bget</code> must make one, possibly reusing a buffer that
held a different sector. It scans the buffer list a second time, looking
for a buffer that is not in use (<code>b-&gt;refcnt = 0</code>); any
such buffer can be used. <code>bget</code> edits the buffer metadata to
record the new device and sector number and acquires its sleep-lock.
Note that the assignment <code>b-&gt;valid = 0</code> ensures that
<code>bread</code> will read the block data from disk rather than
incorrectly using the buffer’s previous contents.</p>
<p>It is important that there is at most one cached buffer per disk
sector, to ensure that readers see writes, and because the file system
uses locks on buffers for synchronization. <code>bget</code> ensures
this invariant by holding the <code>bcache.lock</code> continuously from
the first loop’s check of whether the block is cached through the second
loop’s declaration that the block is now cached (by setting
<code>dev</code>, <code>blockno</code>, and <code>refcnt</code>). This
causes the check for a block’s presence and (if not present) the
designation of a buffer to hold the block to be atomic.</p>
<p>It is safe for <code>bget</code> to acquire the buffer’s sleep-lock
outside of the <code>bcache.lock</code> critical section, since the
non-zero <code>b-&gt;refcnt</code> prevents the buffer from being
re-used for a different disk block. The sleep-lock protects reads and
writes of the block’s buffered content, while the
<code>bcache.lock</code> protects information about which blocks are
cached.</p>
<p>If all the buffers are busy, then too many processes are
simultaneously executing file system calls; <code>bget</code> panics. A
more graceful response might be to sleep until a buffer became free,
though there would then be a possibility of deadlock.</p>
<p>Once <code id="bread_2">bread</code> has read the disk (if needed)
and returned the buffer to its caller, the caller has exclusive use of
the buffer and can read or write the data bytes. If the caller does
modify the buffer, it must call <code id="bwrite_2">bwrite</code> to
write the changed data to disk before releasing the buffer.
<code>bwrite</code> <a
href="xv6-riscv-src/kernel/bio.c#L107"><span>(kernel/bio.c:107)</span></a>
calls <code id="virtio_disk_rw_2">virtio_disk_rw</code> to talk to the
disk hardware.</p>
<p>When the caller is done with a buffer, it must call <code
id="brelse_2">brelse</code> to release it. (The name
<code>brelse</code>, a shortening of b-release, is cryptic but worth
learning: it originated in Unix and is used in BSD, Linux, and Solaris
too.) <code>brelse</code> <a
href="xv6-riscv-src/kernel/bio.c#L117"><span>(kernel/bio.c:117)</span></a>
releases the sleep-lock and moves the buffer to the front of the linked
list <a
href="xv6-riscv-src/kernel/bio.c#L128-L133">(kernel/bio.c:128-133)</a>.
Moving the buffer causes the list to be ordered by how recently the
buffers were used (meaning released): the first buffer in the list is
the most recently used, and the last is the least recently used. The two
loops in <code>bget</code> take advantage of this: the scan for an
existing buffer must process the entire list in the worst case, but
checking the most recently used buffers first (starting at
<code>bcache.head</code> and following <code>next</code> pointers) will
reduce scan time when there is good locality of reference. The scan to
pick a buffer to reuse picks the least recently used buffer by scanning
backward (following <code>prev</code> pointers).</p>
</section>
<section id="logging-layer" class="level2" data-number="8.4">
<h2 data-number="8.4"><span class="header-section-number">8.4</span>
Logging layer</h2>
<p>One of the most interesting problems in file system design is crash
recovery. The problem arises because many file-system operations involve
multiple writes to the disk, and a crash after a subset of the writes
may leave the on-disk file system in an inconsistent state. For example,
suppose a crash occurs during file truncation (setting the length of a
file to zero and freeing its content blocks). Depending on the order of
the disk writes, the crash may either leave an inode with a reference to
a content block that is marked free, or it may leave an allocated but
unreferenced content block.</p>
<p>The latter is relatively benign, but an inode that refers to a freed
block is likely to cause serious problems after a reboot. After reboot,
the kernel might allocate that block to another file, and now we have
two different files pointing unintentionally to the same block. If xv6
supported multiple users, this situation could be a security problem,
since the old file’s owner would be able to read and write blocks in the
new file, owned by a different user.</p>
<p>Xv6 solves the problem of crashes during file-system operations with
a simple form of logging. An xv6 system call does not directly write the
on-disk file system data structures. Instead, it places a description of
all the disk writes it wishes to make in a <em><span
id="log_1">log</span></em> on the disk. Once the system call has logged
all of its writes, it writes a special <em><span
id="commit_1">commit</span></em> record to the disk indicating that the
log contains a complete operation. At that point the system call copies
the writes to the on-disk file system data structures. After those
writes have completed, the system call erases the log on disk.</p>
<p>If the system should crash and reboot, the file-system code recovers
from the crash as follows, before running any processes. If the log is
marked as containing a complete operation, then the recovery code copies
the writes to where they belong in the on-disk file system. If the log
is not marked as containing a complete operation, the recovery code
ignores the log. The recovery code finishes by erasing the log.</p>
<p>Why does xv6’s log solve the problem of crashes during file system
operations? If the crash occurs before the operation commits, then the
log on disk will not be marked as complete, the recovery code will
ignore it, and the state of the disk will be as if the operation had not
even started. If the crash occurs after the operation commits, then
recovery will replay all of the operation’s writes, perhaps repeating
them if the operation had started to write them to the on-disk data
structure. In either case, the log makes operations atomic with respect
to crashes: after recovery, either all of the operation’s writes appear
on the disk, or none of them appear.</p>
</section>
<section id="log-design" class="level2" data-number="8.5">
<h2 data-number="8.5"><span class="header-section-number">8.5</span> Log
design</h2>
<p>The log resides at a known fixed location, specified in the
superblock. It consists of a header block followed by a sequence of
updated block copies (“logged blocks”). The header block contains an
array of sector numbers, one for each of the logged blocks, and the
count of log blocks. The count in the header block on disk is either
zero, indicating that there is no transaction in the log, or non-zero,
indicating that the log contains a complete committed transaction with
the indicated number of logged blocks. Xv6 writes the header block when
a transaction commits, but not before, and sets the count to zero after
copying the logged blocks to the file system. Thus a crash midway
through a transaction will result in a count of zero in the log’s header
block; a crash after a commit will result in a non-zero count.</p>
<p>Each system call’s code indicates the start and end of the sequence
of writes that must be atomic with respect to crashes. To allow
concurrent execution of file-system operations by different processes,
the logging system can accumulate the writes of multiple system calls
into one transaction. Thus a single commit may involve the writes of
multiple complete system calls. To avoid splitting a system call across
transactions, the logging system only commits when no file-system system
calls are underway.</p>
<p>The idea of committing several transactions together is known as
<em><span id="group_commit_1">group commit</span></em>. Group commit
reduces the number of disk operations because it amortizes the fixed
cost of a commit over multiple operations. Group commit also hands the
disk system more concurrent writes at the same time, perhaps allowing
the disk to write them all during a single disk rotation. Xv6’s virtio
driver doesn’t support this kind of <em><span
id="batching_1">batching</span></em>, but xv6’s file system design
allows for it.</p>
<p>Xv6 dedicates a fixed amount of space on the disk to hold the log.
The total number of blocks written by the system calls in a transaction
must fit in that space. This has two consequences. No single system call
can be allowed to write more distinct blocks than there is space in the
log. This is not a problem for most system calls, but two of them can
potentially write many blocks: <code id="write_1">write</code> and <code
id="unlink_1">unlink</code>. A large file write may write many data
blocks and many bitmap blocks as well as an inode block; unlinking a
large file might write many bitmap blocks and an inode. Xv6’s write
system call breaks up large writes into multiple smaller writes that fit
in the log, and <code>unlink</code> doesn’t cause problems because in
practice the xv6 file system uses only one bitmap block. The other
consequence of limited log space is that the logging system cannot allow
a system call to start unless it is certain that the system call’s
writes will fit in the space remaining in the log.</p>
</section>
<section id="code-logging" class="level2" data-number="8.6">
<h2 data-number="8.6"><span class="header-section-number">8.6</span>
Code: logging</h2>
<p>A typical use of the log in a system call looks like this:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode C"><code class="sourceCode c"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>  begin_op<span class="op">();</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>  <span class="op">...</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>  bp <span class="op">=</span> bread<span class="op">(...);</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>  bp<span class="op">-&gt;</span>data<span class="op">[...]</span> <span class="op">=</span> <span class="op">...;</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>  log_write<span class="op">(</span>bp<span class="op">);</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>  <span class="op">...</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>  end_op<span class="op">();</span></span></code></pre></div>
<p><code id="begin_op_1">begin_op</code> <a
href="xv6-riscv-src/kernel/log.c#L127"><span>(kernel/log.c:127)</span></a>
waits until the logging system is not currently committing, and until
there is enough unreserved log space to hold the writes from this call.
<code>log.outstanding</code> counts the number of system calls that have
reserved log space; the total reserved space is
<code>log.outstanding</code> times <code>MAXOPBLOCKS</code>.
Incrementing <code>log.outstanding</code> both reserves space and
prevents a commit from occurring during this system call. The code
conservatively assumes that each system call might write up to
<code>MAXOPBLOCKS</code> distinct blocks.</p>
<p><code id="log_write_1">log_write</code> <a
href="xv6-riscv-src/kernel/log.c#L215"><span>(kernel/log.c:215)</span></a>
acts as a proxy for <code id="bwrite_3">bwrite</code>. It records the
block’s sector number in memory, reserving it a slot in the log on disk,
and pins the buffer in the block cache to prevent the block cache from
evicting it. The block must stay in the cache until committed: until
then, the cached copy is the only record of the modification; it cannot
be written to its place on disk until after commit; and other reads in
the same transaction must see the modifications. <code>log_write</code>
notices when a block is written multiple times during a single
transaction, and allocates that block the same slot in the log. This
optimization is often called <em><span
id="absorption_1">absorption</span></em>. It is common that, for
example, the disk block containing inodes of several files is written
several times within a transaction. By absorbing several disk writes
into one, the file system can save log space and can achieve better
performance because only one copy of the disk block must be written to
disk.</p>
<p><code id="end_op_1">end_op</code> <a
href="xv6-riscv-src/kernel/log.c#L147"><span>(kernel/log.c:147)</span></a>
first decrements the count of outstanding system calls. If the count is
now zero, it commits the current transaction by calling
<code>commit().</code> There are four stages in this process.
<code>write_log()</code> <a
href="xv6-riscv-src/kernel/log.c#L179"><span>(kernel/log.c:179)</span></a>
copies each block modified in the transaction from the buffer cache to
its slot in the log on disk. <code>write_head()</code> <a
href="xv6-riscv-src/kernel/log.c#L103"><span>(kernel/log.c:103)</span></a>
writes the header block to disk: this is the commit point, and a crash
after the write will result in recovery replaying the transaction’s
writes from the log. <code id="install_trans_1">install_trans</code> <a
href="xv6-riscv-src/kernel/log.c#L69"><span>(kernel/log.c:69)</span></a>
reads each block from the log and writes it to the proper place in the
file system. Finally <code>end_op</code> writes the log header with a
count of zero; this has to happen before the next transaction starts
writing logged blocks, so that a crash doesn’t result in recovery using
one transaction’s header with the subsequent transaction’s logged
blocks.</p>
<p><code id="recover_from_log_1">recover_from_log</code> <a
href="xv6-riscv-src/kernel/log.c#L117"><span>(kernel/log.c:117)</span></a>
is called from <code id="initlog_1">initlog</code> <a
href="xv6-riscv-src/kernel/log.c#L55"><span>(kernel/log.c:55)</span></a>,
which is called from <code id="fsinit_1">fsinit</code><a
href="xv6-riscv-src/kernel/fs.c#L42"><span>(kernel/fs.c:42)</span></a>
during boot before the first user process runs <a
href="xv6-riscv-src/kernel/proc.c#L527"><span>(kernel/proc.c:527)</span></a>.
It reads the log header, and mimics the actions of <code>end_op</code>
if the header indicates that the log contains a committed
transaction.</p>
<p>An example use of the log occurs in <code
id="filewrite_1">filewrite</code> <a
href="xv6-riscv-src/kernel/file.c#L135"><span>(kernel/file.c:135)</span></a>.
The transaction looks like this:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode C"><code class="sourceCode c"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>      begin_op<span class="op">();</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>      ilock<span class="op">(</span>f<span class="op">-&gt;</span>ip<span class="op">);</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>      r <span class="op">=</span> writei<span class="op">(</span>f<span class="op">-&gt;</span>ip<span class="op">,</span> <span class="op">...);</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>      iunlock<span class="op">(</span>f<span class="op">-&gt;</span>ip<span class="op">);</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>      end_op<span class="op">();</span></span></code></pre></div>
<p>This code is wrapped in a loop that breaks up large writes into
individual transactions of just a few sectors at a time, to avoid
overflowing the log. The call to <code id="writei_1">writei</code>
writes many blocks as part of this transaction: the file’s inode, one or
more bitmap blocks, and some data blocks.</p>
</section>
<section id="code-block-allocator" class="level2" data-number="8.7">
<h2 data-number="8.7"><span class="header-section-number">8.7</span>
Code: Block allocator</h2>
<p>File and directory content is stored in disk blocks, which must be
allocated from a free pool. Xv6’s block allocator maintains a free
bitmap on disk, with one bit per block. A zero bit indicates that the
corresponding block is free; a one bit indicates that it is in use. The
program <code>mkfs</code> sets the bits corresponding to the boot
sector, superblock, log blocks, inode blocks, and bitmap blocks.</p>
<p>The block allocator provides two functions: <code
id="balloc_1">balloc</code> allocates a new disk block, and <code
id="bfree_1">bfree</code> frees a block. <code>balloc</code> The loop in
<code>balloc</code> at <a
href="xv6-riscv-src/kernel/fs.c#L72"><span>(kernel/fs.c:72)</span></a>
considers every block, starting at block 0 up to <code>sb.size</code>,
the number of blocks in the file system. It looks for a block whose
bitmap bit is zero, indicating that it is free. If <code>balloc</code>
finds such a block, it updates the bitmap and returns the block. For
efficiency, the loop is split into two pieces. The outer loop reads each
block of bitmap bits. The inner loop checks all Bits-Per-Block
(<code>BPB</code>) bits in a single bitmap block. The race that might
occur if two processes try to allocate a block at the same time is
prevented by the fact that the buffer cache only lets one process use
any one bitmap block at a time.</p>
<p><code>bfree</code> <a
href="xv6-riscv-src/kernel/fs.c#L92"><span>(kernel/fs.c:92)</span></a>
finds the right bitmap block and clears the right bit. Again the
exclusive use implied by <code>bread</code> and <code>brelse</code>
avoids the need for explicit locking.</p>
<p>As with much of the code described in the remainder of this chapter,
<code>balloc</code> and <code>bfree</code> must be called inside a
transaction.</p>
</section>
<section id="inode-layer" class="level2" data-number="8.8">
<h2 data-number="8.8"><span class="header-section-number">8.8</span>
Inode layer</h2>
<p>The term <em><span id="inode_3">inode</span></em> can have one of two
related meanings. It might refer to the on-disk data structure
containing a file’s size and list of data block numbers. Or “inode”
might refer to an in-memory inode, which contains a copy of the on-disk
inode as well as extra information needed within the kernel.</p>
<p>The on-disk inodes are packed into a contiguous area of disk called
the inode blocks. Every inode is the same size, so it is easy, given a
number n, to find the nth inode on the disk. In fact, this number n,
called the inode number or i-number, is how inodes are identified in the
implementation.</p>
<p>The on-disk inode is defined by a <code
id="struct_dinode_1">struct dinode</code> <a
href="xv6-riscv-src/kernel/fs.h#L32"><span>(kernel/fs.h:32)</span></a>.
The <code>type</code> field distinguishes between files, directories,
and special files (devices). A type of zero indicates that an on-disk
inode is free. The <code>nlink</code> field counts the number of
directory entries that refer to this inode, in order to recognize when
the on-disk inode and its data blocks should be freed. The
<code>size</code> field records the number of bytes of content in the
file. The <code>addrs</code> array records the block numbers of the disk
blocks holding the file’s content.</p>
<p>The kernel keeps the set of active inodes in memory in a table called
<code id="itable_1">itable</code>; <code
id="struct_inode_1">struct inode</code> <a
href="xv6-riscv-src/kernel/file.h#L17"><span>(kernel/file.h:17)</span></a>
is the in-memory copy of a <code>struct</code> <code>dinode</code> on
disk. The kernel stores an inode in memory only if there are C pointers
referring to that inode. The <code>ref</code> field counts the number of
C pointers referring to the in-memory inode, and the kernel discards the
inode from memory if the reference count drops to zero. The <code
id="iget_1">iget</code> and <code id="iput_1">iput</code> functions
acquire and release pointers to an inode, modifying the reference count.
Pointers to an inode can come from file descriptors, current working
directories, and transient kernel code such as <code>exec</code>.</p>
<p>There are four lock or lock-like mechanisms in xv6’s inode code.
<code>itable.lock</code> protects the invariant that an inode is present
in the inode table at most once, and the invariant that an in-memory
inode’s <code>ref</code> field counts the number of in-memory pointers
to the inode. Each in-memory inode has a <code>lock</code> field
containing a sleep-lock, which ensures exclusive access to the inode’s
fields (such as file length) as well as to the inode’s file or directory
content blocks. An inode’s <code>ref</code>, if it is greater than zero,
causes the system to maintain the inode in the table, and not re-use the
table entry for a different inode. Finally, each inode contains a
<code>nlink</code> field (on disk and copied in memory if in memory)
that counts the number of directory entries that refer to a file; xv6
won’t free an inode if its link count is greater than zero.</p>
<p>A <code>struct</code> <code>inode</code> pointer returned by
<code>iget()</code> is guaranteed to be valid until the corresponding
call to <code>iput()</code>; the inode won’t be deleted, and the memory
referred to by the pointer won’t be re-used for a different inode.
<code>iget()</code> provides non-exclusive access to an inode, so that
there can be many pointers to the same inode. Many parts of the
file-system code depend on this behavior of <code>iget()</code>, both to
hold long-term references to inodes (as open files and current
directories) and to prevent races while avoiding deadlock in code that
manipulates multiple inodes (such as pathname lookup).</p>
<p>The <code>struct</code> <code>inode</code> that <code>iget</code>
returns may not have any useful content. In order to ensure it holds a
copy of the on-disk inode, code must call <code
id="ilock_1">ilock</code>. This locks the inode (so that no other
process can <code>ilock</code> it) and reads the inode from the disk, if
it has not already been read. <code>iunlock</code> releases the lock on
the inode. Separating acquisition of inode pointers from locking helps
avoid deadlock in some situations, for example during directory lookup.
Multiple processes can hold a C pointer to an inode returned by
<code>iget</code>, but only one process can lock the inode at a
time.</p>
<p>The inode table only stores inodes to which kernel code or data
structures hold C pointers. Its main job is synchronizing access by
multiple processes. The inode table also happens to cache
frequently-used inodes, but caching is secondary; if an inode is used
frequently, the buffer cache will probably keep it in memory. Code that
modifies an in-memory inode writes it to disk with
<code>iupdate</code>.</p>
</section>
<section id="code-inodes" class="level2" data-number="8.9">
<h2 data-number="8.9"><span class="header-section-number">8.9</span>
Code: Inodes</h2>
<p>To allocate a new inode (for example, when creating a file), xv6
calls <code id="ialloc_1">ialloc</code> <a
href="xv6-riscv-src/kernel/fs.c#L199"><span>(kernel/fs.c:199)</span></a>.
<code>ialloc</code> is similar to <code id="balloc_2">balloc</code>: it
loops over the inode structures on the disk, one block at a time,
looking for one that is marked free. When it finds one, it claims it by
writing the new <code>type</code> to the disk and then returns an entry
from the inode table with the tail call to <code id="iget_2">iget</code>
<a
href="xv6-riscv-src/kernel/fs.c#L213"><span>(kernel/fs.c:213)</span></a>.
The correct operation of <code>ialloc</code> depends on the fact that
only one process at a time can be holding a reference to
<code>bp</code>: <code>ialloc</code> can be sure that some other process
does not simultaneously see that the inode is available and try to claim
it.</p>
<p><code>iget</code> <a
href="xv6-riscv-src/kernel/fs.c#L247"><span>(kernel/fs.c:247)</span></a>
looks through the inode table for an active entry
(<code>ip-&gt;ref</code> <code>&gt;</code> <code>0</code>) with the
desired device and inode number. If it finds one, it returns a new
reference to that inode <a
href="xv6-riscv-src/kernel/fs.c#L256-L260">(kernel/fs.c:256-260)</a>. As
<code id="iget_3">iget</code> scans, it records the position of the
first empty slot <a
href="xv6-riscv-src/kernel/fs.c#L261-L262">(kernel/fs.c:261-262)</a>,
which it uses if it needs to allocate a table entry.</p>
<p>Code must lock the inode using <code id="ilock_2">ilock</code> before
reading or writing its metadata or content. <code>ilock</code> <a
href="xv6-riscv-src/kernel/fs.c#L293"><span>(kernel/fs.c:293)</span></a>
uses a sleep-lock for this purpose. Once <code id="ilock_3">ilock</code>
has exclusive access to the inode, it reads the inode from disk (more
likely, the buffer cache) if needed. The function <code
id="iunlock_1">iunlock</code> <a
href="xv6-riscv-src/kernel/fs.c#L321"><span>(kernel/fs.c:321)</span></a>
releases the sleep-lock, which may cause any processes sleeping to be
woken up.</p>
<p><code>iput</code> <a
href="xv6-riscv-src/kernel/fs.c#L337"><span>(kernel/fs.c:337)</span></a>
releases a C pointer to an inode by decrementing the reference count <a
href="xv6-riscv-src/kernel/fs.c#L360"><span>(kernel/fs.c:360)</span></a>.
If this is the last reference, the inode’s slot in the inode table is
now free and can be re-used for a different inode.</p>
<p>If <code id="iput_2">iput</code> sees that there are no C pointer
references to an inode and that the inode has no links to it (occurs in
no directory), then the inode and its data blocks must be freed.
<code>iput</code> calls <code id="itrunc_1">itrunc</code> to truncate
the file to zero bytes, freeing the data blocks; sets the inode type to
0 (unallocated); and writes the inode to disk <a
href="xv6-riscv-src/kernel/fs.c#L342"><span>(kernel/fs.c:342)</span></a>.</p>
<p>The locking protocol in <code id="iput_3">iput</code> in the case in
which it frees the inode deserves a closer look. One danger is that a
concurrent thread might be waiting in <code>ilock</code> to use this
inode (e.g., to read a file or list a directory), and won’t be prepared
to find that the inode is no longer allocated. This can’t happen because
there is no way for a system call to get a pointer to an in-memory inode
if it has no links to it and <code>ip-&gt;ref</code> is one. That one
reference is the reference owned by the thread calling
<code>iput</code>. It’s true that <code>iput</code> checks that the
reference count is one outside of its <code>itable.lock</code> critical
section, but at that point the link count is known to be zero, so no
thread will try to acquire a new reference. The other main danger is
that a concurrent call to <code>ialloc</code> might choose the same
inode that <code>iput</code> is freeing. This can happen only after the
<code>iupdate</code> writes the disk so that the inode has type zero.
This race is benign; the allocating thread will politely wait to acquire
the inode’s sleep-lock before reading or writing the inode, at which
point <code>iput</code> is done with it.</p>
<p><code>iput()</code> can write to the disk. This means that any system
call that uses the file system may write to the disk, because the system
call may be the last one having a reference to the file. Even calls like
<code>read()</code> that appear to be read-only, may end up calling
<code>iput().</code> This, in turn, means that even read-only system
calls must be wrapped in transactions if they use the file system.</p>
<p>There is a challenging interaction between <code>iput()</code> and
crashes. <code>iput()</code> doesn’t truncate a file immediately when
the link count for the file drops to zero, because some process might
still hold a reference to the inode in memory: a process might still be
reading and writing to the file, because it successfully opened it. But,
if a crash happens before the last process closes the file descriptor
for the file, then the file will be marked allocated on disk but no
directory entry will point to it.</p>
<p>File systems handle this case in one of two ways. The simple solution
is that on recovery, after reboot, the file system scans the whole file
system for files that are marked allocated, but have no directory entry
pointing to them. If any such file exists, then it can free those
files.</p>
<p>The second solution doesn’t require scanning the file system. In this
solution, the file system records on disk (e.g., in the super block) the
inode inumber of a file whose link count drops to zero but whose
reference count isn’t zero. If the file system removes the file when its
reference count reaches 0, then it updates the on-disk list by removing
that inode from the list. On recovery, the file system frees any file in
the list.</p>
<p>Xv6 implements neither solution, which means that inodes may be
marked allocated on disk, even though they are not in use anymore. This
means that over time xv6 runs the risk that it may run out of disk
space.</p>
</section>
<section id="code-inode-content" class="level2" data-number="8.10">
<h2 data-number="8.10"><span class="header-section-number">8.10</span>
Code: Inode content</h2>
<figure id="fig:inode">
<img src="fig/inode.svg" />
<figcaption>The representation of a file on disk.</figcaption>
</figure>
<p>The on-disk inode structure, <code
id="struct_dinode_2">struct dinode</code>, contains a size and an array
of block numbers (see Figure <a href="#fig:inode"
data-reference-type="ref" data-reference="fig:inode">8.3</a>). The inode
data is found in the blocks listed in the <code>dinode</code> ’s
<code>addrs</code> array. The first <code id="NDIRECT_1">NDIRECT</code>
blocks of data are listed in the first <code>NDIRECT</code> entries in
the array; these blocks are called <em><span id="direct_blocks_1">direct
blocks</span></em>. The next <code id="NINDIRECT_1">NINDIRECT</code>
blocks of data are listed not in the inode but in a data block called
the <em><span id="indirect_block_1">indirect block</span></em>. The last
entry in the <code>addrs</code> array gives the address of the indirect
block. Thus the first 12 kB ( <code>NDIRECT</code> <code>x</code> <code
id="BSIZE_1">BSIZE</code>) bytes of a file can be loaded from blocks
listed in the inode, while the next <code>256</code> kB (
<code>NINDIRECT</code> <code>x</code> <code>BSIZE</code>) bytes can only
be loaded after consulting the indirect block. This is a good on-disk
representation but a complex one for clients. The function <code
id="bmap_1">bmap</code> manages the representation so that higher-level
routines, such as <code id="readi_2">readi</code> and <code
id="writei_2">writei</code>, which we will see shortly, do not need to
manage this complexity. <code>bmap</code> returns the disk block number
of the <code>bn</code>’th data block for the inode <code>ip</code>. If
<code>ip</code> does not have such a block yet, <code>bmap</code>
allocates one.</p>
<p>The function <code id="bmap_2">bmap</code> <a
href="xv6-riscv-src/kernel/fs.c#L383"><span>(kernel/fs.c:383)</span></a>
begins by picking off the easy case: the first <code
id="NDIRECT_2">NDIRECT</code> blocks are listed in the inode itself <a
href="xv6-riscv-src/kernel/fs.c#L388-L396">(kernel/fs.c:388-396)</a>.
The next <code id="NINDIRECT_2">NINDIRECT</code> blocks are listed in
the indirect block at <code>ip-&gt;addrs[NDIRECT]</code>.
<code>bmap</code> reads the indirect block <a
href="xv6-riscv-src/kernel/fs.c#L407"><span>(kernel/fs.c:407)</span></a>
and then reads a block number from the right position within the block
<a
href="xv6-riscv-src/kernel/fs.c#L408"><span>(kernel/fs.c:408)</span></a>.
If the block number exceeds <code>NDIRECT+NINDIRECT</code>,
<code>bmap</code> panics; <code>writei</code> contains the check that
prevents this from happening <a
href="xv6-riscv-src/kernel/fs.c#L513"><span>(kernel/fs.c:513)</span></a>.</p>
<p><code>bmap</code> allocates blocks as needed. An
<code>ip-&gt;addrs[]</code> or indirect entry of zero indicates that no
block is allocated. As <code>bmap</code> encounters zeros, it replaces
them with the numbers of fresh blocks, allocated on demand <a
href="xv6-riscv-src/kernel/fs.c#L389-L390">(kernel/fs.c:389-390)</a> <a
href="xv6-riscv-src/kernel/fs.c#L401-L402">(kernel/fs.c:401-402)</a>.</p>
<p><code id="itrunc_2">itrunc</code> frees a file’s blocks, resetting
the inode’s size to zero. <code>itrunc</code> <a
href="xv6-riscv-src/kernel/fs.c#L426"><span>(kernel/fs.c:426)</span></a>
starts by freeing the direct blocks <a
href="xv6-riscv-src/kernel/fs.c#L432-L437">(kernel/fs.c:432-437)</a>,
then the ones listed in the indirect block <a
href="xv6-riscv-src/kernel/fs.c#L442-L445">(kernel/fs.c:442-445)</a>,
and finally the indirect block itself <a
href="xv6-riscv-src/kernel/fs.c#L447-L448">(kernel/fs.c:447-448)</a>.</p>
<p><code>bmap</code> makes it easy for <code id="readi_3">readi</code>
and <code id="writei_3">writei</code> to get at an inode’s data.
<code>readi</code> <a
href="xv6-riscv-src/kernel/fs.c#L472"><span>(kernel/fs.c:472)</span></a>
starts by making sure that the offset and count are not beyond the end
of the file. Reads that start beyond the end of the file return an error
<a href="xv6-riscv-src/kernel/fs.c#L477-L478">(kernel/fs.c:477-478)</a>
while reads that start at or cross the end of the file return fewer
bytes than requested <a
href="xv6-riscv-src/kernel/fs.c#L479-L480">(kernel/fs.c:479-480)</a>.
The main loop processes each block of the file, copying data from the
buffer into <code>dst</code> <a
href="xv6-riscv-src/kernel/fs.c#L482-L494">(kernel/fs.c:482-494)</a>.
<code id="writei_4">writei</code> <a
href="xv6-riscv-src/kernel/fs.c#L506"><span>(kernel/fs.c:506)</span></a>
is identical to <code id="readi_4">readi</code>, with three exceptions:
writes that start at or cross the end of the file grow the file, up to
the maximum file size <a
href="xv6-riscv-src/kernel/fs.c#L513-L514">(kernel/fs.c:513-514)</a>;
the loop copies data into the buffers instead of out <a
href="xv6-riscv-src/kernel/fs.c#L36"><span>(kernel/fs.c:36)</span></a>;
and if the write has extended the file, <code
id="writei_5">writei</code> must update its size</p>
<p>The function <code id="stati_1">stati</code> <a
href="xv6-riscv-src/kernel/fs.c#L458"><span>(kernel/fs.c:458)</span></a>
copies inode metadata into the <code>stat</code> structure, which is
exposed to user programs via the <code id="stat_1">stat</code> system
call.</p>
</section>
<section id="code-directory-layer" class="level2" data-number="8.11">
<h2 data-number="8.11"><span class="header-section-number">8.11</span>
Code: directory layer</h2>
<p>A directory is implemented internally much like a file. Its inode has
type <code id="T_DIR_1">T_DIR</code> and its data is a sequence of
directory entries. Each entry is a <code
id="struct_dirent_1">struct dirent</code> <a
href="xv6-riscv-src/kernel/fs.h#L56"><span>(kernel/fs.h:56)</span></a>,
which contains a name and an inode number. The name is at most <code
id="DIRSIZ_1">DIRSIZ</code> (14) characters; if shorter, it is
terminated by a NULL (0) byte. Directory entries with inode number zero
are free.</p>
<p>The function <code id="dirlookup_1">dirlookup</code> <a
href="xv6-riscv-src/kernel/fs.c#L552"><span>(kernel/fs.c:552)</span></a>
searches a directory for an entry with the given name. If it finds one,
it returns a pointer to the corresponding inode, unlocked, and sets
<code>*poff</code> to the byte offset of the entry within the directory,
in case the caller wishes to edit it. If <code>dirlookup</code> finds an
entry with the right name, it updates <code>*poff</code> and returns an
unlocked inode obtained via <code id="iget_4">iget</code>.
<code>dirlookup</code> is the reason that <code>iget</code> returns
unlocked inodes. The caller has locked <code>dp</code>, so if the lookup
was for <code id="._1">.</code>, an alias for the current directory,
attempting to lock the inode before returning would try to re-lock
<code>dp</code> and deadlock. (There are more complicated deadlock
scenarios involving multiple processes and <code id=".._1">..</code>, an
alias for the parent directory; <code>.</code> is not the only problem.)
The caller can unlock <code>dp</code> and then lock <code>ip</code>,
ensuring that it only holds one lock at a time.</p>
<p>The function <code id="dirlink_1">dirlink</code> <a
href="xv6-riscv-src/kernel/fs.c#L580"><span>(kernel/fs.c:580)</span></a>
writes a new directory entry with the given name and inode number into
the directory <code>dp</code>. If the name already exists,
<code>dirlink</code> returns an error <a
href="xv6-riscv-src/kernel/fs.c#L586-L590">(kernel/fs.c:586-590)</a>.
The main loop reads directory entries looking for an unallocated entry.
When it finds one, it stops the loop early <a
href="xv6-riscv-src/kernel/fs.c#L563-L564">(kernel/fs.c:563-564)</a>,
with <code>off</code> set to the offset of the available entry.
Otherwise, the loop ends with <code>off</code> set to
<code>dp-&gt;size</code>. Either way, <code>dirlink</code> then adds a
new entry to the directory by writing at offset <code>off</code></p>
</section>
<section id="code-path-names" class="level2" data-number="8.12">
<h2 data-number="8.12"><span class="header-section-number">8.12</span>
Code: Path names</h2>
<p>Path name lookup involves a succession of calls to <code
id="dirlookup_2">dirlookup</code>, one for each path component.
<code>namei</code> <a
href="xv6-riscv-src/kernel/fs.c#L687"><span>(kernel/fs.c:687)</span></a>
evaluates <code>path</code> and returns the corresponding
<code>inode</code>. The function <code
id="nameiparent_1">nameiparent</code> is a variant: it stops before the
last element, returning the inode of the parent directory and copying
the final element into <code>name</code>. Both call the generalized
function <code id="namex_1">namex</code> to do the real work.</p>
<p><code>namex</code> <a
href="xv6-riscv-src/kernel/fs.c#L652"><span>(kernel/fs.c:652)</span></a>
starts by deciding where the path evaluation begins. If the path begins
with a slash, evaluation begins at the root; otherwise, the current
directory <a
href="xv6-riscv-src/kernel/fs.c#L656-L659">(kernel/fs.c:656-659)</a>.
Then it uses <code id="skipelem_1">skipelem</code> to consider each
element of the path in turn <a
href="xv6-riscv-src/kernel/fs.c#L661"><span>(kernel/fs.c:661)</span></a>.
Each iteration of the loop must look up <code>name</code> in the current
inode <code>ip</code>. The iteration begins by locking <code>ip</code>
and checking that it is a directory. If not, the lookup fails <a
href="xv6-riscv-src/kernel/fs.c#L662-L666">(kernel/fs.c:662-666)</a>.
(Locking <code>ip</code> is necessary not because
<code>ip-&gt;type</code> can change underfoot—it can’t—but because until
<code id="ilock_4">ilock</code> runs, <code>ip-&gt;type</code> is not
guaranteed to have been loaded from disk.) If the call is <code
id="nameiparent_2">nameiparent</code> and this is the last path element,
the loop stops early, as per the definition of <code>nameiparent</code>;
the final path element has already been copied into <code>name</code>,
so <code id="namex_2">namex</code> need only return the unlocked
<code>ip</code> <a
href="xv6-riscv-src/kernel/fs.c#L667-L671">(kernel/fs.c:667-671)</a>.
Finally, the loop looks for the path element using <code
id="dirlookup_3">dirlookup</code> and prepares for the next iteration by
setting <code>ip = next</code> <a
href="xv6-riscv-src/kernel/fs.c#L672-L677">(kernel/fs.c:672-677)</a>.
When the loop runs out of path elements, it returns <code>ip</code>.</p>
<p>The procedure <code>namex</code> may take a long time to complete: it
could involve several disk operations to read inodes and directory
blocks for the directories traversed in the pathname (if they are not in
the buffer cache). Xv6 is carefully designed so that if an invocation of
<code>namex</code> by one kernel thread is blocked on a disk I/O,
another kernel thread looking up a different pathname can proceed
concurrently. <code>namex</code> locks each directory in the path
separately so that lookups in different directories can proceed in
parallel.</p>
<p>This concurrency introduces some challenges. For example, while one
kernel thread is looking up a pathname another kernel thread may be
changing the directory tree by unlinking a directory. A potential risk
is that a lookup may be searching a directory that has been deleted by
another kernel thread and its blocks have been re-used for another
directory or file.</p>
<p>Xv6 avoids such races. For example, when executing
<code>dirlookup</code> in <code>namex</code>, the lookup thread holds
the lock on the directory and <code>dirlookup</code> returns an inode
that was obtained using <code>iget</code>. <code>iget</code> increases
the reference count of the inode. Only after receiving the inode from
<code>dirlookup</code> does <code>namex</code> release the lock on the
directory. Now another thread may unlink the inode from the directory
but xv6 will not delete the inode yet, because the reference count of
the inode is still larger than zero.</p>
<p>Another risk is deadlock. For example, <code>next</code> points to
the same inode as <code>ip</code> when looking up ".". Locking
<code>next</code> before releasing the lock on <code>ip</code> would
result in a deadlock. To avoid this deadlock, <code>namex</code> unlocks
the directory before obtaining a lock on <code>next</code>. Here again
we see why the separation between <code>iget</code> and
<code>ilock</code> is important.</p>
</section>
<section id="file-descriptor-layer" class="level2" data-number="8.13">
<h2 data-number="8.13"><span class="header-section-number">8.13</span>
File descriptor layer</h2>
<p>A cool aspect of the Unix interface is that most resources in Unix
are represented as files, including devices such as the console, pipes,
and of course, real files. The file descriptor layer is the layer that
achieves this uniformity.</p>
<p>Xv6 gives each process its own table of open files, or file
descriptors, as we saw in Chapter <a href="#CH:UNIX"
data-reference-type="ref" data-reference="CH:UNIX">1</a>. Each open file
is represented by a <code id="struct_file_1">struct file</code> <a
href="xv6-riscv-src/kernel/file.h#L1"><span>(kernel/file.h:1)</span></a>,
which is a wrapper around either an inode or a pipe, plus an I/O offset.
Each call to <code id="open_1">open</code> creates a new open file (a
new <code>struct</code> <code>file</code>): if multiple processes open
the same file independently, the different instances will have different
I/O offsets. On the other hand, a single open file (the same
<code>struct</code> <code>file</code>) can appear multiple times in one
process’s file table and also in the file tables of multiple processes.
This would happen if one process used <code>open</code> to open the file
and then created aliases using <code id="dup_1">dup</code> or shared it
with a child using <code id="fork_5">fork</code>. A reference count
tracks the number of references to a particular open file. A file can be
open for reading or writing or both. The <code>readable</code> and
<code>writable</code> fields track this.</p>
<p>All the open files in the system are kept in a global file table, the
<code id="ftable_1">ftable</code>. The file table has functions to
allocate a file (<code id="filealloc_1">filealloc</code>), create a
duplicate reference (<code id="filedup_1">filedup</code>), release a
reference (<code id="fileclose_1">fileclose</code>), and read and write
data (<code id="fileread_1">fileread</code> and <code
id="filewrite_2">filewrite</code>).</p>
<p>The first three follow the now-familiar form. <code>filealloc</code>
<a
href="xv6-riscv-src/kernel/file.c#L30"><span>(kernel/file.c:30)</span></a>
scans the file table for an unreferenced file (<code>f-&gt;ref</code>
<code>==</code> <code>0</code>) and returns a new reference; <code
id="filedup_2">filedup</code> <a
href="xv6-riscv-src/kernel/file.c#L48"><span>(kernel/file.c:48)</span></a>
increments the reference count; and <code
id="fileclose_2">fileclose</code> <a
href="xv6-riscv-src/kernel/file.c#L60"><span>(kernel/file.c:60)</span></a>
decrements it. When a file’s reference count reaches zero,
<code>fileclose</code> releases the underlying pipe or inode, according
to the type.</p>
<p>The functions <code id="filestat_1">filestat</code>, <code
id="fileread_2">fileread</code>, and <code
id="filewrite_3">filewrite</code> implement the <code
id="stat_2">stat</code>, <code id="read_1">read</code>, and <code
id="write_2">write</code> operations on files. <code>filestat</code> <a
href="xv6-riscv-src/kernel/file.c#L88"><span>(kernel/file.c:88)</span></a>
is only allowed on inodes and calls <code id="stati_2">stati</code>.
<code>fileread</code> and <code>filewrite</code> check that the
operation is allowed by the open mode and then pass the call through to
either the pipe or inode implementation. If the file represents an
inode, <code>fileread</code> and <code>filewrite</code> use the I/O
offset as the offset for the operation and then advance it <a
href="xv6-riscv-src/kernel/file.c#L122-L123">(kernel/file.c:122-123)</a>
<a
href="xv6-riscv-src/kernel/file.c#L153-L154">(kernel/file.c:153-154)</a>.
Pipes have no concept of offset. Recall that the inode functions require
the caller to handle locking <a
href="xv6-riscv-src/kernel/file.c#L94-L96">(kernel/file.c:94-96)</a> <a
href="xv6-riscv-src/kernel/file.c#L121-L124">(kernel/file.c:121-124)</a>
<a
href="xv6-riscv-src/kernel/file.c#L163-L166">(kernel/file.c:163-166)</a>.
The inode locking has the convenient side effect that the read and write
offsets are updated atomically, so that multiple writing to the same
file simultaneously cannot overwrite each other’s data, though their
writes may end up interlaced.</p>
</section>
<section id="code-system-calls" class="level2" data-number="8.14">
<h2 data-number="8.14"><span class="header-section-number">8.14</span>
Code: System calls</h2>
<p>With the functions that the lower layers provide, the implementation
of most system calls is trivial (see <a
href="xv6-riscv-src/kernel/sysfile.c"><span>(kernel/sysfile.c)</span></a>).
There are a few calls that deserve a closer look.</p>
<p>The functions <code id="sys_link_1">sys_link</code> and <code
id="sys_unlink_1">sys_unlink</code> edit directories, creating or
removing references to inodes. They are another good example of the
power of using transactions. <code>sys_link</code> <a
href="xv6-riscv-src/kernel/sysfile.c#L124"><span>(kernel/sysfile.c:124)</span></a>
begins by fetching its arguments, two strings <code>old</code> and
<code>new</code> <a
href="xv6-riscv-src/kernel/sysfile.c#L129"><span>(kernel/sysfile.c:129)</span></a>.
Assuming <code>old</code> exists and is not a directory <a
href="xv6-riscv-src/kernel/sysfile.c#L133-L136">(kernel/sysfile.c:133-136)</a>,
<code>sys_link</code> increments its <code>ip-&gt;nlink</code> count.
Then <code>sys_link</code> calls <code
id="nameiparent_3">nameiparent</code> to find the parent directory and
final path element of <code>new</code> <a
href="xv6-riscv-src/kernel/sysfile.c#L149"><span>(kernel/sysfile.c:149)</span></a>
and creates a new directory entry pointing at <code>old</code> ’s inode
<a
href="xv6-riscv-src/kernel/sysfile.c#L152"><span>(kernel/sysfile.c:152)</span></a>.
The new parent directory must exist and be on the same device as the
existing inode: inode numbers only have a unique meaning on a single
disk. If an error like this occurs, <code
id="sys_link_2">sys_link</code> must go back and decrement
<code>ip-&gt;nlink</code>.</p>
<p>Transactions simplify the implementation because it requires updating
multiple disk blocks, but we don’t have to worry about the order in
which we do them. They either will all succeed or none. For example,
without transactions, updating <code>ip-&gt;nlink</code> before creating
a link, would put the file system temporarily in an unsafe state, and a
crash in between could result in havoc. With transactions we don’t have
to worry about this.</p>
<p><code>sys_link</code> creates a new name for an existing inode. The
function <code id="create_1">create</code> <a
href="xv6-riscv-src/kernel/sysfile.c#L246"><span>(kernel/sysfile.c:246)</span></a>
creates a new name for a new inode. It is a generalization of the three
file creation system calls: <code id="open_2">open</code> with the <code
id="O_CREATE_1">O_CREATE</code> flag makes a new ordinary file, <code
id="mkdir_1">mkdir</code> makes a new directory, and <code
id="mkdev_1">mkdev</code> makes a new device file. Like <code
id="sys_link_3">sys_link</code>, <code id="create_2">create</code>
starts by calling <code id="nameiparent_4">nameiparent</code> to get the
inode of the parent directory. It then calls <code
id="dirlookup_4">dirlookup</code> to check whether the name already
exists <a
href="xv6-riscv-src/kernel/sysfile.c#L256"><span>(kernel/sysfile.c:256)</span></a>.
If the name does exist, <code>create</code>’s behavior depends on which
system call it is being used for: <code>open</code> has different
semantics from <code id="mkdir_2">mkdir</code> and <code
id="mkdev_2">mkdev</code>. If <code>create</code> is being used on
behalf of <code>open</code> (<code>type</code> <code>==</code> <code
id="T_FILE_1">T_FILE</code>) and the name that exists is itself a
regular file, then <code>open</code> treats that as a success, so
<code>create</code> does too <a
href="xv6-riscv-src/kernel/sysfile.c#L260"><span>(kernel/sysfile.c:260)</span></a>.
Otherwise, it is an error <a
href="xv6-riscv-src/kernel/sysfile.c#L261-L262">(kernel/sysfile.c:261-262)</a>.
If the name does not already exist, <code>create</code> now allocates a
new inode with <code id="ialloc_2">ialloc</code> <a
href="xv6-riscv-src/kernel/sysfile.c#L265"><span>(kernel/sysfile.c:265)</span></a>.
If the new inode is a directory, <code>create</code> initializes it with
<code id="._2">.</code> and <code id=".._2">..</code> entries. Finally,
now that the data is initialized properly, <code
id="create_3">create</code> can link it into the parent directory <a
href="xv6-riscv-src/kernel/sysfile.c#L278"><span>(kernel/sysfile.c:278)</span></a>.
<code>create</code>, like <code id="sys_link_4">sys_link</code>, holds
two inode locks simultaneously: <code>ip</code> and <code>dp</code>.
There is no possibility of deadlock because the inode <code>ip</code> is
freshly allocated: no other process in the system will hold
<code>ip</code> ’s lock and then try to lock <code>dp</code>.</p>
<p>Using <code>create</code>, it is easy to implement <code
id="sys_open_1">sys_open</code>, <code
id="sys_mkdir_1">sys_mkdir</code>, and <code
id="sys_mknod_1">sys_mknod</code>. <code>sys_open</code> <a
href="xv6-riscv-src/kernel/sysfile.c#L305"><span>(kernel/sysfile.c:305)</span></a>
is the most complex, because creating a new file is only a small part of
what it can do. If <code id="open_3">open</code> is passed the <code
id="O_CREATE_2">O_CREATE</code> flag, it calls <code>create</code> <a
href="xv6-riscv-src/kernel/sysfile.c#L320"><span>(kernel/sysfile.c:320)</span></a>.
Otherwise, it calls <code id="namei_2">namei</code> <a
href="xv6-riscv-src/kernel/sysfile.c#L326"><span>(kernel/sysfile.c:326)</span></a>.
<code>create</code> returns a locked inode, but <code>namei</code> does
not, so <code id="sys_open_2">sys_open</code> must lock the inode
itself. This provides a convenient place to check that directories are
only opened for reading, not writing. Assuming the inode was obtained
one way or the other, <code>sys_open</code> allocates a file and a file
descriptor <a
href="xv6-riscv-src/kernel/sysfile.c#L344"><span>(kernel/sysfile.c:344)</span></a>
and then fills in the file <a
href="xv6-riscv-src/kernel/sysfile.c#L356-L361">(kernel/sysfile.c:356-361)</a>.
Note that no other process can access the partially initialized file
since it is only in the current process’s table.</p>
<p>Chapter <a href="#CH:SCHED" data-reference-type="ref"
data-reference="CH:SCHED">7</a> examined the implementation of pipes
before we even had a file system. The function <code
id="sys_pipe_1">sys_pipe</code> connects that implementation to the file
system by providing a way to create a pipe pair. Its argument is a
pointer to space for two integers, where it will record the two new file
descriptors. Then it allocates the pipe and installs the file
descriptors.</p>
</section>
<section id="real-world-7" class="level2" data-number="8.15">
<h2 data-number="8.15"><span class="header-section-number">8.15</span>
Real world</h2>
<p>The buffer cache in a real-world operating system is significantly
more complex than xv6’s, but it serves the same two purposes: caching
and synchronizing access to the disk. Xv6’s buffer cache, like V6’s,
uses a simple least recently used (LRU) eviction policy; there are many
more complex policies that can be implemented, each good for some
workloads and not as good for others. A more efficient LRU cache would
eliminate the linked list, instead using a hash table for lookups and a
heap for LRU evictions. Modern buffer caches are typically integrated
with the virtual memory system to support memory-mapped files.</p>
<p>Xv6’s logging system is inefficient. A commit cannot occur
concurrently with file-system system calls. The system logs entire
blocks, even if only a few bytes in a block are changed. It performs
synchronous log writes, a block at a time, each of which is likely to
require an entire disk rotation time. Real logging systems address all
of these problems.</p>
<p>Logging is not the only way to provide crash recovery. Early file
systems used a scavenger during reboot (for example, the UNIX <code
id="fsck_1">fsck</code> program) to examine every file and directory and
the block and inode free lists, looking for and resolving
inconsistencies. Scavenging can take hours for large file systems, and
there are situations where it is not possible to resolve inconsistencies
in a way that causes the original system calls to be atomic. Recovery
from a log is much faster and causes system calls to be atomic in the
face of crashes.</p>
<p>Xv6 uses the same basic on-disk layout of inodes and directories as
early UNIX; this scheme has been remarkably persistent over the years.
BSD’s UFS/FFS and Linux’s ext2/ext3 use essentially the same data
structures. The most inefficient part of the file system layout is the
directory, which requires a linear scan over all the disk blocks during
each lookup. This is reasonable when directories are only a few disk
blocks, but is expensive for directories holding many files. Microsoft
Windows’s NTFS, macOS’s HFS, and Solaris’s ZFS, just to name a few,
implement a directory as an on-disk balanced tree of blocks. This is
complicated but guarantees logarithmic-time directory lookups.</p>
<p>Xv6 is naive about disk failures: if a disk operation fails, xv6
panics. Whether this is reasonable depends on the hardware: if an
operating systems sits atop special hardware that uses redundancy to
mask disk failures, perhaps the operating system sees failures so
infrequently that panicking is okay. On the other hand, operating
systems using plain disks should expect failures and handle them more
gracefully, so that the loss of a block in one file doesn’t affect the
use of the rest of the file system.</p>
<p>Xv6 requires that the file system fit on one disk device and not
change in size. As large databases and multimedia files drive storage
requirements ever higher, operating systems are developing ways to
eliminate the “one disk per file system” bottleneck. The basic approach
is to combine many disks into a single logical disk. Hardware solutions
such as RAID are still the most popular, but the current trend is moving
toward implementing as much of this logic in software as possible. These
software implementations typically allow rich functionality like growing
or shrinking the logical device by adding or removing disks on the fly.
Of course, a storage layer that can grow or shrink on the fly requires a
file system that can do the same: the fixed-size array of inode blocks
used by xv6 would not work well in such environments. Separating disk
management from the file system may be the cleanest design, but the
complex interface between the two has led some systems, like Sun’s ZFS,
to combine them.</p>
<p>Xv6’s file system lacks many other features of modern file systems;
for example, it lacks support for snapshots and incremental backup.</p>
<p>Modern Unix systems allow many kinds of resources to be accessed with
the same system calls as on-disk storage: named pipes, network
connections, remotely-accessed network file systems, and monitoring and
control interfaces such as <code>/proc</code>. Instead of xv6’s
<code>if</code> statements in <code id="fileread_3">fileread</code> and
<code id="filewrite_4">filewrite</code>, these systems typically give
each open file a table of function pointers, one per operation, and call
the function pointer to invoke that inode’s implementation of the call.
Network file systems and user-level file systems provide functions that
turn those calls into network RPCs and wait for the response before
returning.</p>
</section>
<section id="exercises-7" class="level2" data-number="8.16">
<h2 data-number="8.16"><span class="header-section-number">8.16</span>
Exercises</h2>
<ol>
<li><p>Why panic in <code>balloc</code> ? Can xv6 recover?</p></li>
<li><p>Why panic in <code>ialloc</code> ? Can xv6 recover?</p></li>
<li><p>Why doesn’t <code>filealloc</code> panic when it runs out of
files? Why is this more common and therefore worth handling?</p></li>
<li><p>Suppose the file corresponding to <code>ip</code> gets unlinked
by another process between <code>sys_link</code> ’s calls to
<code>iunlock(ip)</code> and <code>dirlink</code>. Will the link be
created correctly? Why or why not?</p></li>
<li><p><code>create</code> makes four function calls (one to
<code>ialloc</code> and three to <code>dirlink</code>) that it requires
to succeed. If any doesn’t, <code>create</code> calls
<code>panic</code>. Why is this acceptable? Why can’t any of those four
calls fail?</p></li>
<li><p><code>sys_chdir</code> calls <code>iunlock(ip)</code> before
<code>iput(cp-&gt;cwd)</code>, which might try to lock
<code>cp-&gt;cwd</code>, yet postponing <code>iunlock(ip)</code> until
after the <code>iput</code> would not cause deadlocks. Why not?</p></li>
<li><p>Implement the <code>lseek</code> system call. Supporting
<code>lseek</code> will also require that you modify
<code>filewrite</code> to fill holes in the file with zero if
<code>lseek</code> sets <code>off</code> beyond
<code>f-&gt;ip-&gt;size.</code></p></li>
<li><p>Add <code>O_TRUNC</code> and <code>O_APPEND</code> to
<code>open</code>, so that <code>&gt;</code> and <code>&gt;&gt;</code>
operators work in the shell.</p></li>
<li><p>Modify the file system to support symbolic links.</p></li>
<li><p>Modify the file system to support named pipes.</p></li>
<li><p>Modify the file and VM system to support memory-mapped
files.</p></li>
</ol>
</section>
</section>
<section id="CH:LOCK2" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span>
Concurrency revisited</h1>
<p>Simultaneously obtaining good parallel performance, correctness
despite concurrency, and understandable code is a big challenge in
kernel design. Straightforward use of locks is the best path to
correctness, but is not always possible. This chapter highlights
examples in which xv6 is forced to use locks in an involved way, and
examples where xv6 uses lock-like techniques but not locks.</p>
<section id="locking-patterns" class="level2" data-number="9.1">
<h2 data-number="9.1"><span class="header-section-number">9.1</span>
Locking patterns</h2>
<p>Cached items are often a challenge to lock. For example, the file
system’s block cache <a
href="xv6-riscv-src/kernel/bio.c#L26"><span>(kernel/bio.c:26)</span></a>
stores copies of up to <span><code>NBUF</code></span> disk blocks. It’s
vital that a given disk block have at most one copy in the cache;
otherwise, different processes might make conflicting changes to
different copies of what ought to be the same block. Each cached block
is stored in a <span><code>struct buf</code></span> <a
href="xv6-riscv-src/kernel/buf.h#L1"><span>(kernel/buf.h:1)</span></a>.
A <span><code>struct buf</code></span> has a lock field which helps
ensure that only one process uses a given disk block at a time. However,
that lock is not enough: what if a block is not present in the cache at
all, and two processes want to use it at the same time? There is no
<span><code>struct buf</code></span> (since the block isn’t yet cached),
and thus there is nothing to lock. Xv6 deals with this situation by
associating an additional lock (<span><code>bcache.lock</code></span>)
with the set of identities of cached blocks. Code that needs to check if
a block is cached (e.g., <span><code>bget</code></span> <a
href="xv6-riscv-src/kernel/bio.c#L59"><span>(kernel/bio.c:59)</span></a>),
or change the set of cached blocks, must hold
<span><code>bcache.lock</code></span>; after that code has found the
block and <span><code>struct buf</code></span> it needs, it can release
<span><code> bcache.lock</code></span> and lock just the specific block.
This is a common pattern: one lock for the set of items, plus one lock
per item.</p>
<p>Ordinarily the same function that acquires a lock will release it.
But a more precise way to view things is that a lock is acquired at the
start of a sequence that must appear atomic, and released when that
sequence ends. If the sequence starts and ends in different functions,
or different threads, or on different CPUs, then the lock acquire and
release must do the same. The function of the lock is to force other
uses to wait, not to pin a piece of data to a particular agent. One
example is the <span><code>acquire</code></span> in
<span><code>yield</code></span> <a
href="xv6-riscv-src/kernel/proc.c#L503"><span>(kernel/proc.c:503)</span></a>,
which is released in the scheduler thread rather than in the acquiring
process. Another example is the <span><code>acquiresleep</code></span>
in <span><code>ilock</code></span> <a
href="xv6-riscv-src/kernel/fs.c#L293"><span>(kernel/fs.c:293)</span></a>;
this code often sleeps while reading the disk; it may wake up on a
different CPU, which means the lock may be acquired and released on
different CPUs.</p>
<p>Freeing an object that is protected by a lock embedded in the object
is a delicate business, since owning the lock is not enough to guarantee
that freeing would be correct. The problem case arises when some other
thread is waiting in <span><code>acquire</code></span> to use the
object; freeing the object implicitly frees the embedded lock, which
will cause the waiting thread to malfunction. One solution is to track
how many references to the object exist, so that it is only freed when
the last reference disappears. See <span><code>pipeclose</code></span>
<a
href="xv6-riscv-src/kernel/pipe.c#L59"><span>(kernel/pipe.c:59)</span></a>
for an example; <span><code>pi-&gt;readopen</code></span> and
<span><code>pi-&gt;writeopen</code></span> track whether the pipe has
file descriptors referring to it.</p>
<p>Usually one sees locks around sequences of reads and writes to sets
of related items; the locks ensure that other threads see only completed
sequences of updates (as long as they, too, lock). What about situations
where the update is a simple write to a single shared variable? For
example, <code>setkilled</code> and <code>killed</code> <a
href="xv6-riscv-src/kernel/proc.c#L607"><span>(kernel/proc.c:607)</span></a>
lock around their simple uses of <code>p-&gt;killed</code>. If there
were no lock, one thread could write <code>p-&gt;killed</code> at the
same time that another thread reads it. This is a <span
id="race_2">race</span>, and the C language specification says that a
race yields <em><span id="undefined_behavior_1">undefined
behavior</span></em>, which means the program may crash or yield
incorrect results<a href="#fn4" class="footnote-ref" id="fnref4"
role="doc-noteref"><sup>4</sup></a>. The locks prevent the race and
avoid the undefined behavior.</p>
<p>One reason races can break programs is that, if there are no locks or
equivalent constructs, the compiler may generate machine code that reads
and writes memory in ways quite different than the original C code. For
example, the machine code of a thread calling <code>killed</code> could
copy <code>p-&gt;killed</code> to a register and read only that cached
value; this would mean that the thread might never see any writes to
<code>p-&gt;killed</code>. The locks prevent such caching.</p>
</section>
<section id="lock-like-patterns" class="level2" data-number="9.2">
<h2 data-number="9.2"><span class="header-section-number">9.2</span>
Lock-like patterns</h2>
<p>In many places xv6 uses a reference count or a flag in a lock-like
way to indicate that an object is allocated and should not be freed or
re-used. A process’s <span><code>p-&gt;state</code></span> acts in this
way, as do the reference counts in <span><code>file</code></span>,
<span><code>inode</code></span>, and <span><code>buf</code></span>
structures. While in each case a lock protects the flag or reference
count, it is the latter that prevents the object from being prematurely
freed.</p>
<p>The file system uses <span><code>struct inode</code></span> reference
counts as a kind of shared lock that can be held by multiple processes,
in order to avoid deadlocks that would occur if the code used ordinary
locks. For example, the loop in <span><code>namex</code></span> <a
href="xv6-riscv-src/kernel/fs.c#L652"><span>(kernel/fs.c:652)</span></a>
locks the directory named by each pathname component in turn. However,
<span><code>namex</code></span> must release each lock at the end of the
loop, since if it held multiple locks it could deadlock with itself if
the pathname included a dot (e.g., <span><code>a/./b</code></span>). It
might also deadlock with a concurrent lookup involving the directory and
<span><code>..</code></span>. As Chapter <a href="#CH:FS"
data-reference-type="ref" data-reference="CH:FS">8</a> explains, the
solution is for the loop to carry the directory inode over to the next
iteration with its reference count incremented, but not locked.</p>
<p>Some data items are protected by different mechanisms at different
times, and may at times be protected from concurrent access implicitly
by the structure of the xv6 code rather than by explicit locks. For
example, when a physical page is free, it is protected by
<code>kmem.lock</code> <a
href="xv6-riscv-src/kernel/kalloc.c#L24"><span>(kernel/kalloc.c:24)</span></a>.
If the page is then allocated as a pipe <a
href="xv6-riscv-src/kernel/pipe.c#L23"><span>(kernel/pipe.c:23)</span></a>,
it is protected by a different lock (the embedded
<code>pi-&gt;lock</code>). If the page is re-allocated for a new
process’s user memory, it is not protected by a lock at all. Instead,
the fact that the allocator won’t give that page to any other process
(until it is freed) protects it from concurrent access. The ownership of
a new process’s memory is complex: first the parent allocates and
manipulates it in <span><code>fork</code></span>, then the child uses
it, and (after the child exits) the parent again owns the memory and
passes it to <span><code> kfree</code></span>. There are two lessons
here: a data object may be protected from concurrency in different ways
at different points in its lifetime, and the protection may take the
form of implicit structure rather than explicit locks.</p>
<p>A final lock-like example is the need to disable interrupts around
calls to <span><code>mycpu()</code></span> <a
href="xv6-riscv-src/kernel/proc.c#L83"><span>(kernel/proc.c:83)</span></a>.
Disabling interrupts causes the calling code to be atomic with respect
to timer interrupts that could force a context switch, and thus move the
process to a different CPU.</p>
</section>
<section id="no-locks-at-all" class="level2" data-number="9.3">
<h2 data-number="9.3"><span class="header-section-number">9.3</span> No
locks at all</h2>
<p>There are a few places where xv6 shares mutable data with no locks at
all. One is in the implementation of spinlocks, although one could view
the RISC-V atomic instructions as relying on locks implemented in
hardware. Another is the <span><code>started</code></span> variable in
<span><code>main.c</code></span> <a
href="xv6-riscv-src/kernel/main.c#L7"><span>(kernel/main.c:7)</span></a>,
used to prevent other CPUs from running until CPU zero has finished
initializing xv6; the <span><code>volatile</code></span> ensures that
the compiler actually generates load and store instructions.</p>
<p>Xv6 contains cases in which one CPU or thread writes some data, and
another CPU or thread reads the data, but there is no specific lock
dedicated to protecting that data. For example, in
<span><code>fork</code></span>, the parent writes the child’s user
memory pages, and the child (a different thread, perhaps on a different
CPU) reads those pages; no lock explicitly protects those pages. This is
not strictly a locking problem, since the child doesn’t start executing
until after the parent has finished writing. It is a potential memory
ordering problem (see Chapter <a href="#CH:LOCK"
data-reference-type="ref" data-reference="CH:LOCK">6</a>), since without
a memory barrier there’s no reason to expect one CPU to see another
CPU’s writes. However, since the parent releases locks, and the child
acquires locks as it starts up, the memory barriers in
<span><code>acquire</code></span> and <span><code>release</code></span>
ensure that the child’s CPU sees the parent’s writes.</p>
</section>
<section id="parallelism" class="level2" data-number="9.4">
<h2 data-number="9.4"><span class="header-section-number">9.4</span>
Parallelism</h2>
<p>Locking is primarily about suppressing parallelism in the interests
of correctness. Because performance is also important, kernel designers
often have to think about how to use locks in a way that both achieves
correctness and allows parallelism. While xv6 is not systematically
designed for high performance, it’s still worth considering which xv6
operations can execute in parallel, and which might conflict on
locks.</p>
<p>Pipes in xv6 are an example of fairly good parallelism. Each pipe has
its own lock, so that different processes can read and write different
pipes in parallel on different CPUs. For a given pipe, however, the
writer and reader must wait for each other to release the lock; they
can’t read/write the same pipe at the same time. It is also the case
that a read from an empty pipe (or a write to a full pipe) must block,
but this is not due to the locking scheme.</p>
<p>Context switching is a more complex example. Two kernel threads, each
executing on its own CPU, can call <span><code>yield</code></span>,
<span><code>sched</code></span>, and <span><code> swtch</code></span> at
the same time, and the calls will execute in parallel. The threads each
hold a lock, but they are different locks, so they don’t have to wait
for each other. Once in <span><code>scheduler</code></span>, however,
the two CPUs may conflict on locks while searching the table of
processes for one that is <span><code>RUNNABLE</code></span>. That is,
xv6 is likely to get a performance benefit from multiple CPUs during
context switch, but perhaps not as much as it could.</p>
<p>Another example is concurrent calls to <span><code>fork</code></span>
from different processes on different CPUs. The calls may have to wait
for each other for <span><code>pid_lock</code></span> and
<span><code>kmem.lock</code></span>, and for per-process locks needed to
search the process table for an <span><code>UNUSED</code></span>
process. On the other hand, the two forking processes can copy user
memory pages and format page-table pages fully in parallel.</p>
<p>The locking scheme in each of the above examples sacrifices parallel
performance in certain cases. In each case it’s possible to obtain more
parallelism using a more elaborate design. Whether it’s worthwhile
depends on details: how often the relevant operations are invoked, how
long the code spends with a contended lock held, how many CPUs might be
running conflicting operations at the same time, whether other parts of
the code are more restrictive bottlenecks. It can be difficult to guess
whether a given locking scheme might cause performance problems, or
whether a new design is significantly better, so measurement on
realistic workloads is often required.</p>
</section>
<section id="exercises-8" class="level2" data-number="9.5">
<h2 data-number="9.5"><span class="header-section-number">9.5</span>
Exercises</h2>
<ol>
<li><p>Modify xv6’s pipe implementation to allow a read and a write to
the same pipe to proceed in parallel on different cores.</p></li>
<li><p>Modify xv6’s <code>scheduler()</code> to reduce lock contention
when different cores are looking for runnable processes at the same
time.</p></li>
<li><p>Eliminate some of the serialization in xv6’s
<code>fork()</code>.</p></li>
</ol>
</section>
</section>
<section id="CH:SUM" class="level1" data-number="10">
<h1 data-number="10"><span class="header-section-number">10</span>
Summary</h1>
<p>This text introduced the main ideas in operating systems by studying
one operating system, xv6, line by line. Some code lines embody the
essence of the main ideas (e.g., context switching, user/kernel
boundary, locks, etc.) and each line is important; other code lines
provide an illustration of how to implement a particular operating
system idea and could easily be done in different ways (e.g., a better
algorithm for scheduling, better on-disk data structures to represent
files, better logging to allow for concurrent transactions, etc.). All
the ideas were illustrated in the context of one particular, very
successful system call interface, the Unix interface, but those ideas
carry over to the design of other operating systems.</p>
<div id="refs" class="references csl-bib-body hanging-indent"
role="list">
<div id="ref-boehm04" class="csl-entry" role="listitem">
Boehm, Hans-J. 2005. <span>“Threads Cannot Be Implemented as a
Library.”</span> <em>ACM PLDI Conference</em>.
</div>
<div id="ref-dijkstra65" class="csl-entry" role="listitem">
Dijkstra, Edsger. 1965. <span>“Cooperating Sequential Processes.”</span>
<a
href="https://www.cs.utexas.edu/users/EWD/transcriptions/EWD01xx/EWD123.html"
class="uri">https://www.cs.utexas.edu/users/EWD/transcriptions/EWD01xx/EWD123.html</a>.
</div>
<div id="ref-herlihy:art" class="csl-entry" role="listitem">
Herlihy, Maurice, and Nir Shavit. 2012. <em>The Art of Multiprocessor
Programming, Revised Reprint</em>.
</div>
<div id="ref-kernighan" class="csl-entry" role="listitem">
Kernighan, Brian W. 1988. <em>The c Programming Language</em>. Edited by
Dennis M. Ritchie. 2nd ed. Prentice Hall Professional Technical
Reference.
</div>
<div id="ref-sel4" class="csl-entry" role="listitem">
Klein, Gerwin, Kevin Elphinstone, Gernot Heiser, June Andronick, David
Cock, Philip Derrin, Dhammika Elkaduwe, et al. 2009. <span>“SeL4: Formal
Verification of an <span>OS</span> Kernel.”</span> In <em>Proceedings of
the ACM SIGOPS 22nd Symposium on Operating Systems Principles</em>,
207–20.
</div>
<div id="ref-knuth" class="csl-entry" role="listitem">
Knuth, Donald. 1997. <em>Fundamental Algorithms. The Art of Computer
Programming. (Second Ed.)</em>. Vol. 1. addison-wesley.
</div>
<div id="ref-lamport:bakery" class="csl-entry" role="listitem">
Lamport, L. 1974. <span>“A New Solution of Dijkstra’s Concurrent
Programming Problem.”</span> <em>Communications of the ACM</em>.
</div>
<div id="ref-mitre:cves" class="csl-entry" role="listitem">
<span>“Linux Common Vulnerabilities and Exposures
(<span>CVEs</span>).”</span> n.d. <a
href="https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=linux"
class="uri">https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=linux</a>.
</div>
<div id="ref-lions" class="csl-entry" role="listitem">
Lions, John. 2000. <em>Commentary on UNIX 6th Edition</em>. Peer to Peer
Communications.
</div>
<div id="ref-mckenney:rcuusage" class="csl-entry" role="listitem">
Mckenney, Paul E., Silas Boyd-wickizer, and Jonathan Walpole. 2013.
<span>“<span>RCU</span> Usage in the Linux Kernel: One Decade
Later.”</span>
</div>
<div id="ref-ns16550a" class="csl-entry" role="listitem">
Michael, Martin, and Daniel Durich. 1987. <span>“The
<span>NS16550A</span>: <span>UART</span> Design and Application
Considerations.”</span> <a
href="http://bitsavers.trailing-edge.com/components/national/_appNotes/AN-0491.pdf"
class="uri">http://bitsavers.trailing-edge.com/components/national/_appNotes/AN-0491.pdf</a>.
</div>
<div id="ref-aleph:smashing" class="csl-entry" role="listitem">
One, Aleph. n.d. <span>“Smashing the Stack for Fun and Profit.”</span>
<a href="http://phrack.org/issues/49/14.html#article"
class="uri">http://phrack.org/issues/49/14.html#article</a>.
</div>
<div id="ref-riscv" class="csl-entry" role="listitem">
Patterson, David, and Andrew Waterman. 2017. <em>The <span>RISC-V</span>
Reader: An Open Architecture Atlas</em>. Strawberry Canyon.
</div>
<div id="ref-Presotto91plan9" class="csl-entry" role="listitem">
Presotto, Dave, Rob Pike, Ken Thompson, and Howard Trickey. 1991.
<span>“Plan 9, a Distributed System.”</span> In <em>In Proceedings of
the Spring 1991 EurOpen Conference</em>, 43–50.
</div>
<div id="ref-unix" class="csl-entry" role="listitem">
Ritchie, Dennis M., and Ken Thompson. 1974. <span>“The <span>UNIX</span>
Time-Sharing System.”</span> <em>Commun. ACM</em> 17 (7): 365–75. <a
href="https://doi.org/10.1145/361011.361061">https://doi.org/10.1145/361011.361061</a>.
</div>
<div id="ref-riscv:user" class="csl-entry" role="listitem">
Waterman, Andrew, and Krste Asanovic, eds. 2019. <span>“The
<span>RISC-V</span> Instruction Set Manual <span>Volume I</span>:
Unprivileged <span>ISA</span>.”</span> <a
href="https://github.com/riscv/riscv-isa-manual/releases/download/Ratified-IMAFDQC/riscv-spec-20191213.pdf"
class="uri">https://github.com/riscv/riscv-isa-manual/releases/download/Ratified-IMAFDQC/riscv-spec-20191213.pdf</a>.
</div>
<div id="ref-riscv:priv" class="csl-entry" role="listitem">
Waterman, Andrew, Krste Asanovic, and John Hauser, eds. 2021. <span>“The
<span>RISC-V</span> Instruction Set Manual <span>Volume II</span>:
Privileged Architecture.”</span> <a
href="https://github.com/riscv/riscv-isa-manual/releases/download/Priv-v1.12/riscv-privileged-20211203.pdf"
class="uri">https://github.com/riscv/riscv-isa-manual/releases/download/Priv-v1.12/riscv-privileged-20211203.pdf</a>.
</div>
</div>
</section>
<section id="index" class="level1" data-number="11">
<h1 data-number="11"><span class="header-section-number">11</span>
Index</h1>
<ul>
<li><code>.</code>
<a href="#._1">1</a>
<a href="#._2">2</a></li>
<li><code>..</code>
<a href="#.._1">1</a>
<a href="#.._2">2</a></li>
<li><code>/init</code>
<a href="#/init_1">1</a>
<a href="#/init_2">2</a></li>
<li><code>BSIZE</code>
<a href="#BSIZE_1">1</a></li>
<li>CPU
<a href="#CPU_1">1</a></li>
<li><code>DIRSIZ</code>
<a href="#DIRSIZ_1">1</a></li>
<li>ELF format
<a href="#ELF_format_1">1</a></li>
<li><code>ELF_MAGIC</code>
<a href="#ELF_MAGIC_1">1</a></li>
<li>I/O
<a href="#I/O_1">1</a></li>
<li>I/O concurrency
<a href="#I/O_concurrency_1">1</a></li>
<li>I/O redirection
<a href="#I/O_redirection_1">1</a></li>
<li><code>NBUF</code>
<a href="#NBUF_1">1</a></li>
<li><code>NDIRECT</code>
<a href="#NDIRECT_1">1</a>
<a href="#NDIRECT_2">2</a></li>
<li><code>NINDIRECT</code>
<a href="#NINDIRECT_1">1</a>
<a href="#NINDIRECT_2">2</a></li>
<li><code>O_CREATE</code>
<a href="#O_CREATE_1">1</a>
<a href="#O_CREATE_2">2</a></li>
<li><code>PGROUNDUP</code>
<a href="#PGROUNDUP_1">1</a></li>
<li><code>PHYSTOP</code>
<a href="#PHYSTOP_1">1</a>
<a href="#PHYSTOP_2">2</a></li>
<li><code>PID</code>
<a href="#PID_1">1</a></li>
<li><code>PTE_R</code>
<a href="#PTE_R_1">1</a></li>
<li><code>PTE_U</code>
<a href="#PTE_U_1">1</a>
<a href="#PTE_U_2">2</a></li>
<li><code>PTE_V</code>
<a href="#PTE_V_1">1</a></li>
<li><code>PTE_W</code>
<a href="#PTE_W_1">1</a></li>
<li><code>PTE_X</code>
<a href="#PTE_X_1">1</a></li>
<li><code>RUNNABLE</code>
<a href="#RUNNABLE_1">1</a>
<a href="#RUNNABLE_2">2</a>
<a href="#RUNNABLE_3">3</a>
<a href="#RUNNABLE_4">4</a></li>
<li><code>SLEEPING</code>
<a href="#SLEEPING_1">1</a>
<a href="#SLEEPING_2">2</a></li>
<li><code>SYS_exec</code>
<a href="#SYS_exec_1">1</a></li>
<li><code>TRAMPOLINE</code>
<a href="#TRAMPOLINE_1">1</a></li>
<li><code>T_DIR</code>
<a href="#T_DIR_1">1</a></li>
<li><code>T_FILE</code>
<a href="#T_FILE_1">1</a></li>
<li>Translation Look-aside Buffer (TLB)
<a href="#Translation_Look-aside_Buffer_(TLB)_1">1</a>
<a href="#Translation_Look-aside_Buffer_(TLB)_2">2</a></li>
<li>UART
<a href="#UART_1">1</a></li>
<li><code>ZOMBIE</code>
<a href="#ZOMBIE_1">1</a></li>
<li><code>_entry</code>
<a href="#_entry_1">1</a></li>
<li>absorption
<a href="#absorption_1">1</a></li>
<li><code>acquire</code>
<a href="#acquire_1">1</a>
<a href="#acquire_2">2</a></li>
<li>address space
<a href="#address_space_1">1</a></li>
<li><code>argc</code>
<a href="#argc_1">1</a></li>
<li><code>argv</code>
<a href="#argv_1">1</a></li>
<li>atomic
<a href="#atomic_1">1</a></li>
<li><code>balloc</code>
<a href="#balloc_1">1</a>
<a href="#balloc_2">2</a></li>
<li>batching
<a href="#batching_1">1</a></li>
<li><code>bcache.head</code>
<a href="#bcache.head_1">1</a></li>
<li><code>begin_op</code>
<a href="#begin_op_1">1</a></li>
<li><code>bfree</code>
<a href="#bfree_1">1</a></li>
<li><code>bget</code>
<a href="#bget_1">1</a>
<a href="#bget_2">2</a>
<a href="#bget_3">3</a></li>
<li><code>binit</code>
<a href="#binit_1">1</a></li>
<li><code>bmap</code>
<a href="#bmap_1">1</a>
<a href="#bmap_2">2</a></li>
<li>bottom half
<a href="#bottom_half_1">1</a></li>
<li><code>bread</code>
<a href="#bread_1">1</a>
<a href="#bread_2">2</a></li>
<li><code>brelse</code>
<a href="#brelse_1">1</a>
<a href="#brelse_2">2</a></li>
<li>buf
<a href="#buf_1">1</a></li>
<li>busy waiting
<a href="#busy_waiting_1">1</a></li>
<li><code>bwrite</code>
<a href="#bwrite_1">1</a>
<a href="#bwrite_2">2</a>
<a href="#bwrite_3">3</a></li>
<li><code>chan</code>
<a href="#chan_1">1</a>
<a href="#chan_2">2</a></li>
<li>child
<a href="#child_1">1</a></li>
<li>commit
<a href="#commit_1">1</a></li>
<li>concurrency
<a href="#concurrency_1">1</a></li>
<li>concurrency control
<a href="#concurrency_control_1">1</a></li>
<li>condition lock
<a href="#condition_lock_1">1</a></li>
<li>conditional synchronization
<a href="#conditional_synchronization_1">1</a></li>
<li>conflict
<a href="#conflict_1">1</a></li>
<li>contention
<a href="#contention_1">1</a></li>
<li>contexts
<a href="#contexts_1">1</a></li>
<li>convoys
<a href="#convoys_1">1</a></li>
<li>copy-on-write (COW) fork
<a href="#copy-on-write_(COW)_fork_1">1</a></li>
<li><code>copyinstr</code>
<a href="#copyinstr_1">1</a></li>
<li><code>copyout</code>
<a href="#copyout_1">1</a></li>
<li>coroutines
<a href="#coroutines_1">1</a></li>
<li><code>cpu-&gt;context</code>
<a href="#cpu-&gt;context_1">1</a>
<a href="#cpu-&gt;context_2">2</a>
<a href="#cpu-&gt;context_3">3</a></li>
<li>crash recovery
<a href="#crash_recovery_1">1</a></li>
<li><code>create</code>
<a href="#create_1">1</a>
<a href="#create_2">2</a>
<a href="#create_3">3</a></li>
<li>critical section
<a href="#critical_section_1">1</a></li>
<li>current directory
<a href="#current_directory_1">1</a></li>
<li>deadlock
<a href="#deadlock_1">1</a></li>
<li>demand paging
<a href="#demand_paging_1">1</a></li>
<li>direct blocks
<a href="#direct_blocks_1">1</a></li>
<li>direct memory access (DMA)
<a href="#direct_memory_access_(DMA)_1">1</a></li>
<li><code>dirlink</code>
<a href="#dirlink_1">1</a></li>
<li><code>dirlookup</code>
<a href="#dirlookup_1">1</a>
<a href="#dirlookup_2">2</a>
<a href="#dirlookup_3">3</a>
<a href="#dirlookup_4">4</a></li>
<li><code>disk</code>
<a href="#disk_1">1</a></li>
<li>driver
<a href="#driver_1">1</a></li>
<li><code>dup</code>
<a href="#dup_1">1</a></li>
<li><code>ecall</code>
<a href="#ecall_1">1</a>
<a href="#ecall_2">2</a></li>
<li><code>end_op</code>
<a href="#end_op_1">1</a></li>
<li>exception
<a href="#exception_1">1</a></li>
<li><code>exec</code>
<a href="#exec_1">1</a>
<a href="#exec_2">2</a>
<a href="#exec_3">3</a>
<a href="#exec_4">4</a>
<a href="#exec_5">5</a>
<a href="#exec_6">6</a>
<a href="#exec_7">7</a>
<a href="#exec_8">8</a></li>
<li><code>exit</code>
<a href="#exit_1">1</a>
<a href="#exit_2">2</a></li>
<li>file descriptor
<a href="#file_descriptor_1">1</a></li>
<li><code>filealloc</code>
<a href="#filealloc_1">1</a></li>
<li><code>fileclose</code>
<a href="#fileclose_1">1</a>
<a href="#fileclose_2">2</a></li>
<li><code>filedup</code>
<a href="#filedup_1">1</a>
<a href="#filedup_2">2</a></li>
<li><code>fileread</code>
<a href="#fileread_1">1</a>
<a href="#fileread_2">2</a>
<a href="#fileread_3">3</a></li>
<li><code>filestat</code>
<a href="#filestat_1">1</a></li>
<li><code>filewrite</code>
<a href="#filewrite_1">1</a>
<a href="#filewrite_2">2</a>
<a href="#filewrite_3">3</a>
<a href="#filewrite_4">4</a></li>
<li><code>fork</code>
<a href="#fork_1">1</a>
<a href="#fork_2">2</a>
<a href="#fork_3">3</a>
<a href="#fork_4">4</a>
<a href="#fork_5">5</a></li>
<li><code>forkret</code>
<a href="#forkret_1">1</a></li>
<li><code>freerange</code>
<a href="#freerange_1">1</a></li>
<li><code>fsck</code>
<a href="#fsck_1">1</a></li>
<li><code>fsinit</code>
<a href="#fsinit_1">1</a></li>
<li><code>ftable</code>
<a href="#ftable_1">1</a></li>
<li><code>getcmd</code>
<a href="#getcmd_1">1</a></li>
<li>group commit
<a href="#group_commit_1">1</a></li>
<li>guard page
<a href="#guard_page_1">1</a></li>
<li>handler
<a href="#handler_1">1</a></li>
<li>hartid
<a href="#hartid_1">1</a></li>
<li><code>ialloc</code>
<a href="#ialloc_1">1</a>
<a href="#ialloc_2">2</a></li>
<li><code>iget</code>
<a href="#iget_1">1</a>
<a href="#iget_2">2</a>
<a href="#iget_3">3</a>
<a href="#iget_4">4</a></li>
<li><code>ilock</code>
<a href="#ilock_1">1</a>
<a href="#ilock_2">2</a>
<a href="#ilock_3">3</a>
<a href="#ilock_4">4</a></li>
<li>indirect block
<a href="#indirect_block_1">1</a></li>
<li><code>initcode.S</code>
<a href="#initcode.S_1">1</a>
<a href="#initcode.S_2">2</a></li>
<li><code>initlog</code>
<a href="#initlog_1">1</a></li>
<li>inode
<a href="#inode_1">1</a>
<a href="#inode_2">2</a>
<a href="#inode_3">3</a></li>
<li><code>install_trans</code>
<a href="#install_trans_1">1</a></li>
<li>interrupt
<a href="#interrupt_1">1</a></li>
<li><code>iput</code>
<a href="#iput_1">1</a>
<a href="#iput_2">2</a>
<a href="#iput_3">3</a></li>
<li>isolation
<a href="#isolation_1">1</a></li>
<li><code>itable</code>
<a href="#itable_1">1</a></li>
<li><code>itrunc</code>
<a href="#itrunc_1">1</a>
<a href="#itrunc_2">2</a></li>
<li><code>iunlock</code>
<a href="#iunlock_1">1</a></li>
<li><code>kalloc</code>
<a href="#kalloc_1">1</a></li>
<li>kernel
<a href="#kernel_1">1</a>
<a href="#kernel_2">2</a></li>
<li>kernel space
<a href="#kernel_space_1">1</a>
<a href="#kernel_space_2">2</a></li>
<li><code>kfree</code>
<a href="#kfree_1">1</a></li>
<li><code>kinit</code>
<a href="#kinit_1">1</a></li>
<li><code>kvminit</code>
<a href="#kvminit_1">1</a></li>
<li><code>kvminithart</code>
<a href="#kvminithart_1">1</a></li>
<li><code>kvmmake</code>
<a href="#kvmmake_1">1</a></li>
<li><code>kvmmap</code>
<a href="#kvmmap_1">1</a>
<a href="#kvmmap_2">2</a></li>
<li>lazy allocation
<a href="#lazy_allocation_1">1</a></li>
<li>links
<a href="#links_1">1</a></li>
<li><code>loadseg</code>
<a href="#loadseg_1">1</a></li>
<li>lock
<a href="#lock_1">1</a></li>
<li>log
<a href="#log_1">1</a></li>
<li><code>log_write</code>
<a href="#log_write_1">1</a></li>
<li>lost wake-up
<a href="#lost_wake-up_1">1</a></li>
<li>machine mode
<a href="#machine_mode_1">1</a></li>
<li><code>main</code>
<a href="#main_1">1</a>
<a href="#main_2">2</a>
<a href="#main_3">3</a>
<a href="#main_4">4</a></li>
<li><code>malloc</code>
<a href="#malloc_1">1</a></li>
<li><code>mappages</code>
<a href="#mappages_1">1</a></li>
<li>memory barrier
<a href="#memory_barrier_1">1</a></li>
<li>memory model
<a href="#memory_model_1">1</a></li>
<li>memory-mapped
<a href="#memory-mapped_1">1</a>
<a href="#memory-mapped_2">2</a></li>
<li>metadata
<a href="#metadata_1">1</a></li>
<li>microkernel
<a href="#microkernel_1">1</a></li>
<li><code>mkdev</code>
<a href="#mkdev_1">1</a>
<a href="#mkdev_2">2</a></li>
<li><code>mkdir</code>
<a href="#mkdir_1">1</a>
<a href="#mkdir_2">2</a></li>
<li><code>mkfs</code>
<a href="#mkfs_1">1</a></li>
<li>monolithic kernel
<a href="#monolithic_kernel_1">1</a>
<a href="#monolithic_kernel_2">2</a></li>
<li>multi-core
<a href="#multi-core_1">1</a></li>
<li>multiplexing
<a href="#multiplexing_1">1</a></li>
<li>multiprocessor
<a href="#multiprocessor_1">1</a></li>
<li>mutual exclusion
<a href="#mutual_exclusion_1">1</a></li>
<li><code>mycpu</code>
<a href="#mycpu_1">1</a></li>
<li><code>myproc</code>
<a href="#myproc_1">1</a></li>
<li><code>namei</code>
<a href="#namei_1">1</a>
<a href="#namei_2">2</a></li>
<li><code>nameiparent</code>
<a href="#nameiparent_1">1</a>
<a href="#nameiparent_2">2</a>
<a href="#nameiparent_3">3</a>
<a href="#nameiparent_4">4</a></li>
<li><code>namex</code>
<a href="#namex_1">1</a>
<a href="#namex_2">2</a></li>
<li><code>open</code>
<a href="#open_1">1</a>
<a href="#open_2">2</a>
<a href="#open_3">3</a></li>
<li><code>p-&gt;context</code>
<a href="#p-&gt;context_1">1</a></li>
<li><code>p-&gt;killed</code>
<a href="#p-&gt;killed_1">1</a></li>
<li><code>p-&gt;kstack</code>
<a href="#p-&gt;kstack_1">1</a></li>
<li><code>p-&gt;lock</code>
<a href="#p-&gt;lock_1">1</a>
<a href="#p-&gt;lock_2">2</a>
<a href="#p-&gt;lock_3">3</a>
<a href="#p-&gt;lock_4">4</a>
<a href="#p-&gt;lock_5">5</a>
<a href="#p-&gt;lock_6">6</a>
<a href="#p-&gt;lock_7">7</a>
<a href="#p-&gt;lock_8">8</a></li>
<li><code>p-&gt;pagetable</code>
<a href="#p-&gt;pagetable_1">1</a>
<a href="#p-&gt;pagetable_2">2</a></li>
<li><code>p-&gt;state</code>
<a href="#p-&gt;state_1">1</a></li>
<li><code>p-&gt;xxx</code>
<a href="#p-&gt;xxx_1">1</a></li>
<li>page
<a href="#page_1">1</a></li>
<li>page table entries (PTEs)
<a href="#page_table_entries_(PTEs)_1">1</a></li>
<li>page-fault exception
<a href="#page-fault_exception_1">1</a>
<a href="#page-fault_exception_2">2</a></li>
<li>paging area
<a href="#paging_area_1">1</a></li>
<li>paging to disk
<a href="#paging_to_disk_1">1</a></li>
<li>parent
<a href="#parent_1">1</a></li>
<li>path
<a href="#path_1">1</a></li>
<li>persistence
<a href="#persistence_1">1</a></li>
<li>physical address
<a href="#physical_address_1">1</a></li>
<li>pipe
<a href="#pipe_1">1</a></li>
<li><code>piperead</code>
<a href="#piperead_1">1</a></li>
<li><code>pipewrite</code>
<a href="#pipewrite_1">1</a></li>
<li>polling
<a href="#polling_1">1</a>
<a href="#polling_2">2</a></li>
<li><code>pop_off</code>
<a href="#pop_off_1">1</a>
<a href="#pop_off_2">2</a></li>
<li><code>printf</code>
<a href="#printf_1">1</a></li>
<li>priority inversion
<a href="#priority_inversion_1">1</a></li>
<li>privileged instructions
<a href="#privileged_instructions_1">1</a></li>
<li><code>proc_mapstacks</code>
<a href="#proc_mapstacks_1">1</a></li>
<li><code>proc_pagetable</code>
<a href="#proc_pagetable_1">1</a></li>
<li>process
<a href="#process_1">1</a>
<a href="#process_2">2</a></li>
<li>programmed I/O
<a href="#programmed_I/O_1">1</a></li>
<li><code>push_off</code>
<a href="#push_off_1">1</a></li>
<li>race
<a href="#race_1">1</a>
<a href="#race_2">2</a></li>
<li>re-entrant locks
<a href="#re-entrant_locks_1">1</a></li>
<li><code>read</code>
<a href="#read_1">1</a></li>
<li><code>readi</code>
<a href="#readi_1">1</a>
<a href="#readi_2">2</a>
<a href="#readi_3">3</a>
<a href="#readi_4">4</a></li>
<li><code>recover_from_log</code>
<a href="#recover_from_log_1">1</a></li>
<li>recursive locks
<a href="#recursive_locks_1">1</a></li>
<li><code>release</code>
<a href="#release_1">1</a>
<a href="#release_2">2</a></li>
<li>root
<a href="#root_1">1</a></li>
<li>round robin
<a href="#round_robin_1">1</a></li>
<li><code>sbrk</code>
<a href="#sbrk_1">1</a></li>
<li><code>scause</code>
<a href="#scause_1">1</a></li>
<li><code>sched</code>
<a href="#sched_1">1</a>
<a href="#sched_2">2</a>
<a href="#sched_3">3</a>
<a href="#sched_4">4</a>
<a href="#sched_5">5</a>
<a href="#sched_6">6</a>
<a href="#sched_7">7</a></li>
<li><code>scheduler</code>
<a href="#scheduler_1">1</a>
<a href="#scheduler_2">2</a>
<a href="#scheduler_3">3</a>
<a href="#scheduler_4">4</a></li>
<li>semaphore
<a href="#semaphore_1">1</a></li>
<li><code>sepc</code>
<a href="#sepc_1">1</a></li>
<li>sequence coordination
<a href="#sequence_coordination_1">1</a></li>
<li>serializing
<a href="#serializing_1">1</a></li>
<li><code>sfence.vma</code>
<a href="#sfence.vma_1">1</a></li>
<li>shell
<a href="#shell_1">1</a></li>
<li><code>signal</code>
<a href="#signal_1">1</a></li>
<li><code>skipelem</code>
<a href="#skipelem_1">1</a></li>
<li><code>sleep</code>
<a href="#sleep_1">1</a>
<a href="#sleep_2">2</a>
<a href="#sleep_3">3</a></li>
<li>sleep-locks
<a href="#sleep-locks_1">1</a></li>
<li><code>sret</code>
<a href="#sret_1">1</a></li>
<li><code>sscratch</code>
<a href="#sscratch_1">1</a></li>
<li><code>sstatus</code>
<a href="#sstatus_1">1</a></li>
<li><code>stat</code>
<a href="#stat_1">1</a>
<a href="#stat_2">2</a></li>
<li><code>stati</code>
<a href="#stati_1">1</a>
<a href="#stati_2">2</a></li>
<li><code>struct context</code>
<a href="#struct_context_1">1</a></li>
<li><code>struct cpu</code>
<a href="#struct_cpu_1">1</a></li>
<li><code>struct dinode</code>
<a href="#struct_dinode_1">1</a>
<a href="#struct_dinode_2">2</a></li>
<li><code>struct dirent</code>
<a href="#struct_dirent_1">1</a></li>
<li><code>struct elfhdr</code>
<a href="#struct_elfhdr_1">1</a></li>
<li><code>struct file</code>
<a href="#struct_file_1">1</a></li>
<li><code>struct inode</code>
<a href="#struct_inode_1">1</a></li>
<li><code>struct pipe</code>
<a href="#struct_pipe_1">1</a></li>
<li><code>struct proc</code>
<a href="#struct_proc_1">1</a></li>
<li><code>struct run</code>
<a href="#struct_run_1">1</a></li>
<li><code>struct spinlock</code>
<a href="#struct_spinlock_1">1</a></li>
<li><code>stval</code>
<a href="#stval_1">1</a></li>
<li><code>stvec</code>
<a href="#stvec_1">1</a></li>
<li>superblock
<a href="#superblock_1">1</a></li>
<li>supervisor mode
<a href="#supervisor_mode_1">1</a></li>
<li><code>swtch</code>
<a href="#swtch_1">1</a>
<a href="#swtch_2">2</a>
<a href="#swtch_3">3</a>
<a href="#swtch_4">4</a>
<a href="#swtch_5">5</a>
<a href="#swtch_6">6</a>
<a href="#swtch_7">7</a>
<a href="#swtch_8">8</a>
<a href="#swtch_9">9</a>
<a href="#swtch_10">10</a></li>
<li><code>sys_link</code>
<a href="#sys_link_1">1</a>
<a href="#sys_link_2">2</a>
<a href="#sys_link_3">3</a>
<a href="#sys_link_4">4</a></li>
<li><code>sys_mkdir</code>
<a href="#sys_mkdir_1">1</a></li>
<li><code>sys_mknod</code>
<a href="#sys_mknod_1">1</a></li>
<li><code>sys_open</code>
<a href="#sys_open_1">1</a>
<a href="#sys_open_2">2</a></li>
<li><code>sys_pipe</code>
<a href="#sys_pipe_1">1</a></li>
<li><code>sys_sleep</code>
<a href="#sys_sleep_1">1</a>
<a href="#sys_sleep_2">2</a></li>
<li><code>sys_unlink</code>
<a href="#sys_unlink_1">1</a></li>
<li><code>syscall</code>
<a href="#syscall_1">1</a></li>
<li>system call
<a href="#system_call_1">1</a></li>
<li>thread
<a href="#thread_1">1</a></li>
<li>thundering herd
<a href="#thundering_herd_1">1</a></li>
<li><code>ticks</code>
<a href="#ticks_1">1</a></li>
<li><code>tickslock</code>
<a href="#tickslock_1">1</a>
<a href="#tickslock_2">2</a></li>
<li>time-share
<a href="#time-share_1">1</a>
<a href="#time-share_2">2</a></li>
<li>top half
<a href="#top_half_1">1</a></li>
<li>trampoline
<a href="#trampoline_1">1</a>
<a href="#trampoline_2">2</a></li>
<li>transaction
<a href="#transaction_1">1</a></li>
<li>transmit complete
<a href="#transmit_complete_1">1</a></li>
<li>trap
<a href="#trap_1">1</a></li>
<li>trapframe
<a href="#trapframe_1">1</a></li>
<li>undefined behavior
<a href="#undefined_behavior_1">1</a></li>
<li><code>unlink</code>
<a href="#unlink_1">1</a></li>
<li>user memory
<a href="#user_memory_1">1</a></li>
<li>user mode
<a href="#user_mode_1">1</a></li>
<li>user space
<a href="#user_space_1">1</a>
<a href="#user_space_2">2</a></li>
<li><code>usertrap</code>
<a href="#usertrap_1">1</a></li>
<li><code>ustack</code>
<a href="#ustack_1">1</a></li>
<li><code>uvmalloc</code>
<a href="#uvmalloc_1">1</a>
<a href="#uvmalloc_2">2</a></li>
<li><code>valid</code>
<a href="#valid_1">1</a></li>
<li>vector
<a href="#vector_1">1</a></li>
<li><code>virtio_disk_rw</code>
<a href="#virtio_disk_rw_1">1</a>
<a href="#virtio_disk_rw_2">2</a></li>
<li>virtual address
<a href="#virtual_address_1">1</a></li>
<li><code>wait</code>
<a href="#wait_1">1</a>
<a href="#wait_2">2</a>
<a href="#wait_3">3</a>
<a href="#wait_4">4</a></li>
<li>wait channel
<a href="#wait_channel_1">1</a></li>
<li><code>wakeup</code>
<a href="#wakeup_1">1</a>
<a href="#wakeup_2">2</a>
<a href="#wakeup_3">3</a>
<a href="#wakeup_4">4</a></li>
<li><code>walk</code>
<a href="#walk_1">1</a>
<a href="#walk_2">2</a></li>
<li><code>walkaddr</code>
<a href="#walkaddr_1">1</a></li>
<li><code>write</code>
<a href="#write_1">1</a>
<a href="#write_2">2</a></li>
<li><code>writei</code>
<a href="#writei_1">1</a>
<a href="#writei_2">2</a>
<a href="#writei_3">3</a>
<a href="#writei_4">4</a>
<a href="#writei_5">5</a></li>
<li><code>yield</code>
<a href="#yield_1">1</a>
<a href="#yield_2">2</a>
<a href="#yield_3">3</a></li>
</ul>
</section>
<aside id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>This text generally refers to the hardware element that
executes a computation with the term <em><span
id="CPU_1">CPU</span></em>, an acronym for central processing unit.
Other documentation (e.g., the RISC-V specification) also uses the words
processor, core, and hart instead of CPU.<a href="#fnref1"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>By “multi-core” this text means multiple CPUs that share
memory but execute in parallel, each with its own set of registers. This
text sometimes uses the term <em><span
id="multiprocessor_1">multiprocessor</span></em> as a synonym for
multi-core, though multiprocessor can also refer more specifically to a
computer with several distinct processor chips.<a href="#fnref2"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Strictly speaking it is sufficient if
<code>wakeup</code> merely follows the <code>acquire</code> (that is,
one could call <code>wakeup</code> after the <code>release</code>).<a
href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>“Threads and data races” in <a href="
https://en.cppreference.com/w/c/language/memory_model" class="uri">
https://en.cppreference.com/w/c/language/memory_model</a><a
href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</body>
</html>
